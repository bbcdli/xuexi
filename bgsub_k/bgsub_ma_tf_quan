# python -m tensorflow.tensorboard --logdir=./
import tensorflow as tf
import sys
from keras.backend import set_image_dim_ordering
# tf.python.control_flow_ops = tf #hy:for remote

# KERAS_BACKEND=tensorflow python -c "from keras import backend"
# Using TensorFlow backend.
from PIL import ImageFilter
from functools import wraps
from random import randint
import time
import os
import cv2
import numpy as np
import PIL

import tflearn as tflearn
from sklearn import datasets
from scipy import ndimage
import math
import operator
import imutils
from PIL import Image  # hy: create video with images
import tools_classifier_seg as tools

# https://handong1587.github.io/deep_learning/2015/10/09/segmentation.html

# https://keras.io/getting-started/functional-api-guide/

MAX_ITERATION = 20000  # int(1e5 + 1)
NUM_OF_CLASSESS = 255  # 255#151
IMAGE_SIZE = 320
PROJ_DIR = '/home/'
Mul_Class = True
INPUT_CH = 3
do_reduce_mean = True

FLAGS = tf.flags.FLAGS
tf.flags.DEFINE_bool('DEBUG', 'False', 'Debug mode: True/ False')
tf.flags.DEFINE_integer("batch_size", "1", "batch size for training")
tf.flags.DEFINE_integer("view_size", "4", "batch size for view")
tf.flags.DEFINE_string("logs_dir", PROJ_DIR + "logs/", "path to logs directory")
tf.flags.DEFINE_string("data_dir", PROJ_DIR + "Data/data_3_segnet/mul_class/", "path to dataset")
tf.flags.DEFINE_string("val_dir", PROJ_DIR + "Test_Images/MA_video/representations/", "path to val dataset")
tf.flags.DEFINE_string("tb_dir", PROJ_DIR + 'Tensorboard_data/sum107/' + '08-08' + '/', "path to val dataset")
# tf.flags.DEFINE_float("learning_rate", "1e-4", "Learning rate for Adam Optimizer")
tf.flags.DEFINE_bool('log_on', "False", "Log mode: True/ False")

tf.flags.DEFINE_string('mode', "new_train", "Mode new_train/con_train/ visualize / only_validate_data")
tf.flags.DEFINE_integer('current_step', "1751", "current step for training")  # in fact 17050


#################################################################
# use tensorflow
# http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/

def load_and_preprocess_data(h, w):
    data_path = PROJ_DIR + '/Data/MA_cad/training/'  # main_path
    # data_path = '../Data/data_3_segnet/mul_class/MA_real/validation/'  # main_path

    print 'train_path:', data_path

    #####################################################################################
    im_path = data_path + 'others/2images_6cl/'  # for both masks_mul and masks_mul_1extra
    m_path = data_path + 'others/2masks_ex_ring/'

    # im_path = data_path + '6images/'  # total 714 + 155(green,yellow)
    # m_path = data_path + '6masks/'
    data_1s = sorted([s for s in os.listdir(im_path)])
    m_1s = sorted([s for s in os.listdir(m_path)])

    # data_1s = data_1s[0:100] + data_1s[600:700]
    # m_1s = m_1s[0:100] +  m_1s[600:700]
    #data_1s = data_1s[0:33] + data_1s[98:109]
    data_1s = data_1s[0:33]


    # data_1s = data_1s[10:28]
    # m_1s = m_1s[10:28]
    images, masks = tools.import_data_seg_tf_rgb_rnd(im_path, m_path, data_1s, m_1s, h, w, len(data_1s), MUL=True,
                                                     do_Flipping=True, do_gblur=True, do_darken=True)
    # images, masks = tools.import_data_seg_mul_tf_rgb(im_path, m_path, data_1s, m_1s, h, w, len(data_1s),
    #												 do_Flipping=True, do_gblur=True)
    if do_reduce_mean:
        images = tools.reduce_mean_stdev(images)
    # print masks[0]
    if FLAGS.DEBUG:
        import scipy.misc as misc
        for i, im, m in zip(xrange(len(images)), images, masks):
            print im.shape, m.shape
            misc.imsave(data_path + '1masks_3cl_o_val/' + str(i) + '.jpg', im)
            misc.imsave(data_path + '1masks_3cl_o_val/' + str(i) + '.png', m.reshape(h, w))
        # misc.imshow(m.reshape(h,w))

    #####################################################################################
    add_data_2 = False
    if add_data_2:
        im_path2 = data_path + '/6images_g_y/'  #
        m_path2 = data_path + '/6masks_g_y/'  #
        data_2s = sorted([s for s in os.listdir(im_path2)])
        m_2s = sorted([s for s in os.listdir(m_path2)])

        # data_2s = data_2s[100:280]
        # m_2s = m_2s[100:280]

        images2, mask2 = tools.import_data_seg_tf_rgb_rnd(im_path2, m_path2, data_2s, m_2s, h, w, len(data_2s),
                                                          MUL=False,
                                                          do_Flipping=True, do_gblur=True, do_darken=True)
        print 'train_path:', im_path2, ', images2 shape:', images2.shape, ', mask2 shape:', mask2.shape

        if do_reduce_mean:
            images2 = tools.reduce_mean_stdev(images2)

        images = np.concatenate((images, images2), axis=0)
        masks = np.concatenate((masks, mask2), axis=0)

    #####################################################################################
    add_data_3 = False
    if add_data_3:
        im_path3 = data_path + '/4images_exp/'  # main_path
        m_path3 = data_path + '/4masks_exp/'  # main_path
        data_3s = sorted([s for s in os.listdir(im_path3)])
        m_3s = sorted([s for s in os.listdir(m_path3)])

        # data_3s = data_3s[10:15]
        # m_3s = m_3s[10:15]

        images3, mask3 = tools.import_data_seg_tf_rgb_rnd(im_path3, m_path3, data_3s, m_3s, h, w,
                                                          len(data_3s), MUL=False,
                                                          do_Flipping=True, do_gblur=True, do_darken=True)
        print 'train_path:', im_path3, ', images3 shape:', images3.shape, ', mask3 shape:', mask3.shape

        if do_reduce_mean:
            images3 = tools.reduce_mean_stdev(images3)
        images = np.concatenate((images, images3), axis=0)
        masks = np.concatenate((masks, mask3), axis=0)

    add_data_4 = False
    if add_data_4:
        im_path4 = data_path + '/images_blank2/'  # main_path
        m_path4 = data_path + '/masks_blank2/'  # main_path
        data_4s = sorted([s for s in os.listdir(im_path4)])
        m_4s = sorted([s for s in os.listdir(m_path4)])

        # data_4s = data_4s[7:9] + data_4s[15:17] + data_4s[21:22] + data_4s[23:24]
        # m_4s =    m_4s[7:9]      + m_4s[15:17]     + m_4s[21:22] + m_4s[23:24]
        # data_4s = data_4s[22:47]
        # m_4s = m_4s[22:47]

        images4, mask4 = tools.import_data_seg_tf_rgb_rnd(im_path4, m_path4, data_4s, m_4s, h, w,
                                                          len(data_4s), MUL=False,
                                                          do_Flipping=True, do_gblur=True, do_darken=True)
        print 'train_path:', im_path4, ', images4 shape:', images4.shape, ', mask4 shape:', mask4.shape

        if do_reduce_mean:
            images4 = tools.reduce_mean_stdev(images4)
        images = np.concatenate((images, images4), axis=0)
        masks = np.concatenate((masks, mask4), axis=0)

    add_data_5 = False
    if add_data_5:
        im_path5 = data_path + '/4images_exp/'  # main_path  #4images_exp
        m_path5 = data_path + '/4masks_exp/'  # main_path
        data_5s = sorted([s for s in os.listdir(im_path5)])
        m_5s = sorted([s for s in os.listdir(m_path5)])

        # data_5s = data_5s[150:280]
        # m_5s = m_5s[150:280]

        images5, mask5 = tools.import_data_seg_2c_tf_rgb(im_path5, m_path5, data_5s, m_5s, h, w,
                                                         len(data_5s), do_Flipping=True, do_gblur=True)
        print 'train_path:', im_path5, ', images5 shape:', images5.shape, ', mask5 shape:', mask5.shape

        images5 = tools.reduce_mean_stdev(images5)
        images = np.concatenate((images, images5), axis=0)
        masks = np.concatenate((masks, mask5), axis=0)

    ####################################################################################
    ####################################################################################
    return images, masks


def get_online_test_batch(h, w):
    data_path = '../Test_Images/MA/test_represent/'  # main_path

    print 'train_path:', data_path
    #####################################################################################
    im_path = data_path + 'images/'
    m_path = data_path + 'masks/'
    data_1s = sorted([s for s in os.listdir(im_path)])
    m_1s = sorted([s for s in os.listdir(m_path)])

    max_num = len(data_1s)  # 3
    images, masks, images_view = tools.import_data_segnet_2c_tf_misc_cv2_gray(im_path, m_path, data_1s, m_1s, h, w,
                                                                              max_num, MUL=False,
                                                                              do_Flipping=False)
    #####################################################################################

    dummy_mask = np.ones((1, h * w))
    return images, masks, dummy_mask, data_1s, m_1s, images_view


def count_diff_pixel_values(picture, h, w):
    ds = []
    for row in xrange(h):
        for col in xrange(w):
            if picture[row][col] not in ds:
                ds.append(picture[row][col])
    return len(ds), ds


def save_str_to_file(file, lines):
    lines = '\n'.join(lines)
    with open(file, 'w') as f:
        f.writelines(lines)


def test_seg_tf_model_online(dice_results, dice_cad_results, save_name, classifier_model, h, w, dropout_1s, step):
    images, masks, dummy_mask, data_1s, label_1s, images_view = get_online_test_batch(h, w)
    if do_reduce_mean:
        images = tools.reduce_mean_stdev(images)
    # print 'images shape after mean reduction:', images.shape  # images shape: (849, 33856, 1) example 6c
    # images, masks, data_1s = images[0:5], masks[0:5], data_1s[0:5]
    print 'len of images, masks loaded:', len(images), ', ', len(masks)
    new_graph = tf.Graph()

    with tf.Session(graph=new_graph) as sess2:
        # load classifier model
        # sess, saver = tools.load_classifier_model(sess, '../testbench/6classes/', classifier_model=classifier_model)
        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=FLAGS.logs_dir)
        tf.train.import_meta_graph(classifier_model)  # (v)

        ckpt.model_checkpoint_path = classifier_model[:-5]

        if ckpt and ckpt.model_checkpoint_path:
            saver = tf.train.Saver()
            saver.restore(sess2, ckpt.model_checkpoint_path)
            print "Evaluation with model", ckpt.model_checkpoint_path
        else:
            print 'not found model'

        batch_ys = dummy_mask
        batch_ys = batch_ys.reshape(-1, h, w, 1)
        dices = 0
        for im, im_o, ref_mask, imn, mn, idx in zip(images, images_view, masks, data_1s, label_1s, xrange(len(images))):
            imn, mn = imn[:-4], mn[:-4]
            test_im = np.uint8(im_o.reshape((h, w, 3)))
            batch_xs = im.reshape(-1, h, w, INPUT_CH)  # cat

            DEBUG = False
            if DEBUG:
                print 'batch_x, batch_y shape:', batch_xs.shape, ', ', batch_ys.shape
                print 'batch_xs (test_im):'
                cv2.imshow('im', test_im)

            feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}
            # conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)
            pred = sess2.run("prediction:0", feed_dict=feed_dict_view)
            pred = np.squeeze(pred, axis=3)
            pred = pred[0].astype(np.uint8) * 255.0
            pred_thresh = pred.copy()

            conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)

            conv19_view = conv19_view[0].astype(np.uint8) * 255.0
            conv19_r, conv19_c, conv19_ch = conv19_view.shape
            # print '====== conv19 shape:', conv19_view.shape

            conv19_values0 = conv19_view[:, :, 0]
            conv19_values1 = conv19_view[:, :, 1]
            if conv19_ch == 2:
                conv19_values2 = conv19_view[:, :, 1]
            elif conv19_ch == 3:
                conv19_values2 = conv19_view[:, :, 2]

            # print 'pred *255', pred #tasks direct value without *255: [0,2], so what we see with *255: 0,255,510
            # print 'conv19 *255', conv19_view  #tasks value 0,1, => 0, 255

            ref_mask = np.squeeze(ref_mask, axis=2)
            ref_mask_thresh = ref_mask * 255.0

            print '\n'

            def print_count(img, h, w, str):
                n, diff_v = count_diff_pixel_values(img, h, w)
                min_n = min(n, 5)
                print 'num of diff values in ', str, ':', n, ',    ', diff_v[:min_n]

            print_count(pred, h, w, 'pred')
            # print_count(ref_mask_thresh,h,w,'r255')
            print_count(conv19_values0, h, w, 'con0')
            print_count(conv19_values1, h, w, 'con1')
            print_count(conv19_values2, h, w, 'con2')

            ##############################################################
            # calc dice
            ##############################################################
            threshold_ref = True
            if threshold_ref:
                """
				all values larger than thresh_at value will be replaced with a new value
				this is only for the case when only one annotation in foreground and the background are interested
				"""
                thresh_at = 1  # or 1
                ceiling_to_pixel_value = 255
                idx = ref_mask_thresh[:, :] > thresh_at
                ref_mask_thresh[idx] = ceiling_to_pixel_value

            threshold_pred = True
            if threshold_pred:
                thresh_at = 0
                pred_key_value = 255
                idx = pred_thresh[:, :] > thresh_at
                pred_thresh[idx] = pred_key_value

            if 'cad' not in imn:
                dice = tools.calc_dice_simi(pred_thresh, ref_mask_thresh, imn, k=pred_key_value)
                dice_str = imn + ', \t\tdice\t:' + str(dice)
                dice_results.append(dice_str)
                dices += dice
                # dice_2 = tools.calc_dice_simi(pred_thresh, ref_mask_thresh, imn, k=2)
                print imn, ',dice_1', dice
            # print imn,'dice_2:', dice_2
            # dice_results.append(str(avg_dice))


            if 'cad' in imn:
                dice_cad = tools.calc_dice_simi(pred_thresh, ref_mask_thresh, imn, k=pred_key_value)
                dice_cad_str = imn + ', \t\tdice\t:' + str(dice_cad)
                dice_cad_results.append(dice_cad_str)
                print imn, ', dice score: {}'.format(dice_cad)



            ##############################################################
            # save images
            ##############################################################
            display = False
            save_im = True
            if display:
                cv2.imshow('test_pred', pred)
                cv2.imshow('ref', ref_mask)
                cv2.waitKey(5)
            if save_im:
                # cv2.imwrite(save_name + 'pred_' + imn +'.png', pred)
                # cv2.imwrite(save_name + 'ori_' + imn +'.png', test_im)

                # stack view
                #
                def save_stacked_ims(im1, im2, im3):
                    test_im, pred, conv19_view = im1, im2, im3
                    r_rgb, c_rgb, ch = test_im.shape
                    r_gray, c_gray = pred.shape
                    r_rgb2, c_rgb2, conv19_ch = conv19_view.shape

                    r_comb = max(r_rgb, r_gray, r_rgb2)
                    c_comb = c_rgb + c_gray + c_rgb2
                    comb_im = np.zeros(shape=(r_comb, c_comb, ch), dtype=np.uint8)

                    comb_im[:r_rgb, :c_rgb] = test_im
                    comb_im[:r_gray, c_rgb:c_rgb + c_gray] = pred[:, :, None]
                    if conv19_ch < 3:
                        comb_im[:r_rgb2, c_rgb + c_gray:] = np.expand_dims(conv19_view[:, :, 1], 3)
                    else:
                        comb_im[:r_rgb2, c_rgb + c_gray:] = conv19_view
                    cv2.imwrite(save_name + 'com_' + imn + '.png', comb_im)

                save_stacked_ims(test_im, pred, conv19_view)

            # print 'mask path name',save_name + mn + '_ref.png'
            # cv2.imwrite(save_name + 'ref_' + mn + '.png', ref_mask*255)

        avg_dice = float(dices / (len(images) - 6))  # 6 cad
        avg_str = '\t\t\t\t================> step ' + str(step) + ' avg:' + str(avg_dice) + '\n'
        dice_results.append(avg_str)
    return avg_dice, dice_results, dice_cad_results


def train_2c_tensorflow(h, w):  # input 320x320
    # tf" assumes (rows, cols, channels), "th" assumes (channels, rows, cols)
    dice_results, dice_cad_results = [], []
    set_image_dim_ordering(dim_ordering='tf')
    # print 'train binary classes, load data'
    print 'load data'
    bg_LABEL = 'sigmch3conv19_2'
    images, masks = load_and_preprocess_data(h, w)

    Graph_seg = 1

    if Graph_seg == 1:
        # import Graph_seg_convout3 as g_seg
        import Graph_seg_tf1 as g_seg

        print 'import graph seg'
        # arch_str = 'seg'


        learning_rate, dropout_def, dropout_1s, optimizer_type, classifier_tpye, loss_type, \
        x, y, y_int, keep_prob, optimizer, cost, summary_op, optimize_op, \
        conv19_out, conv19, annotation_pred = g_seg.define_model()

    str_log = optimizer_type

    if FLAGS.log_on and ('train' in FLAGS.mode):
        sys.stdout = tools.Logger(FLAGS.log_path, str_log)

    print 'Network parameters etc:\n' \
          'learning_rate:', str(learning_rate), ', dropout_def:', dropout_def,
    ', optimizer_type', optimizer_type, ', classifier_tpye', classifier_tpye, \
    ', loss_type', loss_type  # , ' conv19_out:', conv19_out

    # hy:customized tensor model name
    model_log_name = 'model_' + bg_LABEL

    ## hy include specs of model
    from datetime import datetime
    t = datetime.now()
    date = t.strftime('%m-%d')  # t.strftime('%y-%m-%d')
    # tensorboard_path = '../Tensorboard_data/sum107/' + '06-16' + '/'
    tensorboard_path = FLAGS.tb_dir

    tensor_model_sum_path = '../tensor_model_sum/'

    # Keep training until max iterations is reached or by any other defined conditions
    set_STOP = False
    TrainingProp = 1
    train_size = int(len(images) * TrainingProp)
    print 'train_size', train_size

    # Launch the graph
    with tf.Session() as sess:
        saver = tf.train.Saver()  # hy:

        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=FLAGS.logs_dir)
        if FLAGS.mode == 'new_train':
            # Initializing the variables
            init = tf.global_variables_initializer()
            sess.run(init)
            current_step = 0
        if FLAGS.mode == 'con_train' or FLAGS.mode == 'visualize':
            current_step = FLAGS.current_step
            # or manually
            #ckpt.model_checkpoint_path = PROJ_DIR + '/logs/' + 'model_sigmch3conv19_3_0_I_loss0.36-1000.meta'
            # ckpt.model_checkpoint_path = '../logs/'+'model_seg_adam_h184_w184_b3_II_los-2.23-407'

            # print 'ckpt path', ckpt.model_checkpoint_path

            if ckpt and ckpt.model_checkpoint_path:
                saver.restore(sess, ckpt.model_checkpoint_path)
                print "Continue to train with ", ckpt.model_checkpoint_path
                classifier_model = ckpt.model_checkpoint_path + '.meta'
            else:
                print 'not found model'

        if 'train' in FLAGS.mode:
            ###############################################################
            # hy: can display all results in one graph
            train_writer = tf.train.SummaryWriter(tensorboard_path + '/train', sess.graph)
            # validation_writer = tf.train.SummaryWriter(tensorboard_path + '/vali', sess.graph)
            # test_writer = tf.train.SummaryWriter(tensorboard_path + '/test', sess.graph)

            # hy register finished class learning
            pred_y_diff = 0
            e = 0

            print 'batch size:', FLAGS.batch_size
            total_batches = int(train_size / FLAGS.batch_size)
            im_to = 0
            # f_e = float(MAX_ITERATION * FLAGS.batch_size / train_size)
            # i_e = int(MAX_ITERATION * FLAGS.batch_size / train_size)

            train_ith_batch = 0
            for i in xrange(current_step, current_step + MAX_ITERATION + MAX_ITERATION):
                start_time = time.time()
                ## Set batch data  #############################################################
                # im_from = im_to if im_to < train_size-1 else 0
                # im_to = min(train_size - 1, im_from + FLAGS.batch_size)

                train_index = randint(0, train_size - FLAGS.batch_size)
                im_from, im_to = train_index, train_index + FLAGS.batch_size

                # print 'from',im_from,'to',im_to

                for batch_step in xrange(FLAGS.batch_size):
                    batch_xs, batch_ys = images[im_from:im_to], masks[im_from:im_to]
                # cv2.imshow('tmp_ys',batch_ys[-1,:,:])

                batch_xs = batch_xs.reshape(-1, h, w, INPUT_CH)  # cat
                batch_ys = batch_ys.reshape(-1, h, w, 1)

                ## Feed batch data   ############################################################
                feed_dict = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_def}
                feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}

                ## Start Training  ##############################################################
                loss = sess.run(cost, feed_dict=feed_dict)
                y_sess_value = sess.run(y, feed_dict=feed_dict_view)
                y_sess_value = np.squeeze(y_sess_value, axis=3)
                # print '========== y_sess shape:', y_sess_value.shape

                sess.run(optimize_op, feed_dict=feed_dict)

                view_interval = 500
                if i < 1000:
                    view_interval = 50
                if i > 1000 and i < 5000:
                    view_interval = 250

                if i > 5000:
                    view_interval = 1000

                if i > 50000:
                    view_interval = 2000

                glb_step = i
                if i % view_interval == 0:
                    # summary_str = sess.run(summary, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})
                    # summary_writer.add_summary(summary_str, step)
                    train_ith_batch = 0 if train_ith_batch >= total_batches - view_interval else train_ith_batch + view_interval

                    # print 'Elapsed time:', "{:.2f}".format(elapsed_time / 60), 'min'
                    ## Validation  ####################################################################
                    # val1_acc = sess.run(accuracy, feed_dict={x: val1_batch_xs, y: val1_batch_ys, keep_prob: dropout_1s})
                    train_res = sess.run(summary_op, feed_dict=feed_dict_view)
                    train_writer.add_summary(train_res, i)

                    elapsed_time = int(time.time() - start_time)
                    t_str = datetime.now().strftime('%H-%M-%S')
                    print " %2d-%2d/%2d %s ---> -ETA: %ds -\tloss: %g" % (
                        i, train_ith_batch, total_batches, t_str, elapsed_time, loss)

                # if  i % total_batches == 0:
                if i % view_interval == 0:  # and i > current_step + 500:
                    save_all_models = 1
                    if save_all_models == 1:
                        mn = model_log_name + '_' + str(batch_step) + '_I_loss' + str(round(loss, 2))
                        model_save = FLAGS.logs_dir + mn
                        saver.save(sess, save_path=model_save,
                                   global_step=glb_step)  # saver.save(sess, FLAGS.logs_dir + "model.ckpt", itr)
                        classifier_model = model_save + '-' + str(i) + '.meta'
                        print '--- ---> Iter %5d: saving model to %s' % (glb_step, classifier_model)
                        e += 1

                    # print 'download images'
                    # tools.save_tb_imgs(tensorboard_path + str(i)) #visualization
                    # online test
                    do_online_test = True
                    if do_online_test:
                        n, diff_v = count_diff_pixel_values(y_sess_value[0], h, w)
                        print 'num of diff values in y_sess:', n, ', diff:', diff_v
                        # print 'y_int:', y_int_sess   #(?, 320, 320, 1), dtype=int32)

                        avg, dice_results, dice_cad_results = \
                            test_seg_tf_model_online(dice_results, dice_cad_results, tensorboard_path + str(i),
                                                     classifier_model, h, w, dropout_1s, i)
                        print '================================> step ', glb_step, ' test average dice:', avg

                    control_save = True
                    if control_save:
                        if avg > 0.12 and avg < 1 and i < int(1e7 + 1):
                            # save the model
                            model_save2 = FLAGS.logs_dir + model_log_name + '_II_avg' + str(round(avg, 2))
                            saver.save(sess, save_path=model_save2, global_step=glb_step)
                            print 'model save', model_save2
                            cmd = 'mv ' + FLAGS.logs_dir + 'model*II* ' + tensor_model_sum_path
                            os.system(cmd)

                            cmd = 'rm ' + FLAGS.logs_dir + 'model*I*'
                            os.system(cmd)
                        elif avg > 0.4 and avg < 1:
                            model_save2 = FLAGS.logs_dir + model_log_name + '_II_avg' + str(round(avg, 2))
                            saver.save(sess, save_path=model_save2, global_step=glb_step)
                            print 'model save', model_save2
                            cmd = 'mv ' + FLAGS.logs_dir + 'model*II* ' + tensor_model_sum_path
                            os.system(cmd)

                            cmd = 'rm ' + FLAGS.logs_dir + 'model*I*'
                            os.system(cmd)

                    save_online_eva_to_file = True
                    if save_online_eva_to_file:
                        # keyword = os.path.basename(os.path.normpath(FLAGS.log_dir))
                        keyword = 'dice_results'
                        dice_txt = dice_results + dice_cad_results
                        save_str_to_file(FLAGS.logs_dir + keyword + '.txt', dice_txt)

                    if set_STOP:
                        print 'STOP is set'
                        break

                if i % (view_interval * 2) == 0:
                    REMOVE_IMAGES = True
                    if REMOVE_IMAGES and glb_step > 1:
                        if avg < 0.01:
                            # remove saved images
                            cmd = 'rm ' + FLAGS.tb_dir + str(glb_step) + 'com*'
                            os.system(cmd)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print 'key interrupt'
                    break

        #################
        if FLAGS.mode == 'visualize':
            for view_i in xrange(len(images) - 1):
                im_from, im_to = view_i, view_i + 1
                # train_index = randint(0, train_size - FLAGS.batch_size)
                # im_from, im_to = train_index, train_index + FLAGS.batch_size

                batch_xs, batch_ys = images[im_from:im_to], masks[im_from:im_to]

                batch_xs = batch_xs.reshape(-1, h, w, 3)  # cat
                batch_ys = batch_ys.reshape(-1, h, w, 1)

                ## Feed batch data   ############################################################
                feed_dict_view = {x: batch_xs, y: batch_ys, keep_prob: dropout_1s}

                ################################################################
                pred = sess.run(annotation_pred, feed_dict=feed_dict_view)
                pred = np.squeeze(pred, axis=3)
                print 'pred shape', pred.shape

                conv19_view = sess.run('conv19:0', feed_dict=feed_dict_view)

                show_value = False
                if show_value:
                    counter = 0
                    for i in xrange(320):
                        for j in xrange(100, 320):
                            counter += 1
                            if counter % 100 == 0:
                                print '\n'
                            print pred[0][i][j],

                cv2.imshow('pred', pred[0].astype(np.uint8) * 255)
                cv2.imshow('conv19', conv19_view[0].astype(np.uint8))
                cv2.waitKey()

        print "\nOptimization Finished!"


#######################################

#######################################
if 'only_validate_data' in FLAGS.mode:
    load_and_preprocess_data(320, 320)

if 'train' in FLAGS.mode or 'visualize' in FLAGS.mode:
    train_2c_tensorflow(320, 320)

    print("Training done!")


# if __name__ == "__main__":#
#	tf.app.run()

#wang_jie_gou
import tensorflow as tf
import numpy as np
import tflearn

import settings  # hy: collection of global variables

settings.set_global()
PRINT_ARCHI = True

#when using concat at axis 0 got logits shape [512000,3] and labels shape [102400]
####################################################### Header begin ###################p#############################
###############################
# 0    1      2      3        4        5      6     7      8
# dropout = [1,  1,  1,  1,      1,      1,  1,  1,  1] #1st
dropout = [0.15, 0.25, 0.4, 0.45, 1, 0.4, 0.25, 0.15, 0.15]  # 1st
#dropout = [0.5, 0.5, 0.4, 0.5, 1, 0.4, 0.5, 0.5, 0.5]  # 1st
dropout_1s = [1] * len(dropout)

# output size is now n_classes
n_classes = 2
IMAGE_SIZE = 320  # 320
input_size = IMAGE_SIZE * IMAGE_SIZE  # hy
optimizer_type = 'adam'# 'adam'  # 'adam' #GD-'gradient.descent',#'ProximalGradientDescent', #'SGD', #'RMSprop'
#learning_rate = 0.0005
learning_rate = 0.000498
#learning_rate = 0.00001

classifier_tpye = 3  # 1.category  2.binary  3 ch=3
loss_type = 5  # 1'IoU',  2'sparse_softmax',  3 'loss_dice'   4.sigmoid  5.loss_l2

input_ch = 3
conv19_out = 3  # 2 can set 255, different from fcn, where can only use 256
pad='SAME'  #'VALID'
u = 8
# Tensor must be 4-D with last dim 1, 3, or 4, not [1,320,320,2]

# IoU: conv19_out can be any num
# sparse_softmax: conv19_out can only be 2

######################
# GD
if optimizer_type == 'GD':
	learning_rate = learning_rate  # 0.00405 good #0.09 bad 3549 #0.04049 #0.03049 #0.015 #0.07297 #0.09568# TODO 0.05  0.005 better, 0.001 good \0.02, 0.13799 to 0.14 good for 6 classes,

######################
# RMSprop
if optimizer_type == 'RMSprop':
	decay = 0.00009
	momentum = 0.99
	epsilon_R = 0.009
######################
# SGD
if optimizer_type == 'SGD':
	learning_rate = learning_rate
	momentum_SGD = 0.99  # 0.99
# lr_decay = 0.01
# decay_step = 100
######################
######################
# adam
# Adam with these parameters beta1=0.9,beta2=0.999, epsilon=1e-08 etc the training accuracy is not stable, epsilon = 0.01
if optimizer_type == 'adam':
	learning_rate = learning_rate
	beta1 = 0.9
	beta2 = 0.999
	epsilon = 0.1


####################################################### Header End#### ################################################

def IoU(logits=None, labels=None, pixel_num=input_size):
	with tf.name_scope('cost'):
		# dimension of logits and labels must be equal
		logits = tf.to_float(tf.reshape(logits, (-1, pixel_num)))
		labels = tf.to_float(tf.reshape(labels, (-1, pixel_num)))
		
		# intersection = sum([logits] .* [labels])
		# since only 1s in labels will produce non-zero values, this produce is then the intersection
		intersec = tf.reduce_sum(tf.mul(logits, labels))
		#
		union = tf.reduce_sum(tf.sub(tf.add(logits, labels), tf.mul(logits, labels)))
		loss = tf.sub(tf.constant(1.0, dtype=tf.float32), tf.div(intersec, union))
	return loss


def loss_sigmoid(logits=None, labels=None, pixel_num=input_size):
	with tf.name_scope('cost'):
		logits = tf.to_float(tf.reshape(logits, [-1, pixel_num]))
		labels = tf.to_float(tf.reshape(labels, [-1, pixel_num]))
		cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits, labels)  # no args keyword
		# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels) #no args keyword, all values
		# go to 1 or 0 quickly
		loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')
	return loss


def loss_dice_coeff(logits=None, labels=None, pixel_num=input_size):
	# input labels and logits must be of type float (use tf.to_float)
	# output: loss value, which is the negative dice coefficient
	# accuracy is the dice coefficient
	pixel_num = input_size
	with tf.name_scope('cost'):
		# num_equals_net_output = conv19_out
		# the dimension of labels and logits should be equal
		print 'logits flat shape:', logits.get_shape(), ',  labels flat shape:', labels.get_shape()
		logits = tf.to_float(tf.reshape(logits, (-1, pixel_num)))
		labels = tf.to_float(tf.reshape(labels, (-1, pixel_num)))
		
		print 'logits flat shape:', logits.get_shape(), ',  labels flat shape:', labels.get_shape()
		
		# using * and axis=1
		# intersection = tf.reduce_sum(labels * logits,axis=1,keep_dims=True,name='intersection')
		# Info: intersection size: (?, 1, 102400)
		# ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,102400,102400]
		
		# using mul
		intersection = tf.reduce_sum(tf.mul(labels, logits))
		# Info: intersection size: ()
		# ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,102400,102400]
		
		smooth = 1  # avoid 0 as output
		loss_inter = tf.add(2. * intersection, smooth, name='loss_inter')
		# loss_inter = tf.add(tf.mul(tf.constant(2), intersection,axis=1,keep_dims=True), smooth)
		loss_u1 = tf.add(tf.reduce_sum(labels), tf.reduce_sum(logits), name='loss_u1')
		loss_u = tf.add(loss_u1, smooth, name='loss_u')
		loss_div = tf.div(loss_inter, loss_u, name='loss_div')  # expected value very close to or equal 1
		loss = tf.sub(1.0, loss_div, name='cost')
		'''
	intersection = tf.reduce_sum(flat_logits * flat_labels, axis=1, keep_dims=True)
		union = tf.reduce_sum(tf.mul(flat_logits, flat_logits), axis=1, keep_dims=True) \
						+ tf.reduce_sum(tf.mul(flat_labels, flat_labels), axis=1, keep_dims=True)
		loss = 1 - tf.reduce_mean(2 * intersection / (union))
	'''
	return loss


# return (2. * intersection + smooth) / (K.sum(labels) + K.sum(logits) + smooth)
def loss_l2(logits=None, labels=None, pixel_num=input_size):
	import keras
	with tf.name_scope('cost'):
		logits = tf.to_float(tf.reshape(logits, [-1, pixel_num]))
		labels = tf.to_float(tf.reshape(labels, [-1, pixel_num]))
		
		predictions = logits
		# **************************************************************************************
		# - weighted cross entropy
		# logits, targets, pos_weight, name=None
		# loss_all = tf.nn.weighted_cross_entropy_with_logits(logits, labels, -0.0008, name=None)
		# loss = tf.reduce_mean(loss_all)
		#################################################
		# - binary crossentropy
		# epsilon = 1e-8
		# loss = tf.reduce_mean(-(labels * tf.log(logits + epsilon) +
		#                      (1. - labels) * tf.log(1. - logits + epsilon)))
		# **************************************************************************************
		
		################################################
		times_diff = tf.mul(1.0, (logits - labels))
		loss = tf.reduce_sum(tf.pow(times_diff, 2)) / (1.0 * pixel_num)
	return loss


def loss2(logits=None, labels=None, n_classes=0, cost_name=''):  # cross_entropy, dice_coefficient
	"""
	Constructs the cost function, either cross_entropy, weighted cross_entropy or dice_coefficient.
	Optional arguments are:
	class_weights: weights for the different classes in case of multi-class imbalance
	regularizer: power of the L2 regularizers added to the loss function
	"""
	
	flat_logits = tf.reshape(logits, [-1, n_classes])
	flat_labels = tf.reshape(labels, [-1, n_classes])
	if cost_name == "cross_entropy":
		# class_weights = cost_kwargs.pop("class_weights", None)
		class_weights = None
		
		if class_weights is not None:
			class_weights = tf.constant(np.array(class_weights, dtype=np.float32))
			
			weight_map = tf.mul(flat_labels, class_weights)
			weight_map = tf.reduce_sum(weight_map, axis=1)
			
			loss_map = tf.nn.softmax_cross_entropy_with_logits(flat_logits, flat_labels)
			weighted_loss = tf.mul(loss_map, weight_map)
			
			loss = tf.reduce_mean(weighted_loss)
		
		else:
			loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(flat_logits,
			                                                              flat_labels))
	else:
		raise ValueError("Unknown cost function: " % cost_name)
	
	# regularizer = cost_kwargs.pop("regularizer", None)
	# regularizer = None
	# if regularizer is not None:
	#    regularizers = sum([tf.nn.l2_loss(variable) for variable in variables])
	#    loss += (regularizer * regularizers)

	return loss

def pool(img, k, pool_type='max',pad='SAME', name=None):
	if pool_type == 'max':
		return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)
	if pool_type == 'avg':
		return tf.nn.avg_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)

def p_relu0(x):
  alphas = tf.get_variable('alpha', x.get_shape()[-1],
                       initializer=tf.constant_initializer(0.0),
                        dtype=tf.float32)
  pos = tf.nn.relu(x)
  neg = alphas * (x - abs(x)) * 0.5
  return pos + neg

def p_relu(_x, name="prelu"):
  _alpha = tf.get_variable(name, shape=_x.get_shape()[-1],
                           dtype=_x.dtype, initializer=tf.constant_initializer(0.1))
  return tf.maximum(0.0, _x) + _alpha * tf.minimum(0.0, _x)

def conv2d(img, w, b, k, pad='SAME', name=None):
	#x = tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b)
	return p_relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b),name=name)
	#return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b), name=name)



def conv2d_leaky(img, w, b, k, pad='SAME', name=None):
	alpha = 0.0001
	x = tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b)
	return tf.maximum(alpha*x,x,name=name)


def conv2d_1(img, w, b, k, pad='SAME', name=None):
	return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b), name=name)


def max_pool(img, k, pad='SAME', name=None):  # 'SAME', 'VALID'
	return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)


def avg_pool(img, k, pad='SAME', name=None):  # 'SAME', 'VALID'
	return tf.nn.avg_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)


def train(loss_val, var_list, learning_rate):
	optimizer = tf.train.AdamOptimizer(learning_rate)
	grads = optimizer.compute_gradients(loss_val, var_list=var_list)
	return optimizer.apply_gradients(grads)


def conv_net(_X, _weights, _biases, _dropout, filter_size, filter_size_out, k_conv, k_pool, k_out, SEED):
	# - INPUT Layer
	# Reshape input picture
	
	# _X = tf.reshape(_X, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 3])
	
	################################
	# - Convolution Layer 1,2
	# - Convolution Layer 1,2
	conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'], k_conv, name='conv1')  # 4
	conv2 = conv2d(conv1, _weights['wc2'], _biases['bc2'], k_conv, pad=pad,name='conv2')
	pool1 = pool(conv2, k_pool, name='pool1')
	pool1d = tf.nn.dropout(pool1, _dropout[0])  # TODO comment it later!
	
	################################
	# - Convolution Layer 3,4
	conv3 = conv2d(pool1d, _weights['wc3'], _biases['bc3'], k_conv, name='conv3')
	conv4 = conv2d(conv3, _weights['wc4'], _biases['bc4'], k_conv, pad=pad,name='conv4')
	pool2 = pool(conv4, k_pool, name='pool2')
	pool2d = tf.nn.dropout(pool2, _dropout[1])
	
	# - Convolution Layer 5,6
	conv5 = conv2d(pool2d, _weights['wc5'], _biases['bc5'], k_conv, name='conv5')
	conv6 = conv2d(conv5, _weights['wc6'], _biases['bc6'], k_conv, pad=pad,name='conv6')
	pool3 = pool(conv6, k_pool, name='pool3')
	pool3d = tf.nn.dropout(pool3, _dropout[2])
	
	# - Convolution Layer 7,8
	conv7 = conv2d(pool3d, _weights['wc7'], _biases['bc7'], k_conv, name='conv7')
	conv8 = conv2d(conv7, _weights['wc8'], _biases['bc8'], k_conv, pad=pad,name='conv8')
	pool4 = pool(conv8, k_pool, name='pool4')
	pool4d = tf.nn.dropout(pool4, _dropout[3])
	################################
	
	################################
	# - Convolution Layer 9,10
	conv9 = conv2d(pool4d, _weights['wc9'], _biases['bc9'], k_conv, name='conv9')
	conv10 = conv2d(conv9, _weights['wc10'], _biases['bc10'], k_conv,pad=pad, name='conv10')
	conv10 = tf.nn.dropout(conv10, _dropout[4])
	
	
	def conv2d_transpose(name_or_scope, input_tensor, out_ch, ksize=3, stride=2, re_shape=None, padding=pad):
		strides = [1, stride, stride, 1]
		debug = False
		with tf.variable_scope(name_or_scope):
			in_ch = input_tensor.get_shape()[3].value
			
			if re_shape is None:
				# get shape out of input_tensor
				in_shape = tf.shape(input_tensor)
				if debug:
					print 'input shape:', in_shape.get_shape()
				
				if (padding is 'SAME'):
					out_h = in_shape[1] * stride  # 20*2 = 40
					out_w = in_shape[2] * stride  # 20*2 = 40
				elif padding is 'VALID':
					out_h = ((in_shape[1] - 1) * stride) - 1  # (20-1)*2 -1 = 38
					out_w = ((in_shape[2] - 1) * stride) - 1  # (20-1)*2 -1 = 38
				# out_h = ((in_shape[1] - 1) * stride) + 1 #
				# out_w = ((in_shape[2] - 1) * stride) + 1
				new_shape = [in_shape[0], out_h, out_w,
				             out_ch]  # batch size, h,w,ch: ?,20,20,32, ch is num of classes
			else:
				# or from defined re_shape
				new_shape = [re_shape[0], re_shape[1], re_shape[2], out_ch]
			output_shape = tf.pack(new_shape)  # ?,40,40,32
			
			print '\nup Layer: %s, ch-in: %d, ch-out: %d' % (name_or_scope, in_ch, out_ch)
			weights_tensor_shape = [ksize, ksize, out_ch, in_ch]  # 3,3,32,32
			
			# create weights tensor for transpose
			num_input = ksize * ksize * in_ch / stride  # 3x3x32/2 = 9x16
			stddev = (2 / num_input) ** 0.5  # 2/(9x16) ** 0.5 = 1/72 ** 0.5 #'**' uppacking args list
			# using stddev = 0.1 the same value as for other nodes produces better learning speed
			# 1_0_diff increases quicker, once pred_train for one of labels becomes 0, the training must be
			# terminated, otherwise, the other values will also be trained to be 0.
			weights = tf.get_variable('w_transp', weights_tensor_shape,
			                          initializer=tf.truncated_normal_initializer(stddev=0.1, seed=SEED))
			
			# weights = self.get_deconv_filter(weight_tensor_shape)
			deconv = tf.nn.conv2d_transpose(input_tensor, weights, output_shape,
			                                strides=strides, padding=pad)
			# conv=tf.nn.conv2d(img,w,strides=[1,k,k,1],padding)
			# add=tf.nn.bias_add(conv,bias_)
			# activation=tf.nn.relu(add,name=name)
			# tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b),name=name)
			
			# initialize bias
			bias = tf.get_variable('b', [out_ch], initializer=tf.truncated_normal_initializer(0.1, seed=SEED))
			# bias = tf.get_variable('b', [output_ch], initializer=tf.truncated_normal_initializer(0.1, seed=SEED))
			
			deconv_add = tf.nn.bias_add(deconv, bias)
			activation = tf.nn.relu(deconv_add)
			
			if debug:
				deconv = tf.Print(deconv, [tf.shape(deconv)],
				                  message='Shape of %s' % name_or_scope,
				                  summarize=4, first_n=1)
		
		return activation
	
	# UP_1
	# re_shape:[?,h,w,ch]
	up1 = conv2d_transpose('up1', conv10, 4*u, ksize=3, stride=2, re_shape=None, padding=pad) #(?, 40, 40, 32)
	
	concat_axis = 3
	up1_concat = tf.concat(concat_axis, [conv8, up1])  # conv8: 40,40,32, merged: (?, 40, 40, 64)
	
	up1_concatd = tf.nn.dropout(up1_concat, _dropout[5])
	# - Convolution Layer 11,12
	conv11 = conv2d(up1_concatd, _weights['wc11'], _biases['bc11'], k_conv, name='conv11')  # conv11_out = 32
	conv12 = conv2d(conv11, _weights['wc12'], _biases['bc12'], k_conv, name='conv12')
	
	##########################################################
	# UP_2
	up2 = conv2d_transpose('up2', conv12, 4*u, ksize=3, stride=2, re_shape=None, padding=pad)
	
	up2s = tf.slice(up2, [0, 0, 0, 0], [0, 80, 80,     2*u])
	conv6s = tf.slice(conv6, [0, 0, 0, 0], [0, 80, 80, 2*u])
	concat_axis = 3
	up2_concat1 = tf.concat(concat_axis, [conv6s, up2s], name='up2conv6')  # conv6: 80,80,32
	# the input ch for conv13 is 32, so the output ch for up2_concat must be reduced to 32
	#tf.random_crop(value, size, seed=None, name=None)
	#up2_concats = tf.slice(up2_concat, [1:-1, 0, 0, 0], [1:-1, 80, 80, 4*u]) #[1:-1, :, :, :]
	#up2_concats = tf.slice(up2_concat, [0, 0, 0, 0], [0, 80, 80, 4*u]) #[1:-1, :, :, :] #(up2conv6, Slice/begin, Slice/size)
	
	#up2_concat = conv2d(up2_concat1, _weights['concat2'], _biases['bc_concat2'], k_conv, name='concat2')
	
	up2_concatd = tf.nn.dropout(up2_concat1, _dropout[6])
	
	# - Convolution Layer 13,14
	conv13 = conv2d(up2_concatd, _weights['wc13'], _biases['bc13'], k_conv, name='conv13') #32
	conv14 = conv2d(conv13, _weights['wc14'], _biases['bc14'], k_conv, name='conv14')
	
	##########################################################
	# UP_3
	up3 = conv2d_transpose('up3', conv14, 2*u, ksize=3, stride=2, re_shape=None, padding=pad)  # ?, 160, 160, 32  16

	conv4s = tf.slice(conv4, [0, 0, 0, 0], [0, 160, 160, 2*u])  #  cov4  # ?, 160, 160,16
	up3s   = tf.slice(up3, [0, 0, 0, 0], [0, 160, 160,   2*u])
	concat_axis = 3
	up3_concat1 = tf.concat(concat_axis, [conv4s, up3s])

	up3_concatd = tf.nn.dropout(up3_concat1, _dropout[7])
	
	
	# - Convolution Layer 15,16
	conv15 = conv2d(up3_concatd, _weights['wc15'], _biases['bc15'], k_conv, name='conv15')  #16, 16
	conv16 = conv2d(conv15, _weights['wc16'], _biases['bc16'], k_conv, name='conv16')
	# conv16 can be 8
	
	##########################################################
	# UP_4 (16-in, 8-out)
	
	if PRINT_ARCHI:
		print 'input tensor', _X.get_shape()
		print 'conv1 ( f=', filter_size, 'k=', k_conv, ')', conv1.get_shape()
		print 'conv2 ( f=', filter_size, 'k=', k_conv, ')', conv2.get_shape(), '<===4'
		print 'conv2 - max pooling (k=', k_pool, ')', pool1.get_shape()
		print '- dropout ( keep rate', dropout[0], ')', pool1d.get_shape()
		print '\nconv3 ( f=', filter_size, 'k=', k_conv, ')', conv3.get_shape()
		print 'conv4 ( f=', filter_size, 'k=', k_conv, ')', conv4.get_shape(), '<===3'
		print 'conv4 max pooling ( k=', k_pool, ')', pool2.get_shape()
		print '- dropout ( keep rate', dropout[1], ')', pool2d.get_shape()
		print '\nconv5 ( f=', filter_size, 'k=', k_conv, ')', conv5.get_shape()
		print 'conv6 ( f=', filter_size, 'k=', k_conv, ')', conv6.get_shape(), '<===2'
		print 'conv6 max pooling ( k=', k_pool, ')', pool3.get_shape()
		print '- dropout ( keep rate', dropout[2], ')', pool3d.get_shape()
		print '\nconv7 ( f=', filter_size, 'k=', k_conv, ')', conv7.get_shape()
		print 'conv8 ( f=', filter_size, 'k=', k_conv, ')', conv8.get_shape(), '<===1'
		print 'conv8 max pooling ( k=', k_pool, ')', pool4.get_shape()
		print '- dropout ( keep rate', dropout[3], ')', pool4d.get_shape()
		print '\nconv9 ( f=', filter_size, 'k=', k_conv, ')', conv9.get_shape()
		print 'conv10 ( f=', filter_size, 'k=', k_conv, ')', conv10.get_shape()
		print '- dropout ( keep rate', dropout[4], ')', conv10.get_shape()
		print 'conv8 shape:', conv8.get_shape(), ',  up1 transpose shape:', up1.get_shape(), '<=== 1'  # [?,40,40,32]
		print 'up1,conv8 concat(a=', concat_axis, ') shape:', up1_concat.get_shape()
		print 'up1 concat - dropout ( keep rate', dropout[5], ')', up1_concatd.get_shape()
		print '\nconv11 ( f=', filter_size, 'k=', k_conv, ')', conv11.get_shape()
		print 'conv12 ( f=', filter_size, 'k=', k_conv, ')', conv12.get_shape()
		print 'conv6 shape:', conv6.get_shape(), ',  up2 transpose shape:', up2.get_shape(), '<=== 2'
		#print 'up2,conv6 concat (a=', concat_axis, ') shape:', up2_concat.get_shape()
		#print 'up2_concat slice shape:', up2_concats.get_shape()
		print '- merge conv6, dropout ( keep rate', dropout[6], ')', up2_concatd.get_shape()  # ?, 80, 80, 64
		print '\nconv13 ( f=', filter_size, 'k=', k_conv, ')', conv13.get_shape()  # ?, 80, 80, 32
		print 'conv14 ( f=', filter_size, 'k=', k_conv, ')', conv14.get_shape()  # ?, 80, 80, 32
		print '\nup3 transposed shape:', up3.get_shape()
		print 'conv4:', conv4.get_shape(), ',  up3:', up3.get_shape()
		#print 'conv4 slice:', conv4s.get_shape(), ',  up3 slice:', up3s.get_shape(), '<=== 3'
		#print 'up3,conv4s concat(a=', concat_axis, ') shape:', up3_concat.get_shape()  # (?, 160, 160, 48)    #use slice (1, 160, 160, 32)
		print '- up3 concat, dropout ( keep rate', dropout[7], ')', up3_concatd.get_shape()  # ?, 160, 160, 48
		print '\nconv15 ( f=', filter_size, 'k=', k_conv, ')', conv15.get_shape()  # ?, 160, 160, 16
		print 'conv16 ( f=', filter_size, 'k=', k_conv, ')', conv16.get_shape()  # ?, 160, 160, 8
	
	up4 = conv2d_transpose('up4', conv16, u, ksize=3, stride=2, re_shape=None, padding=pad) #tmp u to 2u
	print '\nup4 transposed shape:', up4.get_shape()  # (?, 320, 320, 8)
	
	#up4s = tf.slice(up4,[0,0,0,0], [0,320,320,8])
	#print 'conv2:', conv2.get_shape(), ',  up4:', up4s.get_shape(), '<=== 4'  # conv2: (?, 320, 320, 8)
	
	concat_axis = 0
	up4_concat1 = tf.concat(concat_axis, [conv2, up4]) #-1,320,320,8
	print 'up4_concat1:', up4_concat1.get_shape()
	
	#up4_concat = conv2d(up4_concat1, _weights['concat4'], _biases['bc_concat4'], k_conv, name='concat4')
	
	#concat_axis = 3
	#up4_concat = tf.concat(concat_axis, [up4_concat, up4s]) #-1,320,320,16
	#print 'up4_concat:', up4_concat.get_shape()
	
	#print 'up4,conv2 concat (a=', concat_axis, ') shape:', up4_concat.get_shape()
	# conv4 first output size (?, 160, 160, 16)
	up4_concat = tf.nn.dropout(up4_concat1, _dropout[8])
	print '- dropout ( keep rate', dropout[8], ')', up4_concat.get_shape()
	
	# - Convolution Layer 17,18
	conv17 = conv2d(up4_concat, _weights['wc17'], _biases['bc17'], k_conv, name='conv17')
	#                                 8, 8
	print '\nconv17 ( f=', filter_size, 'k=', k_conv, ')', conv17.get_shape()  # ?, 320, 320, 8
	
	conv18 = conv2d(conv17, _weights['wc18'], _biases['bc18'], k_conv, name='conv18')
	print 'conv18 ( f=', filter_size, 'k=', k_conv, ')', conv18.get_shape()  # ?, 320, 320, 8

	# without activation, many outputs are negative, so should not use
	# or use relu, when it combined with softmax logits, all values go to 1 quickly
	# conv19 = conv2d(conv18, _weights['wc19'], _biases['bc19'], k_out, name='conv19')
	#
	# or use sigmoid activation for final conv layer, when it combined with softmax logits, all values go to 0 quickly
	# tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding='SAME'), b))
	#
	
	
	conv19 = tf.nn.sigmoid(tf.nn.bias_add(tf.nn.conv2d(conv18, _weights['wc19'],
	              strides=[1, k_out, k_out, 1], padding=pad), _biases['bc19']),name='conv19')  # 1, 1
	print '\nconv19 ( f=', filter_size, 'k=', k_out, ')', conv19.get_shape()  # ?, 320, 320, 1
	
	
	
	# Output, class prediction
	#annotation_pred_max = tf.argmax(conv19, dimension=3)
	annotation_pred_max = tf.argmax(conv19, axis=3) #use tf.abs not work
	#annotation_pred_max = tf.argmax(conv19, axis=3)
	annotation_pred = tf.expand_dims(annotation_pred_max, dim=3, name='prediction')
	
	return [_X, conv1, conv2, conv3, conv4, conv5, conv6, conv7, \
	        conv8, conv9, conv10, up1, conv11, conv12, up2, conv13, conv14, up3, conv15,
	        conv16, up4, conv17, conv18, conv19, annotation_pred]


###################################################################
def define_model():
	# General input for tensorflow
	# hy: Graph input, same placeholders for various architectures
	x = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, input_ch], name="x")
	
	################################################ Graph 4conv begin
	keep_prob = tf.placeholder(tf.float32, len(dropout), name="keep_prob")
	# hy: define receptive field size
	filter_size, filter_size_out = 3, 1
	k_conv, k_out, k_pool = 1, 1, 2
	
	
	SEED = 8  # hy: number of filters in conv1  8, 16, 64
	conv1_out, conv2_out, conv3_out, conv4_out = u, u, 2*u, 2*u
	conv5_out, conv6_out, conv7_out, conv8_out, conv9_out, conv10_out = 4*u, 4*u, 4*u, 4*u, 4*u, 4*u
	
	# must use the same name for up1 here and in net
	#up1, concat1, conv11_out, conv12_out, up2, concat2, conv13_out, conv14_out = 4*u, 8*u, 4*u, 4*u, 4*u, 4*u, 4*u, 4*u  # concat2:64 or 32
	up1,   concat1, conv11_out, conv12_out, up2,   concat21, concat2, conv13_out, conv14_out \
    = 4*u, 8*u,     4*u,        4*u,        4*u,    4*u,     4*u,     4*u,        4*u  # concat2:64 or 32
	
	## upsampling using fcn deconv
	# up3, concat3, conv15_out, conv16_out,  up4,concat4, conv17_out, conv18_out = 32, 16, 16, 8,         16, 8,  8,  8  # conv16_out:16 or 8
	
	# upsampling using version 1
	#up3, concat3, conv15_out, conv16_out,    up4, concat4, conv17_out, conv18_out = 4*u, 4*u, 2*u, u,       2*u, u, u, u  # conv16_out:16 or 8
	#                                                                                  ^
	
	up3,   concat31, concat3,   conv15_out, conv16_out,     up4, concat41,concat4, conv17_out, conv18_out \
	= 2*u, 4*u,      4*u,       2*u,        2*u,            u,   u,        u,       u,          u  # conv16_out:16 or 8
	# stddev = sqrt(2 / fan_in)  #np.random.randn(n) / sqrt(n)
	
	weights = {
		'wc1': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, input_ch, conv1_out], stddev=np.sqrt(2.0 / SEED).astype(np.float32),
			                    seed=SEED),
			name="wc1"),
		
		'wc2': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, SEED, conv2_out], stddev=np.sqrt(2.0 / SEED).astype(np.float32),
			                    seed=SEED),
			name="wc2"),
		'wc3': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv2_out, conv3_out],
		                                       stddev=np.sqrt(2.0 / conv2_out).astype(np.float32), seed=SEED),
		                   name="wc3"),
		'wc4': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv3_out, conv4_out],
		                                       stddev=np.sqrt(2.0 / conv3_out).astype(np.float32), seed=SEED),
		                   name="wc4"),
		'wc5': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv4_out, conv5_out],
		                                       stddev=np.sqrt(2.0 / conv4_out).astype(np.float32), seed=SEED),
		                   name="wc5"),
		'wc6': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv5_out, conv6_out],
		                                       stddev=np.sqrt(2.0 / conv5_out).astype(np.float32), seed=SEED),
		                   name="wc6"),
		'wc7': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv6_out, conv7_out],
		                                       stddev=np.sqrt(2.0 / conv6_out).astype(np.float32), seed=SEED),
		                   name="wc7"),
		'wc8': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv7_out, conv8_out],
		                                       stddev=np.sqrt(2.0 / conv7_out).astype(np.float32), seed=SEED),
		                   name="wc8"),
		'wc9': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv8_out, conv9_out],
		                                       stddev=np.sqrt(2.0 / conv8_out).astype(np.float32), seed=SEED),
		                   name="wc9"),
		'wc10': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv9_out, conv10_out],
		                                        stddev=np.sqrt(2.0 / conv9_out).astype(np.float32), seed=SEED),
		                    name="wc10"),
		
		'up1': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv10_out, up1],
		                                       stddev=np.sqrt(2.0 / conv10_out).astype(np.float32), seed=SEED)),
		'concat1': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up1, concat1], stddev=np.sqrt(2.0 / up1).astype(np.float32),
			                    seed=SEED)),
		
		'wc11': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat1, conv11_out],
		                                        stddev=np.sqrt(2.0 / concat1).astype(np.float32), seed=SEED),
		                    name="wc11"),
		'wc12': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv11_out, conv12_out],
		                                        stddev=np.sqrt(2.0 / conv11_out).astype(np.float32), seed=SEED),
		                    name="wc12"),
		
		'up2': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv12_out, up2],
		                                       stddev=np.sqrt(2.0 / conv12_out).astype(np.float32), seed=SEED)),
		
		'concat21': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up2, concat21], stddev=np.sqrt(2.0 / up2).astype(np.float32),
			                    seed=SEED)),
		
		'concat2': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, concat21, concat2], stddev=np.sqrt(2.0 / concat21).astype(np.float32),
			                    seed=SEED)),
		
		'wc13': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat2, conv13_out],
		                                        stddev=np.sqrt(2.0 / concat2).astype(np.float32), seed=SEED),
		                    name="wc13"),
		'wc14': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv13_out, conv14_out],
		                                        stddev=np.sqrt(2.0 / conv13_out).astype(np.float32), seed=SEED),
		                    name="wc14"),
		
		'up3': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv14_out, up3],
		                                       stddev=np.sqrt(2.0 / conv14_out).astype(np.float32), seed=SEED)),
		'concat31': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up3, concat31], stddev=np.sqrt(2.0 / up3).astype(np.float32),
			                    seed=SEED)),
		
		'concat3': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, concat31, concat3], stddev=np.sqrt(2.0 / concat31).astype(np.float32),
			                    seed=SEED)),
		
		'wc15': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat3, conv15_out],
		                                        stddev=np.sqrt(2.0 / concat3).astype(np.float32), seed=SEED),
		                    name="wc15"),
		'wc16': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv15_out, conv16_out],
		                                        stddev=np.sqrt(2.0 / conv15_out).astype(np.float32), seed=SEED),
		                    name="wc16"),
		
		'up4': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv16_out, up4],
		                                       stddev=np.sqrt(2.0 / conv16_out).astype(np.float32), seed=SEED)),
		'concat41': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up4, concat41], stddev=np.sqrt(2.0 / up4).astype(np.float32),
			                    seed=SEED)),
		
		'concat4': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, concat41, concat4], stddev=np.sqrt(2.0 / concat41).astype(np.float32),
			                    seed=SEED)),
		
		'wc17': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat4, conv17_out],
		                                        stddev=np.sqrt(2.0 / concat4).astype(np.float32), seed=SEED),
		                    name="wc17"),
		'wc18': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv17_out, conv18_out],
		                                        stddev=np.sqrt(2.0 / conv17_out).astype(np.float32), seed=SEED),
		                    name="wc18"),
		'wc19': tf.Variable(tf.truncated_normal([filter_size_out, filter_size_out, conv18_out, conv19_out],
		                                        stddev=np.sqrt(2.0 / conv18_out).astype(np.float32), seed=SEED),
		                    name="wc19"),
	}
	
	biases = {
		'bc1': tf.Variable(tf.truncated_normal([conv1_out]), name="bc1"),
		'bc2': tf.Variable(tf.truncated_normal([conv2_out]), name="bc2"),  # hy: use variable, instead fixed number
		'bc3': tf.Variable(tf.truncated_normal([conv3_out]), name="bc3"),
		'bc4': tf.Variable(tf.truncated_normal([conv4_out]), name="bc4"),
		'bc5': tf.Variable(tf.truncated_normal([conv5_out]), name="bc5"),
		'bc6': tf.Variable(tf.truncated_normal([conv6_out]), name="bc6"),
		'bc7': tf.Variable(tf.truncated_normal([conv7_out]), name="bc7"),
		'bc8': tf.Variable(tf.truncated_normal([conv8_out]), name="bc8"),
		'bc9': tf.Variable(tf.truncated_normal([conv9_out]), name="bc9"),
		'bc10': tf.Variable(tf.truncated_normal([conv10_out]), name="bc10"),
		
		'bc_up1': tf.Variable(tf.truncated_normal([up1])),
		'bc_concat1': tf.Variable(tf.truncated_normal([concat1])),
		
		'bc11': tf.Variable(tf.truncated_normal([conv11_out]), name="bc11"),
		'bc12': tf.Variable(tf.truncated_normal([conv12_out]), name="bc12"),
		
		'bc_up2': tf.Variable(tf.truncated_normal([up2])),
		'bc_concat21': tf.Variable(tf.truncated_normal([concat21])),
		'bc_concat2': tf.Variable(tf.truncated_normal([concat2])),
		
		'bc13': tf.Variable(tf.truncated_normal([conv13_out]), name="bc13"),
		'bc14': tf.Variable(tf.truncated_normal([conv14_out]), name="bc14"),
		
		'bc_up3': tf.Variable(tf.truncated_normal([up3])),
		'bc_concat31': tf.Variable(tf.truncated_normal([concat31])),
		'bc_concat3': tf.Variable(tf.truncated_normal([concat3])),
		
		'bc15': tf.Variable(tf.truncated_normal([conv15_out]), name="bc15"),
		'bc16': tf.Variable(tf.truncated_normal([conv16_out]), name="bc16"),
		
		'bc_up4': tf.Variable(tf.truncated_normal([up4])),
		'bc_concat41': tf.Variable(tf.truncated_normal([concat41])),
		'bc_concat4': tf.Variable(tf.truncated_normal([concat4])),
		
		'bc17': tf.Variable(tf.truncated_normal([conv17_out]), name="bc17"),
		'bc18': tf.Variable(tf.truncated_normal([conv18_out]), name="bc18"),
		'bc19': tf.Variable(tf.truncated_normal([conv19_out]), name="bc19"),
	}
	
	# hy: try with zero mean
	# tf.image.per_image_whitening(x)
	# this operation computes (x-mean)/adjusted_stddev
	
	#################################
	# tmp: test
	[_X, conv1, conv2, conv3, conv4, conv5, conv6, conv7, \
	 conv8, conv9, conv10, up1, conv11, conv12, up2, conv13, conv14, up3, conv15, \
	 conv16, up4, conv17, conv18, conv19, annotation_pred] \
		= conv_net(x, weights, biases, keep_prob, filter_size, filter_size_out, k_conv, k_pool, k_out, SEED)
	
	#################################
	y = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name="y")
	# logits = conv19,  labels: y
	y_int = tf.cast(y, tf.int32)
	#sparse_softmax can only take [0,3) for y and logits
	cost = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=conv19,
	                                                                      labels=tf.squeeze(y_int, squeeze_dims=[3]),
	                                                                      name="loss")))
	#conv19 = tf.reshape(conv19, shape=[None,IMAGE_SIZE,IMAGE_SIZE,3])
	
	#cost = tf.reduce_mean((tf.nn.softmax_cross_entropy_with_logits(logits=conv19,
	#																	  labels=y_int,
	#																	  name="loss")))

	#############################################################################
	# Define loss function and optimizer
	# ****************************************************************************
	# ****************************************************************************
	trainable_var = tf.all_variables()
	if optimizer_type == 'adam':
		# hy: Adam with these parameters beta1=0.9,beta2=0.999, epsilon=1e-08 etc the training
		# accuracy is not stable, epsilon = 0.01 better for these data
		print '\noptimizer:', optimizer_type, 'learning_rate:', learning_rate, '\nbeta11:', beta1, \
			'\tbeta2:', beta2, '\tepsilon:', epsilon
		# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon,
		#                                  use_locking=False, name='Adam').minimize(cost)
		# trainable_var = tf.all_variables()
		optimizer = tf.train.AdamOptimizer(learning_rate)
		grads = optimizer.compute_gradients(cost, var_list=trainable_var)
		optimize_op = optimizer.apply_gradients(grads)  # must feed optimizer... to a variable to be as tensor run in sess
	
	# hy: Adam with only learning rate as parameter can also be used to continue a training that was done previously with beta,epsilon setup
	# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # TODO change to ADAM
	if optimizer_type == 'GD':
		# hy: GradientDescentOptimizer
		print '\noptimizer:', optimizer_type, '\tlearning_rate:', learning_rate
		optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate, name="GD").minimize(cost)
	# for learning_rate_i in xrange(5):
	# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_i*(1+learning_rate_i*0.000001),name="GD").minimize(cost)
	
	if optimizer_type == 'SGD':
		print '\noptimizer:', optimizer_type, '\tlearning_rate:', learning_rate, '\tmomentum:', momentum_SGD
		optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum_SGD, name='SGD').minimize(
			cost)
	# = tf.train.MomentumOptimizer(LEARNING_RATE, MOMENTUM).minimize(cost)
	
	#################################################################
	# Build the summary operation based on the TF collection of Summaries.
	# Adding variables to be visualized
	tf.summary.scalar('Loss', cost)
	# tf.summary.image("a_ground_truth", tf.cast(y*255, tf.uint8), max_outputs=2)    #tf.cast(annotation, tf.uint8)
	tf.summary.image("a_ground_truth", tf.reshape(tf.to_float(y), shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=2)
	tf.summary.image("b_pred", tf.reshape(tf.to_float(annotation_pred * 255), shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]),
	                 max_outputs=4)
	
	#tf.summary.image('c_conv19', tf.reshape(conv19*255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, conv19_out]), max_outputs=4)
	#c_conv19_s1 = tf.slice(conv19, [0, 0, 0, 0], [0, IMAGE_SIZE, IMAGE_SIZE, 1])
	#c_conv19_s2 = tf.slice(conv19, [0, 0, 0, 0], [0, IMAGE_SIZE, IMAGE_SIZE, 2])
	tf.summary.image('c_conv19', tf.reshape(conv19*255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	#tf.summary.image('c_conv19s2', tf.reshape(c_conv19_s2*255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	tf.summary.image('c_ori_rgb', tf.reshape(x * 255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, input_ch]), max_outputs=4)
	
	def view_conv_(layers, layer_names):
		for layer, layer_name in zip(layers, layer_names):
			conv_view_size = layer.get_shape().as_list()[1]
			tf.summary.image(layer_name, tf.reshape(layer, shape=[-1, conv_view_size, conv_view_size, 1]), max_outputs=4)
	
	view_conv_([conv1, conv3, conv5, conv7, conv10],
	           ['conv1', 'conv3', 'conv5', 'conv7', 'conv10'])

	
	##############
	tf.summary.histogram('histogram_conv1w', weights['wc1'])
	tf.summary.histogram('histogram_conv10w', weights['wc10'])
	tf.summary.histogram('histogram_conv19w', weights['wc19'])
	
	summary_op = tf.summary.merge_all()
	
	return (learning_rate, dropout, dropout_1s, optimizer_type, classifier_tpye, loss_type,
	        x,y, y_int, keep_prob, optimizer, cost, summary_op, optimize_op,
	        conv19_out, conv19, annotation_pred)


import keras
import seg_net_arch as u_a
import tensorflow as tf
import numpy as np


def init_tf_var():
	x = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 3), name='x')
	y = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1), name='y')
	
	test_result = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1), name='test_result')
	conv19view = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1), name='conv19view')
	dropout = [0.15, 0.25, 0.4, 0.45, 1, 0.4, 0.25, 0.15, 0.15]
	keep_prob = tf.placeholder(tf.float32, len(dropout), name="keep_prob")
	dropout_1s = [1] * len(dropout)
	######
	tf.summary.image('original_rgb', tf.reshape(x, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 3]), max_outputs=4)
	tf.summary.image('ground_truth', tf.reshape(y, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	tf.summary.image('test_result', tf.reshape(test_result, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	##############
	summary_op = tf.summary.merge_all()
	##############
	return x, y, conv19view, test_result, keep_prob, dropout, dropout_1s, summary_op


def run_model_k(images, masks):
	keras.callbacks.History()
	epochs = 3000  # 1200
	learning_rate = 0.0002
	decay_rate = learning_rate / epochs
	momentum = 0.99
	sgd = u_a.SGD(lr=learning_rate, momentum=momentum)  # hy:decay_rate
	model, conv19 = u_a.get_model(IMAGE_SIZE, IMAGE_SIZE)  # (?, 1, 320, 320)
	
	##############
	'''
	conv19view = tf.transpose(conv19, perm=[0,2,3,1])
	print 'conv19 shape:', conv19.get_shape()
	tf.summary.image('conv19', tf.reshape(conv19, shape=[-1, 320, 320, 1]), max_outputs=4)
	batch_xs = np.transpose(images, (0, 3, 1, 2))  # -1,3, 320,320
	batch_ys = np.transpose(masks, (0, 3, 1, 2))  # -1,3, 320,320
	######
	tf.summary.image('original_color', tf.reshape(input_img, shape=[-1, 320, 320, 1]), max_outputs=4)
	#tf.summary.image("ground_truth", tf.reshape(tf.to_float(y), shape=[-1, tensor_h, tensor_w, 1]), max_outputs=2)
	#tf.summary.image('conv19_255', tf.reshape(conv19_255, shape=[-1, tensor_h, tensor_w, 1]), max_outputs=4)

	##############
	tf.summary.histogram('histogram_conv19', conv19view)
	summary_op = tf.summary.merge_all()
	##############
	train_res = sess.run(summary_op, feed_dict={'x:0': batch_xs[0], 'y:0': batch_ys[0], 'keep_prob:0': dropout_1s})
	train_writer.add_summary(train_res, train_step)
	##############
	'''
	loss_type, optimizer_type = 'binary_crossentropy', 'sgd'
	model.compile(loss='binary_crossentropy', optimizer=sgd)
	
	# images.reshape((None,1,h,w))
	# fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None,
	#    shuffle=True, class_weight=None, sample_weight=None)
	# docu example
	# weights.{epoch:02d}-{val_loss:.2f}.hdf5 # val_loss, must first define validation set
	# '/Users/Alex/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')
	
	model_path = '../testbench/seg_mul/kintf_seg/'
	save_model_name = model_path + 'weights.{epoch:02d}'  # model_path + 'test_' + '{epoch:02d}.hdf5'
	save_params = keras.callbacks.ModelCheckpoint(filepath=model_path + 'weights.{epoch:02d}.hdf5',
	                                              monitor='val_loss', verbose=2,
	                                              save_best_only=False, save_weights_only=False, mode='auto')
	
	# keras.callbacks.TensorBoard(log_dir='../Tensorboard_data/sum108/', histogram_freq=1, write_graph=True, write_images=True)
	
	class LossHistory(keras.callbacks.Callback):
		def on_train_begin(self, logs={}):
			self.losses = []
		
		def on_batch_end(self, batch, logs={}):
			self.losses.append(logs.get('loss'))
	
	# train
	history_train = model.fit(images, masks, batch_size=1, nb_epoch=epochs, callbacks=[save_params], shuffle=True)
	
	history = LossHistory()
	print history.losses
	print 'val keys', history_train.history.keys()
	
	model.save(model_path + 'model_' + '.h5')



#eva
# change logs are located in tensor_train.py
import tensorflow as tf
#from import Image
import cv2
import numpy as np
import sys
from keras.backend import set_image_dim_ordering
from keras.models import load_model
import keras
from PIL import ImageDraw
from PIL import ImageFilter
from PIL import ImageOps
import time
from functools import wraps
from random import randint
import os
import datetime
import settings  # hy: collection of global variables
import tools_classifier_seg as tools
import time
from sklearn import datasets
import math
import imutils
from PIL import Image  # hy: create video with images
# from keras.models import Model
# from keras.callbacks import ModelCheckpoint, LearningRateScheduler
#import seg_narch as u_a
import re
# from background_learning_s import dice_coef_loss
# tf.python.control_flow_ops = tf #hy:for remote
# KERAS_BACKEND=tensorflow python -c "from keras import backend"
# Using TensorFlow backend.
#####################################################################################################
settings.set_global()
start_time = time.time()
PROJ_DIR = '/home/haiyan/Documents/MA/'

FLAGS = tf.flags.FLAGS
tf.flags.DEFINE_string('seg_model_search_p', PROJ_DIR+'testbench/tf_seg_model/', "path to seg model file")
tf.flags.DEFINE_string('class_model_search_p', PROJ_DIR+'logs/', "path to classifier model file")
tf.flags.DEFINE_string('save_res_path', PROJ_DIR + 'testbench/tf_imgs/', "path to save res images")

tf.flags.DEFINE_bool('with_gt', "True", "test with ground truth: True/ False") #
tf.flags.DEFINE_bool('CLOSE_ALL', "False", "test with ground truth: True/ False") #
tf.flags.DEFINE_bool('INFO_0', "False", "print Info level 0 on: True/ False")
tf.flags.DEFINE_bool('DEBUG', "False", "print Info level 0 on: True/ False")
tf.flags.DEFINE_string('mode', "visualize", "Mode: test/ visualize")

do_multiple_test = False  # this can only be started with prepared shell script eva.sh
############################################################
settings.set_global()
start_time = time.time()

# check
# add_statistic_page
# close_all
# out comment all segmodel
# save pretensor

if do_multiple_test:
	# use setup in eva.sh
	Seg_MODEL_to_load = sys.argv[1]  # + ".meta"
	print 'model load from shell:', Seg_MODEL_to_load
	if sys.argv[2] == '1':
		use_cut_bounding_box = True
	else:
		use_cut_bounding_box = False
	
	if sys.argv[3] == '1':
		use_seg_limit = True
	else:
		use_seg_limit = False
	MODEL_ID = sys.argv[4]
	save_res_path = sys.argv[5]
	SAVE_MUL_RES_PATH = sys.argv[5]
	print 'model id:', MODEL_ID

else:
	use_cut_bounding_box = True  # cut or seg cut
	use_seg_limit = False
	MODEL_ID = 17

NUM_OF_CLASSESS = 2 #255#151
do_6face_classification = True
Seg_MODEL_to_load = 'model_sigmch3conv19_2_II_avg0.58-28000' + '.meta' #'testbench/tf_seg_model/'

EVA_IMAGE_SEG_TF = 1
EVALUATE_WEBCAM_seg_tf_and_classify = 0
EVA_IMAGE_SEG_TF_cat = 0
EVALUATE_VIDEO_  = 0


generate_train_data = False
Img_255 = False #False- mul, True- bin
save_ori_frame = True
thresh_res = 29 #
search_str = 'cad'
in_ch = 1
border = 0
result_for_table = False

classifier_model = "../testbench/6classes/" + "model-12811.meta"  #
n_hidden = 360  # n_hidden = classifier_model.split('conv')[1][0:3]
#n_hidden = 128*4  # n_hidden = classifier_model.split('conv')[1][0:3]
do_active_fields_test = 0
#dropout = [0.3, 0.3, 0.5, 0.5]  # 3,4,5,5
dropout = [0.15, 0.25, 0.4, 0.5, 1, 0.4, 0.25, 0.15, 0.15]  # 1st
#dropout = [0.3, 0.3, 0.5, 0.5, 0.5]  # 3,4,5,5
# dropout = [0.3]  # 3,4,5,5
dropout_1s = [1] * len(dropout)

##########
result_for_table = 1
SAVE_Misclassified = 0
SAVE_CorrectClassified = 0

###########
LABEL_LIST = '../FileList.txt'
LABEL_PATH = settings.data_label_path
# LABEL_LIST_TEST = '../FileList_TEST1_sing.txt'
LABEL_LIST_TEST = '../FileList_TEST1.txt'
# LABEL_LIST_TEST = '../FileList_TEST.txt'
LABEL_PATH_TEST = settings.test_label_path8

######################################################################################################
def get_test_batch_im_m(h, w):
	data_path = '../Test_Images/MA/test_represent/'  # main_path
	
	print 'train_path:', data_path
	#####################################################################################
	read_path_im = data_path + 'images/'
	read_path_m = data_path + 'masks/'
	files_im = sorted([s for s in os.listdir(read_path_im)])
	files_m = sorted([s for s in os.listdir(read_path_m)])
	
	max_num = len(files_im)  # 3
	images, masks, images_view = tools.import_data_segnet_2c_tf_misc_cv2_gray(read_path_im, read_path_m, files_im,
	                                                                          files_m, h, w,
	                                                                          max_num, MUL=False,
	                                                                          do_Flipping=False)
	#####################################################################################
	
	dummy_mask = np.ones((1, h * w))
	read_paths_im, read_paths_m = [], []
	for i in xrange(len(images)):
		read_paths_im.append(read_path_im)
		read_paths_m.append(read_path_m)
	
	return read_paths_im, read_paths_m, files_im, files_m, images, masks, dummy_mask, images_view


# return images, masks, dummy_mask, data_1s, m_1s, images_view
# read_paths_im, read_paths_m, files_im, files_m

# Load TEST SET
def get_test_batch_im_m_6cl(h, w):
	data_path = '../Test_Images/MA/test_represent/'  # main_path
	
	print 'train_path:', data_path
	#####################################################################################
	# folders = ['rechts/']
	
	total_files_im, total_files_m = [], []
	all_read_path_im, all_read_path_m = [], []
	
	for folder in folders:
		read_path_im = PROJ_DIR + '/Test_Images/MA/test_represent/im/' + folder
		read_path_m = PROJ_DIR + '/Test_Images/MA/test_represent/m/' + folder
		
		#read_path_m = read_path_im
		
		files_im = sorted([s for s in os.listdir(read_path_im) ])  # if 'offi' in s])
		files_m = sorted([s for s in os.listdir(read_path_m) ])  # if 'offi' in s])
		
		# files_im = sorted([s for s in os.listdir(read_path_im) if 'offi' in s]) #set scene
		# files_m = sorted([s for s in os.listdir(read_path_m) if 'offi' in s])
		
		total_files_im = total_files_im + files_im
		total_files_m = total_files_m + files_m
		
		for i in xrange(len(files_im)):
			all_read_path_im.append(read_path_im)
			all_read_path_m.append(read_path_m)
		
		
		# max_num = len(files_im)  # 3
		# images, masks, images_view = tools.import_data_segnet_2c_tf_misc_cv2_gray(read_path_im, read_path_m, files_im, files_m, h, w,
		# max_num, MUL=False,do_Flipping=False)
		# images = np.concatenate((images, images), axis=0)
		# masks = np.concatenate((masks, masks), axis=0)
		# i#mages_view = np.concatenate((images_view, images_view), axis=0)
	
	#####################################################################################
	
	dummy_mask = np.ones((1, h * w))
	return all_read_path_im, all_read_path_m, total_files_im, total_files_m, dummy_mask

def get_read_path_and_files_for_im(h,w,read_from_file=False):
	if read_from_file:
		files = []

		file_with_gt = False
		if file_with_gt:
			# read file containing filename and ground truth data
			with open('../Test_Images/full_frames_sun/seg_ground_truth/pass_single_gt.txt', 'r') as f:
				lines = [r.split()[0] for r in f]
		else:
			lines = open(
				'../Test_Images/img_list/fail_full_frames_clear_w_table_patches_136top.txt').read().splitlines()

		read_path = os.path.dirname(lines[0]) + '/'
		print 'read_path', read_path
		for line in lines:
			files.append(os.path.basename(line))

	else:
		read_path = '../Test_Images/'
		files = [s for s in os.listdir(read_path) if '.jpg' in s]
	
	read_paths_im = []
	for i in xrange(len(files)):
		read_paths_im.append(read_path)
		
	# for im in res_fail_list:
	#	files.append(read_path + im)
	dummy_mask = np.ones((1, h * w))
	print 'num of files:', len(files), ',  files[0]:', files[0]
	return read_paths_im, files,dummy_mask

def demo_result_imgs(file1, file2, file3, frame_i=1, save_file=False, demo=False):
	if save_file:
		cv2.imwrite('../testbench/frame_res_' + Seg_MODEL_to_load[:-5] + '%03d.jpg' % frame_i, np.uint8(file1))
		cv2.imwrite("../testbench/frame_combined_%03d.jpg" % frame_i, file2)
		cv2.imwrite("../testbench/frame_color_%03d.jpg" % frame_i, file3)
	if demo:
		#title = 'direct_output_' + str(frame_i)
		title = 'direct_output'
		cv2.namedWindow(title, cv2.WINDOW_AUTOSIZE)
		# cv2.namedWindow('result_win', cv2.WINDOW_NORMAL)
		cv2.putText(np.uint8(file1), 'No. ' + str(frame_i), org=(320 / 10, 320 / 8),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1,
		            color=(0, 255, 255), thickness=2)
		cv2.imshow(title, np.uint8(file1))
		#print 'seg_feature uint8\n',np.uint8(file1)
	# debug
	# cv2.imshow('combined',file2)
	# cv2.imshow('color',file3)
	# cv2.waitKey(20)

def get_bounding_box(conture, img=None, draw_box=False):
	""" Calculates the bounding box of a ndarray"""
	# get approx, return index
	# epsilon = 0.1 * cv2.arcLength(x, True)
	# approx_box = cv2.approxPolyDP(x, epsilon, True)
	# print 'app box', approx_box  # Min [[[ 56  85]]  [[318 231]]]
	# leftpointX = approx_box[0][0][0]
	# print 'app box 2', leftpointX  # Min [[[ 56  85]] Max [[318 231]]]
	# approx_box_s = int(0.9*approx_box)
	# print 'app box s',approx_box_s
	
	# get rectangle
	x, y, w, h = cv2.boundingRect(conture)  # x,y: top-left coordinate
	# draw rectangle
	if draw_box:
		cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
	cv2.waitKey(2)
	return (x, y, w, h)

#for more contures
def get_bounding_merged_box(contures, img=None):
	""" Calculates the bounding box of a ndarray"""
	# get approx, return index
	# epsilon = 0.1 * cv2.arcLength(x, True)
	# approx_box = cv2.approxPolyDP(x, epsilon, True)
	# print 'app box', approx_box  # Min [[[ 56  85]]  [[318 231]]]
	# leftpointX = approx_box[0][0][0]
	# print 'app box 2', leftpointX  # Min [[[ 56  85]] Max [[318 231]]]
	# approx_box_s = int(0.9*approx_box)
	# print 'app box s',approx_box_s
	
	# get rectangle
	x, y, w, h = cv2.boundingRect(contures)  # x,y: top-left coordinate
	# draw rectangle
	cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
	cv2.waitKey(10)
	return (x, y, w, h)

def get_bounding_box_with_limit_old(im_cv):
	im_pil = cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB)
	im = Image.fromarray(im_pil)
	im = im.convert('LA')  # convert Image object image to gray
	pix = im.load()
	w, h = im.size
	print 'w,h:', w, h
	wh_count, t, b, l, r = 0, 0, 0, 0, 0
	ts, bs, ls, rs = [], [], [], []
	t_counts, b_counts, l_counts, r_counts = [], [], [], []
	limit = 1
	set_break = False  # t
	for y in xrange(h):
		wh_count = 0
		for x in xrange(w):
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				t_counts.append(wh_count)
	
	limit = np.median(sorted(t_counts))
	print 't_median t', limit
	for y in xrange(h):
		wh_count = 0
		if set_break:
			break
		for x in xrange(w):
			# print pix[x,y]
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				t = y
				set_break = True
				break
	
	limit = 1
	for y in xrange(h):
		wh_count = 0
		for x in xrange(w):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				b_counts.append(wh_count)
	
	limit = np.median(sorted(b_counts))
	print 'limit b', limit
	# b_mean = np
	set_break = False  # b
	for y in xrange(h):
		wh_count = 0
		if set_break:
			break
		for x in xrange(w):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				b = h - y - 1
				set_break = True
				break
	
	###################################
	limit = 1
	for x in xrange(w):
		wh_count = 0
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				l_counts.append(wh_count)
	limit = np.median(sorted(l_counts))
	
	set_break = False  # l
	for x in xrange(w):
		wh_count = 0
		if set_break:
			print 'x:', wh_count
			break
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				l = x
				print wh_count
				set_break = True
				break
	
	################################
	limit = 1
	for x in xrange(w):
		wh_count = 0
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				r_counts.append(wh_count)
	
	limit = np.median(sorted(r_counts))
	
	set_break = False  # r
	for x in xrange(w):
		wh_count = 0
		if set_break:
			break
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				r = w - x - 1
				set_break = True
				break
	
	print 't,b,l,r', t, ',', b, ',', l, ',', r
	x, y, w, h = l, t, (r - l), (b - t)
	return x, y, w, h

def get_roi_in_white_bg_seg(roi, base_img, w, h, factor=1):
	roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
	roi_PIL = Image.fromarray(roi_gray)
	datas_obj = roi_PIL.getdata()
	
	fr = cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB)
	fr_PIL = Image.fromarray(fr)
	datas_fr = fr_PIL.getdata()
	
	new_im = Image.new("RGB", (w, h))  ## default black!
	new_im.paste((255, 255, 255), (0, 0, w, h))  ##
	datas_roi = new_im.convert("RGBA")
	
	newData = []
	
	def white(data_obj):
		if data_obj[0] == 255 and data_obj[1] == 255 and data_obj[2] == 255:
			white = True
		else:
			white = False
		return white
	
	#debug
	#print 'datas_obj format', datas_obj.size
	
	# way 1
	for data_obj, data_fr in zip(datas_obj, datas_fr):
		if white(data_obj):
			newData.append((data_fr[0], data_fr[1], data_fr[2]))
		else:
			newData.append((255, 255, 255))
		
		## convert PIL back to CV2
	datas_roi.putdata(newData)
	pil_image = datas_roi.convert('RGB')
	open_cv_image = np.array(pil_image)
	# Convert RGB to BGR
	open_cv_image = open_cv_image[:, :, ::-1].copy()
	
	'''
	if datas_obj.size > 0:
		largest_areas = sorted(contours, key=cv2.contourArea)
		x, y, box_w, box_h = get_bounding_box(largest_areas[-1], open_cv_image)  # x,y:top-left coord
		if box_w > 30 and box_h > 30:
			screen_out = True
			print 'x, y, w, h', x, y, box_w, box_h
			box_h = int(0.99 * box_h)
			box_w = int(0.99 * box_w)
			open_cv_image = open_cv_image[y:y + box_h, x:x + box_w]  # [crop_y1:crop_y2, crop_x1:crop_x2]
			open_cv_image = cv2.resize(np.uint8(open_cv_image), (h, w))
		else:
			screen_out = False
	else:
		screen_out = False
'''
	screenout = True
	return open_cv_image, screenout

def get_roi_with_white_bg_cut_old(roi, base_img, w=184, h=184, factor=1):
	overlay = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
	if FLAGS.INFO_0:
		print 'get_white_cut,size',roi.shape,'base size',base_img.shape
	roi_PIL = Image.fromarray(overlay)
	# datas_obj = roi_PIL.getdata()
	
	fr = cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB)
	fr_PIL = Image.fromarray(fr)
	
	new_im = Image.new("RGB", (w, h))  ## default black!
	new_im.paste((255, 255, 255), (0, 0, w, h))  ##
	
	# time for cut 5.85256004333 start from here
	def white2(p_col, p_row):
		counter_point_white_col = (p_col == [255, 255, 255]).sum()
		counter_point_white_row = (p_row == [255, 255, 255]).sum()
		return counter_point_white_col, counter_point_white_row
	
	# print 'datas_obj format', datas_obj.size
	
	# http://effbot.org/zone/pil-pixel-access.htm
	out = Image.new(roi_PIL.mode, roi_PIL.size, None)  # w h
	p_roi = np.asarray(roi)  # h w ch
	
	# print 'p31 shape', p31[0][0][0], 'shp2', p31[0][0][1], 'shp point', p31[0][0]
	
	in_pixel = fr_PIL.load()
	out_pixel = out.load()
	tmp_time0 = time.time()
	
	for col in xrange(w):
		for row in xrange(h):
			if row > 1079:
				print 'h',row
			thresh1, thresh2 = white2(p_roi[row, 0:h - 1], p_roi[0:w - 1, col])
			if thresh1 > 10 * 11 and thresh2 > 10 * 11:
				out_pixel[col, row] = in_pixel[col, row]
			
			else:
				# out_pixel[col, row] = (0, 0, 0)
				out_pixel[col, row] = (255, 255, 255)
	
	# cv2.waitKey(5900)
	# time for cut 0.00150513648987 start from here
	data_roi = out
	pil_image = data_roi.convert('RGB')
	open_cv_image = np.array(pil_image)
	# Convert RGB to BGR
	open_cv_image = open_cv_image[:, :, ::-1].copy()
	tmp_time3 = time.time()
	if FLAGS.INFO_0:
		print 'time for cut', tmp_time3 - tmp_time0
	
	################
	gray = cv2.cvtColor(open_cv_image, cv2.COLOR_BGR2GRAY)
	ret, gray = cv2.threshold(gray, 254, 255, cv2.THRESH_BINARY_INV)
	###############
	
	contours, hierarchy = cv2.findContours(gray, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
	if len(contours) > 0:
		largest_areas = sorted(contours, key=cv2.contourArea)
		x, y, box_w, box_h = get_bounding_box(largest_areas[-1], open_cv_image)  # x,y:top-left coord
		if box_w > 30 and box_h > 30:
			screen_out = True
			if FLAGS.INFO_0:
				print 'x, y, w, h', x, y, box_w, box_h
			box_h_s = int(0.9 * box_h)  # smaller region
			box_w_s = int(0.9 * box_w)
			open_cv_image = open_cv_image[y:y + box_h_s, x:x + box_w_s]  # [crop_y1:crop_y2, crop_x1:crop_x2]
			open_cv_image = cv2.resize(np.uint8(open_cv_image), (h, w))
		else:
			screen_out = False
	else:
		screen_out = False
	return open_cv_image, screen_out

def get_bounding_box_with_limit(bin_im_cv, draw_box=False):
	im_pil = bin_im_cv.copy()
	if len(bin_im_cv.shape) == 3:
		im_pil = cv2.cvtColor(bin_im_cv, cv2.COLOR_BGR2RGB)
	im = Image.fromarray(im_pil)
	im = im.convert('LA')  # convert Image object image to gray
	pix = im.load()
	w, h = im.size
	if FLAGS.INFO_0:
		print 'w,h:', w, h
	wh_count, t, b, l, r = 0, 0, 0, 0, 0
	ts, bs, ls, rs = [], [], [], []
	t_counts, b_counts, l_counts, r_counts = [], [], [], []
	limit = 1
	set_break = False  # t
	for y in xrange(h):
		wh_count = 0
		for x in xrange(w):
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				t_counts.append(wh_count)
	
	limit = np.median(sorted(t_counts))
	if FLAGS.INFO_0:
		print 't_median t', limit
	for y in xrange(h):
		wh_count = 0
		if set_break:
			break
		for x in xrange(w):
			# print pix[x,y]
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				t = y
				set_break = True
				break
	
	limit = 1
	for y in xrange(h):
		wh_count = 0
		for x in xrange(w):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				b_counts.append(wh_count)
	
	limit = np.median(sorted(b_counts))
	if FLAGS.DEBUG:
		print 'limit b', limit
	# b_mean = np
	set_break = False  # b
	for y in xrange(h):
		wh_count = 0
		if set_break:
			break
		for x in xrange(w):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				b = h - y - 1
				set_break = True
				break
	
	###################################
	limit = 1
	for x in xrange(w):
		wh_count = 0
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				l_counts.append(wh_count)
	limit = np.median(sorted(l_counts))
	
	set_break = False  # l
	for x in xrange(w):
		wh_count = 0
		if set_break:
			if FLAGS.DEBUG:
				print 'x:', wh_count
			break
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[x, y] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				l = x
				if FLAGS.DEBUG:
					print wh_count
				set_break = True
				break
	
	################################
	limit = 1
	for x in xrange(w):
		wh_count = 0
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				r_counts.append(wh_count)
	
	limit = np.median(sorted(r_counts))
	
	set_break = False  # r
	for x in xrange(w):
		wh_count = 0
		if set_break:
			break
		for y in xrange(h):
			# print pix[w-x-1,h-y-1]
			if pix[w - x - 1, h - y - 1] != (255, 255):
				wh_count += 1
			if wh_count > limit:
				r = w - x - 1
				set_break = True
				break
	
	if FLAGS.DEBUG:
		print 't,b,l,r', t, ',', b, ',', l, ',', r
	x, y, bw, bh = l, t, (r - l), (b - t)
	if draw_box:
		cv2.rectangle(bin_im_cv, (x, y), (x + w, y + h), (0, 255, 0), 2)
	return x, y, bw, bh

def get_reversed_mask(roi_bin, base_img, w, h, factor=1):
	roi_gray = cv2.cvtColor(roi_bin, cv2.COLOR_BGR2RGB)
	roi_PIL = Image.fromarray(roi_gray)
	datas_obj = roi_PIL.getdata()
	
	fr = cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB)
	fr_PIL = Image.fromarray(fr)
	datas_fr = fr_PIL.getdata()
	
	new_im = Image.new("RGB", (w, h))  ## default black!
	new_im.paste((255, 255, 255), (0, 0, w, h))  ##
	datas_roi = new_im.convert("RGBA")
	
	newData = []
	
	def white(data_obj):
		if data_obj[0] == 255 and data_obj[1] == 255 and data_obj[2] == 255:
			white = True
		else:
			white = False
		return white
	
	# debug
	# print 'datas_obj format', datas_obj.size
	
	# way 1
	for data_obj, data_fr in zip(datas_obj, datas_fr):
		if white(data_obj):
			newData.append((data_fr[0], data_fr[1], data_fr[2]))
		else:
			newData.append((255, 255, 255))
		
		## convert PIL back to CV2
	datas_roi.putdata(newData)
	pil_image = datas_roi.convert('RGB')
	open_cv_image = np.array(pil_image)
	# Convert RGB to BGR
	open_cv_image = open_cv_image[:, :, ::-1].copy()
	return open_cv_image

def get_roi_with_white_bg_cut(roi_bin, base_img, w, h, cont,factor=1,set_limit=True):
	#cv2.imwrite('../roi.png',roi)
	ret,mask = cv2.threshold(roi_bin,254,255,cv2.THRESH_BINARY_INV)
	if set_limit:
		x, y, box_w, box_h = get_bounding_box_with_limit(mask,draw_box=False)  # x,y:top-left coord mask
	else:
		x, y, box_w, box_h = get_bounding_box(cont,draw_box=False)  # x,y:top-left coord mask

	if FLAGS.DEBUG:
		print 'get_white_cut,size',roi_bin.shape,'base size',base_img.shape
	if box_w > 3 and box_h > 3:
		screen_out = True
		if FLAGS.INFO_0:
			print 'x, y, w, h', x, y, box_w, box_h
	else:
		screen_out = False
		x, y, box_w, box_h = 0, 0, 0, 0
	return x, y, box_w, box_h, screen_out

def ROI(pred_thresh, im_crop, w, h, im_i=0, save_file=False):  # obj_area size 320x320 #(roi)
	# input
	fr = im_crop.copy()
	im_crop_rz = cv2.resize(im_crop, (h, w))
	blackbg = np.zeros((w, h, 3), np.uint8)
	whitebg = np.zeros((w, h, 3), np.uint8)
	whitebg.fill(255)
	new_mask = np.zeros((w, h, 3), np.uint8)
	
	###############
	def find_contour(obj_area, thresh):
		gray = cv2.resize(np.uint8(obj_area), (h, w))  # direct
		ret, gray = cv2.threshold(gray, thresh, 255, 0)
		contours, hierarchy = cv2.findContours(gray, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)  # 1
		if len(contours) > 0:
			screen_out = True
		else:
			screen_out = False
		return contours, screen_out
	
	def find_contour_mul(obj_area):
		contours, screen_out = find_contour(obj_area, 10)  # 150 #current: 50
		if not screen_out:
			if FLAGS.INFO_0:
				print 'try with another threshold: 15'
			contours, screen_out = find_contour(obj_area, 3)  # current: 15
			if not screen_out:
				if FLAGS.INFO_0:
					print 'try with another threshold: 2'
				contours, screen_out = find_contour(obj_area, 2)
		return contours, screen_out
	
	#########################################################
	contours, screen_out = find_contour(pred_thresh, thresh_res)  # 160/255 = 0.62
	
	if screen_out:
		# debug
		# time for contour 0.000312089920044
		fr_add_cont = cv2.resize(np.uint8(fr), (h, w))  # 0,1
		fr_ori_int8 = fr_add_cont.copy()  # 0,1
		roi_res = im_crop_rz.copy()
		new_roi_res = im_crop_rz.copy()
		######################################
		largest_areas = sorted(contours, key=cv2.contourArea)  # 1
		# an operation, returns fr - masked with the largest contour
		# create a black image for showing white contour. with it a clear bounding box
		# can be created safely
		
		cv2.drawContours(fr_add_cont, [largest_areas[-1]], 0, (255, 255, 255, 255), -1)  # 1
		cv2.drawContours(blackbg, [largest_areas[-1]], 0, (255, 255, 255, 255), -1)  # 1
		
		# alternative way: feed the learned segment to classification model
		# way 2 and 3: cut or seg
		# time for cut contour, get res 5.75804781914
		# current best: use cut,limit,
		# both methods cut and seg have same mask, only cut has full bg within bounding box, seg has seg bg
		old_roi_res = roi_res
		if use_cut_bounding_box:
			
			x, y, bw, bh, screen_out = get_roi_with_white_bg_cut(blackbg, fr_ori_int8, w, h, largest_areas[-1], 1,
			                                                     set_limit=use_seg_limit)
			if screen_out:
				r1, r2, c1, c2 = y, y + bh, x, x + bw
				new_roi_res = roi_res[r1:r2, c1:c2]
				#new_mask[r1:r1 + bh, c1:c1 + bw, :] = blackbg[r1:r2, c1:c2]  # better
				use_border = True #tmp
				if use_border:
					new_mask[r1-border:r1 + bh, c1:c1 + bw, :] = blackbg[r1-border:r2, c1:c2]  # try
					new_roi_res = roi_res[r1-border:r2, c1:c2]
					
				whitebg[r1-border:r1 + bh, c1:c1 + bw, :] = new_roi_res #try
				#whitebg[r1:r1 + bh, c1:c1 + bw, :] = new_roi_res # fixed
				if FLAGS.DEBUG:
					cv2.imshow('new_roi', roi_res)
					cv2.imshow('old_mask', blackbg)
					cv2.imshow('new_mask', new_mask)
			else:
				x, y, bw, bh = 0, 0, 0, 0
				r1, r2, c1, c2 = y, y + bh, x, x + bw
			# new_roi_res = im_crop_rz.copy()  # no roi
		
		else:
			# roi_res = get_reversed_mask(blackbg, fr_ori_int8, w, h, 1)  # roi, base_img, w, h, factor=1
			roi_res = get_reversed_mask(blackbg, fr_ori_int8, w, h, 1)  # roi, base_img, w, h, factor=1
			
			# print 'shape:', roi_res.shape
			
			if not use_seg_limit:
				
				x, y, bw, bh = get_bounding_box(largest_areas[-1], blackbg, draw_box=False)
				if bw > 3 and bh > 3:
					screen_out = True
					r1, r2, c1, c2 = y, y + bh, x, x + bw
					old_roi_res = roi_res[r1:r2, c1:c2]
					new_roi_res = old_roi_res.copy()
					new_mask = blackbg
					whitebg = roi_res[r1:r2, c1:c2]
				
				else:
					screen_out = False  # no roi
					x, y, bw, bh = 0, 0, 0, 0
					r1, r2, c1, c2 = y, y + bh, x, x + bw
					old_roi_res = roi_res
					new_roi_res = old_roi_res.copy()
					new_mask = blackbg
				# whitebg = roi_res #remain white
			
			if use_seg_limit:
				screen_out = False
				x, y, bw, bh = get_bounding_box_with_limit(roi_res, draw_box=False)
				if bw > 3 and bh > 3:
					screen_out = True
					r1, r2, c1, c2 = y, y + bh, x, x + bw
					new_roi_res = roi_res[r1:r2, c1:c2]
					new_mask[r1:r1 + bh, c1:c1 + bw, :] = blackbg[r1:r2, c1:c2]
					whitebg[r1:r1 + bh, c1:c1 + bw, :] = roi_res[r1:r2, c1:c2]
				
				else:
					x, y, bw, bh = 0, 0, 0, 0
					r1, r2, c1, c2 = y, y + bh, x, x + bw
					old_roi_res = roi_res
					new_roi_res = old_roi_res.copy()
					new_mask = blackbg
				# whitebg = roi_res.copy() #remain white
			# else:
			#	new_roi_res = old_roi_res
			#	new_mask = blackbg[r1:r2, c1:c2]
			
			if FLAGS.DEBUG:
				cv2.imshow('old_roi', old_roi_res)
				cv2.imshow('new_roi', new_roi_res)
				cv2.imshow('old_mask', blackbg)
				cv2.imshow('new_mask', new_mask)
	
	else:
		print 'no contour found (1)'
		screen_out = False
		old_roi_res = np.zeros((w, h, 3), np.uint8)
		#old_roi_res.fill(255)
		new_roi_res = np.zeros((w, h, 3), np.uint8)  # no roi
		new_roi_res.fill(255) #full white, i.e. empty
		new_mask = blackbg #full black
		fr_add_cont = im_crop_rz.copy()  # no conture overlay
		x, y, bw, bh = 0, 0, 0, 0
		r1, r2, c1, c2 = y, y + bh, x, x + bw
	
	# roi_res[:] = (0, 0, 255) #fill white
	# time for contour,add mask 0.00493407249451
	# Get bounding box and smaller region from that
	#####################################################################
	# cv2.waitKey(10)
	# cv2.imshow('roi_final',roi_res)
	# new_roi_res: ori size obj of interest with white background
	old_mask = blackbg
	roi_whitebg = whitebg.copy()
	return fr_add_cont, old_mask, old_roi_res, new_mask, new_roi_res, r1, r2, c1, c2, screen_out, roi_whitebg

# display
def get_classify_result(sess, test_image, test_labels, im_i, frame, frame_crop,
                        crop_y1, crop_y2, crop_x1, crop_x2, border, screen_out=False, fname='', target=0):
	# print 'frame shape', frame.shape[0], frame.shape[1]
	######################### Tensorflow
	if not result_for_table:
		print '\nNo.', im_i + 1, ' file:', fname
	# test_image= test_image[crop_y1:crop_y2,crop_x1:crop_x2]
	
	batch_xs, batch_ys = test_image, test_labels
	
	crop_y1, crop_y2, crop_x1, crop_x2 = int(frame_crop.shape[0] * crop_y1 / 320), int(
		frame_crop.shape[0] * crop_y2 / 320), \
	                                     int(frame_crop.shape[1] * crop_x1 / 320), int(
		frame_crop.shape[1] * crop_x2 / 320)
	
	# print 'batch_xs, batch_ys:', batch_xs, ', ', batch_ys
	dropout_cl = [0, 0, 0, 0]
	dropout_1s_cl = [1] * len(dropout_cl)
	
	output = sess.run("pred:0", feed_dict={"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s_cl})
	
	# print("Output for external=",output)
	output = tools.convert_to_confidence(output)  #
	np.set_printoptions(precision=3)
	if FLAGS.DEBUG:
		print 'output', output
	rank_outputs = sorted(range(len(settings.LABELS)), key=output[0].__getitem__)
	
	RES = np.argmax(output)  # hy index starting from 0, 0-5 corresponding predicted label 1-6
	
	label_pred_str = settings.LABELS_en[RES][:-1]
	if target == RES:
		# print 'RES:', RES, 'target', target
		tp_color = (0, 0, 255)
	else:
		tp_color = (0, 0, 0)
	label_pred_str2 = settings.LABELS_en[rank_outputs[-2]][:-1]
	label_pred_str3 = settings.LABELS_en[rank_outputs[-3]][:-1]
	label_pred_str4 = settings.LABELS_en[rank_outputs[-4]][:-1]
	label_pred_str5 = settings.LABELS_en[rank_outputs[-5]][:-1]
	label_pred_str6 = settings.LABELS_en[rank_outputs[-6]][:-1]
	
	prob_str = str('{:.4f}'.format(output[0][RES]))
	prob_str2 = str('{:.4f}'.format(output[0][rank_outputs[-2]]))
	prob_str3 = str('{:.4f}'.format(output[0][rank_outputs[-3]]))
	prob_str4 = str('{:.4f}'.format(output[0][rank_outputs[-4]]))
	prob_str5 = str('{:.4f}'.format(output[0][rank_outputs[-5]]))
	prob_str6 = str('{:.4f}'.format(output[0][rank_outputs[-6]]))
	
	if FLAGS.INFO_0:
		print prob_str2, prob_str3, prob_str4, prob_str5, prob_str6
	
	# hy: for sub-classes
	# label_pred_str, label_pred_num = tools.convert_result(RES) # hy use it when sub-classes are applied
	# RES_sub_to_face = class_label #hy added
	# print "target, predict =", target, ', ', RES  # hy
	frame = frame_crop
	if FLAGS.DEBUG:
		print 'frame shape(classify demo):', frame.shape
	if frame.shape[0] > 330:
		frontsize, frontsize_no, frontsize_stat, thickness, thickness_no = 2, 1, 1.5, 4, 2
	else:
		frontsize, frontsize_no, frontsize_stat, thickness, thickness_no = 0.4, 0.5, 1.5, 1, 1
	
	if do_6face_classification:  ##---
		print 'No.', im_i + 1, fname, target, label_pred_str, prob_str, label_pred_str2, prob_str2, label_pred_str3, \
			prob_str3, label_pred_str4, prob_str4, label_pred_str5, prob_str5, label_pred_str6, prob_str6
	demo_window_width = 600  # 600 * 1080 / 1920
	add_statistic_page = False
	stat = np.zeros((600, 600, 3), np.uint8)
	stat.fill(255)
	txt_col1 = 13  # int(demo_window_width*0.022)
	txt_col2 = 93  # int(demo_window_width* 0.155)
	wait_sec = 2
	if screen_out and not FLAGS.CLOSE_ALL:
		cv2.rectangle(frame, (crop_x1, crop_y1), (crop_x2, crop_y2), color=(0, 255, 0), thickness=thickness_no)
		# frame_demo = imutils.resize(frame, width=600)
		cv2.putText(frame, "predicted top 1: " + label_pred_str + ' confid.:' + prob_str,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.1)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(0, 255, 0), thickness=thickness)
		if add_statistic_page:  # int(demo_window_width * 0.1)
			cv2.putText(stat, "1: " + label_pred_str,
			            org=(txt_col1, 60),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=tp_color, thickness=thickness)
			cv2.putText(stat, prob_str,
			            org=(txt_col2, 60),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.waitKey(wait_sec)
		
		###########################################################################################################
		cv2.putText(frame, "predicted top 2: " + label_pred_str2 + ' confid.:' + prob_str2,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.20)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		if add_statistic_page:
			cv2.putText(stat, "2: " + label_pred_str2,
			            org=(txt_col1, 120),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.putText(stat, prob_str2,
			            org=(txt_col2, 120),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.waitKey(wait_sec)
		
		###########################################################################################################
		cv2.putText(frame, "predicted top 3: " + label_pred_str3 + ' confid.:' + prob_str3,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.25)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		
		if add_statistic_page:
			cv2.putText(stat, "3: " + label_pred_str3,
			            org=(txt_col1, 150),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.putText(stat, prob_str3,
			            org=(txt_col2, 150),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.waitKey(wait_sec)
		
		###########################################################################################################
		
		cv2.putText(frame, "predicted top 4: " + label_pred_str4 + ' confid.:' + prob_str4,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.30)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		if add_statistic_page:
			cv2.putText(stat, "4: " + label_pred_str4,
			            org=(txt_col1, 180),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.putText(stat, prob_str4,
			            org=(txt_col2, 180),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			
			cv2.waitKey(wait_sec)
		
		###########################################################################################################
		cv2.putText(frame, "predicted top 5: " + label_pred_str5 + ' confid.:' + prob_str5,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.35)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		if add_statistic_page:
			cv2.putText(stat, "5: " + label_pred_str5,
			            org=(txt_col1, 210),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.putText(stat, prob_str5,
			            org=(txt_col2, 210),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.waitKey(wait_sec)
		
		###########################################################################################################
		cv2.putText(frame, "predicted top 6: " + label_pred_str6 + ' confid.:' + prob_str6,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.4)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		if add_statistic_page:
			cv2.putText(stat, "6: " + label_pred_str6,
			            org=(txt_col1, 240),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.putText(stat, prob_str6,
			            org=(txt_col2, 240),
			            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_stat, color=(0, 0, 0), thickness=thickness)
			cv2.waitKey(wait_sec)
		
		# print 'video.get',str(video.get(1))
		# frame.shape[1] *9/ 12 for w, frame.shape[0] *11/ 12 for h
		
		frame_demo = imutils.resize(frame,
		                            width=demo_window_width)  # cannot use frame because it should be reserved for receiving next input
		demo_window_height = int(demo_window_width * frame.shape[0] / frame.shape[1])
		if FLAGS.DEBUG:
			print 'demo height', demo_window_height
		cv2.putText(frame_demo, 'No. ' + str(im_i + 1), org=(10, demo_window_height - 16),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_no, color=(0, 255, 0), thickness=thickness_no)
		
		if not FLAGS.CLOSE_ALL:
			cv2.namedWindow('Demo', cv2.WINDOW_AUTOSIZE)
			cv2.imshow("Demo", frame_demo)
	
	else:
		frame_demo = imutils.resize(frame, width=demo_window_width)
	
	return int(RES), frame_demo, stat

#MA display col
def demo_stacked_n_col_images(prefix, fn, list_of_imgs, winname, save_im=False):  # 2
	width = len(list_of_imgs)
	if not FLAGS.CLOSE_ALL:  # and not FLAGS.CLOSE_ALL:
		max_r, c_comb, dim_im = 0, 0, 2
		for im in list_of_imgs:
			if len(im.shape) == 3:
				r_im, c_im, dim_im = im.shape
			else:
				r_im, c_im = im.shape
			
			c_comb += c_im
			if r_im > max_r:
				max_r = r_im
		
		r_comb = max_r
		frame_border = 1
		c_comb = c_comb + (width - 1) * frame_border
		comb_im = np.zeros(shape=(r_comb, c_comb, dim_im), dtype=np.uint8)
		white = np.zeros(shape=(r_comb, frame_border, dim_im), dtype=np.uint8)
		white2 = np.zeros(shape=(r_comb, frame_border), dtype=np.uint8)
		white.fill(255)
		white2.fill(255)
		
		current_column = 0
		for im in list_of_imgs:
			if len(im.shape) == 3:
				comb_im[:(im.shape[0]), current_column:current_column + im.shape[1]] = im
				if current_column + im.shape[1] < c_comb:
					comb_im[:(im.shape[0]), current_column + im.shape[1]:current_column + im.shape[1] + frame_border] = white
			else:
				comb_im[:(im.shape[0]), current_column:current_column + im.shape[1]] = im[:, :, None]
				if current_column + im.shape[1] < c_comb:
					comb_im[:(im.shape[0]), current_column + im.shape[1]:current_column + im.shape[1] + frame_border] = white2[:,
					                                                                                                    :, None]
			
			current_column = current_column + im.shape[1] + frame_border
		
		if not FLAGS.CLOSE_ALL and not do_multiple_test:
			cv2.imshow(winname, comb_im)
			cv2.waitKey(5)
		if save_im:
			cv2.imwrite(prefix + 'comb_' + fn + '.png', comb_im)
		#print 'comb im shape:', comb_im.shape
	else:
		print 'CLOSE_ALL is set - for demo stacked images'
	return comb_im

def demo_stacked_n_row_images(prefix, fn, list_of_imgs_res, winname, save_im=False):
	r_comb, c_comb, dim_im = 0, 0, 3
	for im in list_of_imgs_res:
		r_im, c_im, dim_im = im.shape
		r_comb += r_im
	
	frame_border = 2
	c_comb = c_im
	r_comb = r_comb + frame_border * (len(list_of_imgs_res) - 1)
	comb_im = np.zeros(shape=(r_comb, c_comb, dim_im), dtype=np.uint8)
	
	white = np.zeros(shape=(frame_border, c_comb, dim_im), dtype=np.uint8)
	white.fill(255)
	
	current_row = 0
	for im in list_of_imgs_res:
		comb_im[current_row:current_row + im.shape[0], :im.shape[1]] = im #left:
		if current_row + im.shape[1] < r_comb:
			comb_im[current_row + im.shape[0]:current_row + im.shape[0] + frame_border, :im.shape[1]] = white
		
		current_row = current_row + im.shape[0] + frame_border
	
	cv2.imshow(winname, comb_im)
	if save_im:
		cv2.imwrite(prefix + fn + '.png', comb_im)

def get_classify_resultold(sess, test_image, test_labels, im_i, frame, frame_crop,
                        crop_y1, crop_y2, crop_x1, crop_x2, border, screen_out=False):
	# print 'frame shape', frame.shape[0], frame.shape[1]
	######################### Tensorflow
	print '\nNo.', im_i
	batch_xs, batch_ys = test_image, test_labels
	scale_h, scale_w = int(frame_crop.shape[0] / 320), int(frame_crop.shape[1] / 320)
	# border = 10
	crop_y1, crop_y2, crop_x1, crop_x2 = (crop_y1 - border) * scale_h, (crop_y2 + border) * scale_h, \
	                                     (crop_x1 - border) * scale_w, (crop_x2 + border) * scale_w
	
	# print 'batch_xs, batch_ys:', batch_xs, ', ', batch_ys
	dropout_cl = [0, 0, 0, 0]
	dropout_1s_cl = [1] * len(dropout_cl)
	
	output = sess.run("pred:0", feed_dict={"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s_cl})
	
	# print("Output for external=",output)
	output = tools.convert_to_confidence(output)  #
	np.set_printoptions(precision=3)
	if FLAGS.INFO_0:
		print 'output', output
	rank_outputs = sorted(range(len(settings.LABELS)), key=output[0].__getitem__)
	
	RES = np.argmax(output)  # hy index starting from 0, 0-5 corresponding predicted label 1-6
	print 'RES:', RES
	
	label_pred_str = settings.LABELS[RES][:-1]
	label_pred_str2 = settings.LABELS[rank_outputs[-2]][:-1]
	label_pred_str3 = settings.LABELS[rank_outputs[-3]][:-1]
	label_pred_str4 = settings.LABELS[rank_outputs[-4]][:-1]
	label_pred_str5 = settings.LABELS[rank_outputs[-5]][:-1]
	label_pred_str6 = settings.LABELS[rank_outputs[-6]][:-1]
	
	prob_str2 = str(output[0][rank_outputs[-2]])[:12]
	prob_str3 = str(output[0][rank_outputs[-3]])[:12]
	prob_str4 = str(output[0][rank_outputs[-4]])[:12]
	prob_str5 = str(output[0][rank_outputs[-5]])[:12]
	prob_str6 = str(output[0][rank_outputs[-6]])[:12]
	
	if FLAGS.INFO_0:
		print prob_str2, prob_str3, prob_str4, prob_str5, prob_str6
	
	# hy: for sub-classes
	# label_pred_str, label_pred_num = tools.convert_result(RES) # hy use it when sub-classes are applied
	# RES_sub_to_face = class_label #hy added
	# print "target, predict =", target, ', ', RES  # hy
	frame = frame_crop
	print 'frame shape(classify demo):', frame.shape
	if frame.shape[0] > 330:
		frontsize, frontsize_no, thickness, thickness_no = 2, 1, 4, 2
	else:
		frontsize, frontsize_no, thickness, thickness_no = 0.4, 0.5, 1, 1
	
	if screen_out:
		cv2.rectangle(frame, (crop_x1, crop_y1), (crop_x2, crop_y2), color=(0, 255, 0), thickness=thickness_no)
		# frame_demo = imutils.resize(frame, width=600)
		prob_str = str(output[0][RES])[:4]  # label index starts from 0
		cv2.putText(frame, "predicted top 1: " + label_pred_str + ' acc:' + prob_str,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.1)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(0, 255, 0), thickness=thickness)
		
		cv2.putText(frame, "predicted top 2: " + label_pred_str2 + ' acc:' + prob_str2,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.20)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		
		cv2.putText(frame, "predicted top 3: " + label_pred_str3 + ' acc:' + prob_str3,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.25)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		
		cv2.putText(frame, "predicted top 4: " + label_pred_str4 + ' acc:' + prob_str4,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.30)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		
		cv2.putText(frame, "predicted top 5: " + label_pred_str5 + ' acc:' + prob_str5,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.35)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		
		cv2.putText(frame, "predicted top 6: " + label_pred_str6 + ' acc:' + prob_str6,
		            org=(frame.shape[1] / 5, int(frame.shape[0] * 0.4)),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize, color=(255, 0, 0), thickness=thickness)
		
		# print 'video.get',str(video.get(1))
		# frame.shape[1] *9/ 12 for w, frame.shape[0] *11/ 12 for h
		demo_window_width = 600  # 600 * 1080 / 1920
		frame_demo = imutils.resize(frame,
		                            width=demo_window_width)  # cannot use frame because it should be reserved for receiving next input
		demo_window_height = int(demo_window_width * frame.shape[0] / frame.shape[1])
		print 'demo height', demo_window_height
		cv2.putText(frame_demo, 'No. ' + str(im_i), org=(demo_window_width - 120, demo_window_height - 16),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=frontsize_no, color=(0, 255, 0), thickness=thickness_no)
		# h:shape[0]
		
		
		cv2.namedWindow('Demo', cv2.WINDOW_AUTOSIZE)
		cv2.imshow("Demo", frame_demo)
	return int(RES)

def do_statistics(confMat1, confMat2, data_length):
	# for i in range(0, len(settings.LABELS)):
	#	confMat1[i, :] = confMat1[i, :] / np.sum(confMat1[i, :])
	tools.print_label_title()
	print confMat1
	tp = confMat2[0, 0]
	tn = confMat2[1, 1]
	overall_cl_acc = round(tp / data_length, 2)
	print 'TEST overall acc:', overall_cl_acc
	
	return overall_cl_acc

def do_segment_tf_cv2(sess2,im_crop_ori,im_crop, im_i,h,w,save_imgs=False):
	res_pass_list, res_fail_list = [], []
	save_res_path = PROJ_DIR + 'testbench/seg_mul_imgs/seg_tf/'

	#read_paths, files = get_read_path_and_files_for_im(read_from_file=False)

	'''
	# load tenforflow model
	new_graph = tf.Graph()
	with tf.Session(graph=new_graph) as sess2:
		tf_model_path = '../logs/'
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=tf_model_path)
		ckpt.model_checkpoint_path, seg_tf_model = get_tf_seg_model(ckpt, tf_model_path, model_meta='model_sigmdata1conv19_3_seg_adam_h320_0_I_0.73-500.meta',
																	copy=True)  # xx.meta

		tf.train.import_meta_graph(seg_tf_model)

		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess2, ckpt.model_checkpoint_path)
			print "Evaluation with model", ckpt.model_checkpoint_path
		else:
			print 'not found model'

	'''
	im_ori_cv2 = cv2.resize(im_crop_ori,(h,w))
	im_crop =  cv2.cvtColor(im_crop, cv2.COLOR_BGR2GRAY)
	im_crop = cv2.resize(im_crop,(h,w))

	batch_xs = im_ori_cv2.reshape(-1,h,w,3)
	batch_ys = im_crop.reshape(-1,h,w,1)
	#batch_xs, batch_ys, im_ori_cv2 = get_test_batch_misc_for_webcam(im_crop_color, im_crop, h, w)

	if FLAGS.DEBUG:
		print 'batch_x, batch_y shape:', batch_xs.shape, ', ', batch_ys.shape
		tb_im = np.array(batch_xs)
		tb_im = np.uint8(tb_im.reshape((h, w)))
		cv2.imshow('test_im', tb_im)  # this will be shown in blue tone

	feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}
	conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)
	pred = sess2.run("prediction:0", feed_dict=feed_dict_view)
	pred = np.squeeze(pred, axis=3)
	
	#cv2.imshow('test_im', im_ori_cv2)

	# min_p, max_p = min(tf_out[0]), max(tf_out[0])
	# print 'direct min:', min_p, ',  direct max:', max_p

	conv19_view = conv19_view[0].astype(np.uint8)  #
	#cv2.imshow('conv19', conv19_view)

	# if resize for next application required
	# pred = cv2.resize(np.uint8(pred), (h, w))

	pred = pred[0].astype(np.uint8) * 255
	#cv2.imshow('pred', pred)

	# postprocessing
	do_postprocessing = True
	if do_postprocessing:
		print '\ntf out uint8 shape:', pred.shape, '\ntf_out uint8 show values:', pred
		fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = ROI(pred, im_ori_cv2, h, w, im_i,
																	  save_file=False)
		# fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = ROI(tf_out, frame_crop_resize_gray, h, w, im_i=i, save_file=False)
		print 'r1, r2, c1, c2, screen_out:', r1, r2, c1, c2, screen_out
		print '########################### end\n'
		# screen_out = False
		# tf_out = np.uint8(tf_out)
		# if not screen_out:
		# out_img = cv2.resize(np.uint8(tf_out), (h, w))
		# cv2.imshow('res',out_img)
		cv2.imshow('image_crop_roi', image_crop_roi)

		res = pred
		if save_imgs and screen_out:
			# screen_out = False
			prefix = save_res_path + get_model_index(seg_tf_model, search_by='-')
			fn = ''
			demo_final_seg_result(fn, res, fr_add_cont, image_crop_roi, prefix, screen_out, save_stack_imgs=False,save_imgs=save_imgs)


		#########################################
	'''
	k = cv2.waitKey(30) & 0xFF
	while True:
		if k == ord('n'):
			print 'add to fail_list'
			res_fail_list.append(read_path + os.path.basename(files[i]))
			# res_fail_list.append(files[i])
			break
		elif k == ord('y'):
			print 'add to pass_list'
			res_pass_list.append(read_path + os.path.basename(files[i]))
			break
		elif k == ord('q'):  # ESC
			break
		else:
			k = cv2.waitKey(30) & 0xFF
			if k != 255:
				print 'k:', k  # 81-l, 83-r, 82-u, 84-d

	if cv2.waitKey(1) & 0xFF == ord('q'):
		print 'key interrupt'
		break
	'''
	return res, fr_add_cont, image_crop_roi, screen_out

#webcam
def do_segment_tf_misc(sess2,im_crop_ori,im_crop, im_i,h,w,show_bbox=False,save_imgs=False):
	import scipy.misc as misc
	res_pass_list, res_fail_list = [], []
	save_res_path = PROJ_DIR + 'testbench/seg_mul_imgs/seg_tf/'

	#read_paths, files = get_read_path_and_files_for_im(read_from_file=False)
	read_path = '../'
	#im_f = 'tmp.png' #test
	im_f = im_crop
	batch_xs, batch_ys, im_rz_cv2 = get_test_batch_misc(im_f, read_path, h, w, load_tmp_file=True)
	
	# batch_xs, batch_ys, dummy_mask, data_1s, label_1s, images_view = get_test_batch(h, w)
	do_reduce_mean = True
	if do_reduce_mean:
		batch_xs = tools.reduce_mean_stdev(batch_xs)

	'''
	# load tenforflow model
	new_graph = tf.Graph()
	with tf.Session(graph=new_graph) as sess2:
		tf_model_path = '../logs/'
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=tf_model_path)
		ckpt.model_checkpoint_path, seg_tf_model = get_tf_seg_model(ckpt, tf_model_path, model_meta='model_sigmdata1conv19_3_seg_adam_h320_0_I_0.73-500.meta',
																	copy=True)  # xx.meta

		tf.train.import_meta_graph(seg_tf_model)

		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess2, ckpt.model_checkpoint_path)
			print "Evaluation with model", ckpt.model_checkpoint_path
		else:
			print 'not found model'

	

	#to check
	im_ori_view = cv2.resize(im_crop_ori,(h,w))
	im_ori_batch = misc.imresize(im_crop_ori, [h, w], interp='nearest')
	im_crop =  cv2.cvtColor(im_crop, cv2.COLOR_BGR2GRAY)
	im_crop = cv2.resize(im_crop,(h,w))
	#normalize
	im_ori_view = im_ori_view / 255.0

	batch_xs = im_ori_batch.reshape(-1,h,w,3)
	batch_ys = im_crop.reshape(-1,h,w,1)
	#batch_xs, batch_ys, im_ori_cv2 = get_test_batch_misc_for_webcam(im_crop_color, im_crop, h, w)
  '''
	if FLAGS.DEBUG:
		print 'batch_x, batch_y shape:', batch_xs.shape, ', ', batch_ys.shape
		tb_im = np.array(batch_xs)
		tb_im = np.uint8(tb_im.reshape((h, w)))
		cv2.imshow('test_im', tb_im)  # this will be shown in blue tone

	feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}
	conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)
	pred = sess2.run("prediction:0", feed_dict=feed_dict_view)
	pred = np.squeeze(pred, axis=3)
	#cv2.imshow('test_im', im_f)

	# min_p, max_p = min(tf_out[0]), max(tf_out[0])
	# print 'direct min:', min_p, ',  direct max:', max_p

	conv19_view = conv19_view[0].astype(np.uint8)  #
	conv19_r, conv19_c, conv19_ch = conv19_view.shape
	# print '====== conv19 shape:', conv19_view.shape

	conv19_values0 = conv19_view[:, :, 0]
	conv19_values1 = conv19_view[:, :, 1]
	if conv19_ch == 2:
		conv19_view = conv19_view[:, :, 1]*255#1
	elif conv19_ch == 3:
		conv19_values2 = conv19_view[:, :, 2]


	# if resize for next application required
	# pred = cv2.resize(np.uint8(pred), (h, w))

	pred = pred[0].astype(np.uint8) * 255
	#cv2.imshow('pred', pred)

	k = cv2.waitKey(30) & 0xFF
	while True:
		if k == ord('n'):
			print 'add to fail_list'
			res_fail_list.append(im_i)
			# res_fail_list.append(files[i])
			break
		elif k == ord('y'):
			print 'add to pass_list'
			res_pass_list.append(im_i)
			save_imgs = True
			if save_imgs:
				save_ori_frame=False
				if save_ori_frame:
					im_save = cv2.resize(im_rz_cv2,(1920,1080),interpolation=cv2.INTER_CUBIC) #upsampling
				else:
					im_save = im_rz_cv2
				cv2.imwrite('../classified/MA_1/pass_cv_' + str(im_i) + '.png', im_save)
				misc.imsave('../classified/MA_1/pass_misc_' + str(im_i) + '.png', im_save)
			break
		elif k == ord('q'):  # ESC
			break
		else:
			k = cv2.waitKey(30) & 0xFF
			if k != 255:
				print 'k:', k  # 81-l, 83-r, 82-u, 84-d


	# Webcam
	do_postprocessing = True
	if do_postprocessing:
		print '\ntf out uint8 shape:', pred.shape, '\ntf_out uint8 show values:', pred
		fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = ROI(pred, im_rz_cv2, h, w, im_i,
																	  save_file=False)
		# fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = ROI(tf_out, frame_crop_resize_gray, h, w, im_i=i, save_file=False)
		print 'r1, r2, c1, c2, screen_out:', r1, r2, c1, c2, screen_out
		print '########################### end\n'
		# screen_out = False
		# tf_out = np.uint8(tf_out)
		# if not screen_out:
		# out_img = cv2.resize(np.uint8(tf_out), (h, w))
		# cv2.imshow('res',out_img)
		cv2.imshow('image_crop_roi', image_crop_roi)

		res = pred
		#save_imgs = False
		if show_bbox and screen_out:
			# screen_out = False
			prefix = save_res_path + get_model_index(seg_tf_model, search_by='-')
			fn = ''
			demo_final_seg_result(fn, res, fr_add_cont, image_crop_roi, prefix, screen_out, save_stack_imgs=False,save_imgs=save_imgs)



	else:
		#res, fr_add_cont, image_crop_roi, screen_out
		res = pred
		fr_add_cont, image_crop_roi = im_rz_cv2, im_rz_cv2
		screen_out = False
		#########################################
	'''
	k = cv2.waitKey(30) & 0xFF
	while True:
		if k == ord('n'):
			print 'add to fail_list'
			res_fail_list.append(read_path + os.path.basename(files[i]))
			# res_fail_list.append(files[i])
			break
		elif k == ord('y'):
			print 'add to pass_list'
			res_pass_list.append(read_path + os.path.basename(files[i]))
			break
		elif k == ord('q'):  # ESC
			break
		else:
			k = cv2.waitKey(30) & 0xFF
			if k != 255:
				print 'k:', k  # 81-l, 83-r, 82-u, 84-d

	if cv2.waitKey(1) & 0xFF == ord('q'):
		print 'key interrupt'
		break
	'''
	return res, fr_add_cont, image_crop_roi, screen_out

def do_segment_tf_video_misc(sess2,im_view,im_crop, im_i,h,w,show_bbox=False,save_imgs=False): #current2
	import scipy.misc as misc
	res_pass_list, res_fail_list = [], []
	load_tmp_file = False
	read_path = '../'
	if load_tmp_file:
		im_frame = 'tmp.png' #test
	else:
		im_frame = im_crop
		
	batch_xs, batch_ys, im_rz_cv2 = get_test_batch_misc(im_view,im_frame, h, w, load_tmp_file=load_tmp_file,read_path=read_path)
	
	'''
	im_ori_view = cv2.resize(im_crop_ori, (h, w))
	im_ori_batch = misc.imresize(im_crop_ori, [h, w], interp='nearest')
	im_crop = cv2.cvtColor(im_crop, cv2.COLOR_BGR2GRAY)
	im_crop = cv2.resize(im_crop, (h, w))
	# normalize
	im_ori_view = im_ori_view / 255.0
	
	batch_xs = im_ori_batch.reshape(-1, h, w, 3)
	batch_ys = im_crop.reshape(-1, h, w, 1)
	'''
	
	do_reduce_mean = True
	if do_reduce_mean:
		batch_xs = tools.reduce_mean_stdev(batch_xs)
	DEBUG = False
	if DEBUG:
		print 'batch_x, batch_y shape:', batch_xs.shape, ', ', batch_ys.shape
		tb_im = np.array(batch_xs)
		tb_im = np.uint8(tb_im.reshape((h, w)))
		cv2.imshow('test_im', tb_im)  # this will be shown in blue tone

	feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}
	conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)
	pred = sess2.run("prediction:0", feed_dict=feed_dict_view)
	pred = np.squeeze(pred, axis=3)
	cv2.imshow('test_im', im_frame)

	# min_p, max_p = min(tf_out[0]), max(tf_out[0])
	# print 'direct min:', min_p, ',  direct max:', max_p

	conv19_view = conv19_view[0].astype(np.uint8)  #
	conv19_r, conv19_c, conv19_ch = conv19_view.shape
	# print '====== conv19 shape:', conv19_view.shape

	conv19_values0 = conv19_view[:, :, 0]
	conv19_values1 = conv19_view[:, :, 1]
	if conv19_ch == 2:
		conv19_view = conv19_view[:, :, 1]*255#1
	elif conv19_ch == 3:
		conv19_values2 = conv19_view[:, :, 2]

	cv2.imshow('conv19', conv19_view)

	# if resize for next application required
	# pred = cv2.resize(np.uint8(pred), (h, w))

	pred255 = pred[0].astype(np.uint8) * 255
	#cv2.imshow('pred', pred)

	# postprocessing
	do_postprocessing = True
	if do_postprocessing:
		if FLAGS.DEBUG:
			print '\ntf out uint8 shape:', pred255.shape, '\ntf_out uint8 show values:', pred255
		fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = ROI(pred255, im_frame, h, w, im_i,
																	  save_file=False)
		print 'r1, r2, c1, c2, screen_out:', r1, r2, c1, c2, screen_out
		print '########################### end\n'
		# screen_out = False
		# tf_out = np.uint8(tf_out)
		# if not screen_out:
		# out_img = cv2.resize(np.uint8(tf_out), (h, w))
		# cv2.imshow('res',out_img)
		cv2.imshow('image_crop_roi', image_crop_roi)

		res = image_crop_roi
		#save_imgs = False
		if show_bbox and screen_out:
			# screen_out = False
			prefix = FLAGS.save_res_path + get_model_index(seg_tf_model, search_by='-')
			fn = ''
			demo_final_seg_result(fn, res, fr_add_cont, image_crop_roi, prefix, screen_out, save_stack_imgs=False,save_imgs=save_imgs)



	else:
		#res, fr_add_cont, image_crop_roi, screen_out
		res = pred255
		fr_add_cont, image_crop_roi = im_frame, im_frame
		screen_out = False
		#########################################
	return res, fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out
	#return res, fr_add_cont, image_crop_roi, screen_out

def get_tensor(im_i, pre_tensor, n_classes, cvtcolor, screen_out):
	# tensorImgIn = cv2.imread('../testbench/frame_color_tmp.jpg')
	# transform color and size to fit trained classifier model
	if screen_out and min(pre_tensor.shape) > 0 and not FLAGS.DEBUG:
		pre_tensor_ori = pre_tensor.copy()
		if FLAGS.DEBUG:
			cv2.namedWindow('pre_tensor', cv2.WINDOW_NORMAL)
		# pre_tensor.shape[1] / 16 for w, pre_tensor.shape[0] / 8 for h
		cv2.putText(pre_tensor, 'No. ' + str(im_i), org=(10, 20),
		            fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5,
		            color=(0, 255, 0), thickness=2)
		if FLAGS.DEBUG:
			cv2.imshow('pre_tensor', pre_tensor)
	
	if cvtcolor:
		pre_tensor = cv2.cvtColor(pre_tensor, cv2.COLOR_BGR2GRAY)
	
	# in case gray image as test image, no need to cvt
	
	test_image = cv2.resize(pre_tensor, (settings.h_resize, settings.w_resize))
	test_image = np.asarray(test_image, np.float32)
	
	tensorImgIn = test_image.reshape((-1, test_image.size))
	tensorImgIn = np.expand_dims(np.array(tensorImgIn), 2).astype(np.float32)
	tensorImgIn = tensorImgIn / 255 - 0.5  # TODO here is tricky, double check wit respect to the formats
	
	test_labels = np.zeros((1, n_classes))  # Making a dummy label tp avoid errors as initial predict
	return tensorImgIn, test_labels, pre_tensor_ori


def EVALUATE_IMAGE_classify_poseOLD(label_text,
                                 frame, frame_crop, frame_crop_roi,
                                 crop_y1, crop_y2, crop_x1, crop_x2,
                                 count_test, confMat1_TEST, confMat2_TEST, border, file_i, n_classes):
	# load classifier model
	new_graph = tf.Graph()
	
	with tf.Session(graph=new_graph) as sess2:
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=FLAGS.class_model_search_p)
		saver = tf.train.import_meta_graph(classifier_model)  # (v)
		
		ckpt.model_checkpoint_path = classifier_model[:-5]
		
		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess2, ckpt.model_checkpoint_path)
			print "Evaluation with model", ckpt.model_checkpoint_path
		else:
			print 'not found model'
		
		tensorImgIn, test_labels, pre_tensor = get_tensor(file_i, frame_crop_roi, n_classes, cvtcolor=True, screen_out=True)
		
		RES, frame_demo, stat = get_classify_result(sess2, tensorImgIn, test_labels, file_i, frame, frame_crop,
		                          crop_y1, crop_y2, crop_x1, crop_x2, border, screen_out=True)
		
		target = tools.get_ground_truth_label_im(label_text, default=False)
		# target = tools.get_ground_truth_label(file_i, default=False)
		
		confMat1_TEST, confMat2_TEST = tools.process_res(confMat1_TEST, confMat2_TEST, RES, frame_crop,
		                                                 SAVE_CorrectClassified, SAVE_Misclassified, file_i,
		                                                 target=target)
		do_statistics(confMat1_TEST, confMat2_TEST, count_test)
		
		cv2.waitKey(10)  # required for roi_seg
		return pre_tensor
	
def EVALUATE_IMAGE_classify_pose(read_path_im,
                                 frame, frame_crop, frame_crop_roi,
                                 crop_y1, crop_y2, crop_x1, crop_x2,
                                 count_test, confMat1_TEST, confMat2_TEST, border, file_i, n_classes, fname=''):
	
	# load classifier model
	new_graph = tf.Graph()
	# print 'frame ori size:',frame_crop.shape   #(1080, 1920, 3)
	# print 'frame crop size:',frame_crop_roi.shape   #(199, 129, 3)
	
	with tf.Session(graph=new_graph) as sess2:
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=FLAGS.class_model_search_p)
		saver = tf.train.import_meta_graph(classifier_model)  # (v)
		
		ckpt.model_checkpoint_path = classifier_model[:-5]
		
		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess2, ckpt.model_checkpoint_path)
			if not result_for_table:
				print "Evaluation with model", ckpt.model_checkpoint_path
		else:
			print 'not found model'
		
		tensorImgIn, test_labels, pre_tensor = get_tensor(file_i, frame_crop_roi, n_classes, cvtcolor=True,
		                                                  screen_out=True)
		
		target = tools.get_ground_truth_label_im(read_path_im, default=False)
		# target = tools.get_ground_truth_label(file_i, default=False) #for video, by frame number
		
		# IMAGE
		RES, demo_im, stat = get_classify_result(sess2, tensorImgIn, test_labels, file_i, frame, frame_crop,
		                                         crop_y1, crop_y2, crop_x1, crop_x2, border, screen_out=True, fname=fname,
		                                         target=target)
		
		confMat1_TEST, confMat2_TEST = tools.process_res(confMat1_TEST, confMat2_TEST, RES, frame_crop,
		                                                 SAVE_CorrectClassified, SAVE_Misclassified, file_i,
		                                                 target=target)
		
		overall_cl_acc = do_statistics(confMat1_TEST, confMat2_TEST, count_test)
		
		cv2.waitKey(10)  # required for roi_seg
		return pre_tensor, overall_cl_acc, demo_im, confMat1_TEST, stat


#For quick evaluation
def copy_model_files(key_term, save_path):
	from shutil import copyfile
	# key_term = 'model_1'
	m_files = [s for s in os.listdir(save_path) if key_term in s]
	for i in xrange(len(m_files)):
		if key_term + '(copy)' in m_files[i]:
			# print 'alreay exisits'
			break
		else:
			ext = os.path.splitext(m_files[i])[1]
			# print '\nfile', files[i], ', ext', ext
			copyfile(save_path + m_files[i], save_path + os.path.splitext(m_files[i])[0] + '(copy)' + ext)

def count_diff_pixel_values(picture, h, w):
	ds = []
	for row in xrange(h):
		for col in xrange(w):
			if picture[row][col] not in ds:
				ds.append(picture[row][col])
	return len(ds), ds

def get_tf_seg_model(ckpt,save_path ='',model_meta='',copy=True):
	seg_tf_model_key_term = str(ckpt).split('\"')[3]  # feed the second last saved model
	if len(model_meta) > 6 and 'model' in model_meta:
		seg_tf_model = save_path + model_meta
	elif copy:
		copy_model_files(os.path.basename(seg_tf_model_key_term), save_path)
		seg_tf_model = seg_tf_model_key_term + '(copy).meta'
	else:
	 seg_tf_models = [s for s in os.listdir(save_path) if '.meta' in s]
	 seg_tf_model = save_path + seg_tf_models[0]
		
	print 'seg_tf_model:', seg_tf_model
	
	return seg_tf_model[:-5], seg_tf_model

def get_test_batch_misc(im_rgb,im_f, h, w,load_tmp_file=False,read_path=''): #current1
	import scipy.misc as misc
	#need extra read with cv2 to show normal colour
	if load_tmp_file:
		im_ori_view = cv2.resize(cv2.imread(read_path + im_f, 3), (h, w))  # load in rgb
		image = misc.imresize(misc.imread(read_path + im_f), [h, w], interp='nearest')  # load in rgb
		
	else:
		#img_test = '../9780_ori_test_hinten.png'
		img_test = cv2.resize(im_f,(h,w))
		im_ori_view = cv2.resize(im_rgb,(h,w))
		img_test = np.array(img_test).astype(np.float32)
		image = img_test
    
	image = image / 255.0
	
	image = tools.reduce_mean_stdev(image)
	
	dummy_mask = np.ones((1, h * w))  # Making a dummy label tp avoid errors as initial predict
	
	batch_xs = image.reshape(-1, 320, 320, 3)
	batch_ys = dummy_mask.reshape(-1, 320, 320, 1)
	return batch_xs, batch_ys, im_ori_view


def get_test_batch_misc_image(im_f, h, w, load_tmp_file=False, read_path=''):  # current1
	import scipy.misc as misc
	# need extra read with cv2 to show normal colour
	if load_tmp_file:
		print 'read file path:', read_path+im_f
		im_ori_view = cv2.resize(cv2.imread(read_path + im_f, 3), (h, w))  # load in rgb
		image = misc.imresize(misc.imread(read_path + im_f), [h, w], interp='nearest')  # load in rgb
	
	else:
		# img_test = '../9780_ori_test_hinten.png'
		image = cv2.resize(im_f, (h, w))
		im_ori_view = image.copy()
		image = np.array(image).astype(np.float32)
	
	image = image / 255.0
	
	image = tools.reduce_mean_stdev(image)
	
	dummy_mask = np.ones((1, h * w))  # Making a dummy label tp avoid errors as initial predict
	
	batch_xs = image.reshape(-1, 320, 320, 3)
	batch_ys = dummy_mask.reshape(-1, 320, 320, 1)
	return batch_xs, batch_ys, im_ori_view


def get_test_batch(im_f, read_path, h, w,display=False):
	im_ori_rgb = cv2.resize(cv2.imread(read_path + im_f, 3), (h, w))  # load in rgb
	if display:
		cv2.imshow('ori', im_ori_rgb)
	
	im_ori_gray = cv2.imread(read_path + im_f, 0)  # load in grayscale
	
	# resize
	im_rz = imutils.resize(im_ori_gray, width=w)
	im_rz = cv2.resize(im_rz, (h, w))
	tf_grays, im_grays = [], []
	im_grays.append(im_rz)
	
	frame_crop_resize_gray = imutils.resize(im_rz, width=w)
	frame_crop_resize_gray = cv2.resize(frame_crop_resize_gray, (h, w))
	frame_crop_resize_gray = np.float32(frame_crop_resize_gray.reshape(h, w))
	
	tf_grays.append(frame_crop_resize_gray)
	
	images_np = np.asarray(im_ori_rgb)
	images = images_np.reshape((len(images_np), -1))
	images = np.expand_dims(np.array(images), 2).astype(np.float32)
	#images = tools.reduce_mean_stdev(images)
	
	dummy_mask = np.ones((1, h * w))  # Making a dummy label tp avoid errors as initial predict
	batch_x, batch_y = images, dummy_mask
	
	batch_xs = batch_x.reshape(-1, 320, 320, 3)
	batch_ys = batch_y.reshape(-1, 320, 320, 1)
	return batch_xs,batch_ys, im_ori_rgb

def get_model_index(path, search_by):
	import re
	m = os.path.basename(path)
	found_index = re.search(search_by, m)
	f_wo_ext = os.path.splitext(path)[0]
	if found_index:
		index = found_index.start() + 1
		f_wo_ext = os.path.basename(f_wo_ext)[index:]
	else:
		f_wo_ext = os.path.basename(f_wo_ext)
	return f_wo_ext

def demo_final_seg_result(fn, res, fr_add_cont, image_crop_roi, prefix,screen_out,save_stack_imgs=False,save_imgs=False):
	if save_imgs:
		if save_stack_imgs:
			r_gray1, c_gray1 = res.shape
			r_rgb, c_rgb, ch = image_crop_roi.shape
			r_rgb2, c_rgb2, ch = fr_add_cont.shape
	
			r_comb = max(r_rgb,r_gray1,r_rgb2)
			c_comb = c_rgb + c_gray1 + c_rgb2
			comb_im = np.zeros(shape=(r_comb,c_comb,ch), dtype=np.uint8)
	
			comb_im[:r_rgb, :c_gray1]          = res[:,:, None]      # direct output
			comb_im[:r_rgb, c_gray1:c_rgb+c_gray1] = image_crop_roi  # reverse mask
			comb_im[:r_rgb, c_rgb+c_gray1:]      = fr_add_cont #bounding box
	
			cv2.imwrite(prefix+'comb_'+ fn + '.png', comb_im)
		else:
			cv2.imwrite(prefix + '_conv19_' + fn + '.png', res)
			cv2.imwrite(prefix + '_rev_mask_' + fn + '.png', image_crop_roi)
			cv2.imwrite(prefix + '_box_' + fn + '.png', fr_add_cont)

	if screen_out and FLAGS.DEBUG:
		# cv2.imshow('seg_direct', np.uint8(res))
		cv2.imshow('conv19', res)
		cv2.imshow('box', fr_add_cont)
		cv2.imshow('rev_mask', image_crop_roi)

def EVALUATE_WEBCAM_seg_tf_and_classify(sess, camera_port, stop, num_class, show_bbox=False,save_imgs=False):  # (cam)
	import scipy.misc as misc
	n_classes = num_class
	save_res_path = PROJ_DIR + 'testbench/seg_mul_imgs/seg_tf/'
	# hy: check camera availability
	camera = cv2.VideoCapture(camera_port)

	# resolution can be set here, up to max resolution will be taken
	camera.set(cv2.cv.CV_CAP_PROP_FRAME_WIDTH, 2300)
	camera.set(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT, 1920)

	# area_step_size_webcam = 1080  # 479 #200

	confMat1_TEST = np.zeros((n_classes, n_classes), dtype=np.float)
	confMat2_TEST = np.zeros((2, 2), dtype=np.float)

	frame_i = 0
	eva_count = 0
	# hy: initialize confmatrix
	confMat2_TEST_Video = np.zeros((2, 2), dtype=np.float)

	# if stop == False:
	# if ckpt and ckpt.model_checkpoint_path:
	# Camera 0 is the integrated web cam

	# Number of frames to throw away while the camera adjusts to light levels
	#  ramp_frames = 1

	while True and not stop:  # hy: confirm camera is available
		# Now we can initialize the camera capture object with the cv2.VideoCapture class.
		# All it needs is the index to a camera port.
		print 'Getting image...'

		ret, frame = camera.read()

		# Captures a single image from the camera and returns it in PIL format

		# ret = camera.set(3, 320) #hy use properties 3 and 4 to set frame resolution. 3- w, 4- h
		# ret = camera.set(4, 240)

		cv2.waitKey(1)
		# A nice feature of the imwrite method is that it will automatically choose the
		# correct format based on the file extension you provide.
		# cv2.imwrite(file, camera_capture)

		####################################  /////////////////////////////


		if frame is not None:
			# print 'frame from webcam obtained'

			# hy: before continue check if image is read correctly
			# while frame is not None:
			frame_i += 1
			# hy:
			h_frame = frame.shape[0]
			w_frame = frame.shape[1]  # hy: h 1536 x w 2304

			# hy: info
			print "h and w", h_frame, ",", w_frame

			if frame_i % 30 == 0:
				fn = 'cam_' + str(frame_i)
				eva_count += 1
				use_focus_window = 0
				if use_focus_window:
					crop_x1 = 320  # 550
					crop_y1 = 320  # 700# 300
					area_step_size = 320  # 1080  # 740# 640
					crop_x2 = crop_x1 + area_step_size * 1
					crop_y2 = crop_y1 + area_step_size * 1 * settings.h_resize / settings.w_resize
					frame_crop = frame[crop_y1:crop_y2, crop_x1:crop_x2]
					# frame_crop_color = frame_crop.copy()
					# cv2.imwrite('../testbench/frame_color_tmp.jpg', np.uint8(frame_crop_color))

					cv2.imwrite('../tmp.png', frame_crop)
					frame_crop_ori = misc.imread('../tmp.png')

				else:
					# use full frame
					crop_x1 = 0
					crop_y1 = 0

					crop_x2 = 0 + w_frame  # 2300  #1920
					crop_y2 = 0 + h_frame  # 1536  #1080
					frame_crop = frame[crop_y1:crop_y2, crop_x1:crop_x2]

					# frame_crop_ori = frame_crop.copy()
					# misc.imsave('../tmp.png',frame_crop) #image saved with misc will show the obj in blue
					cv2.imwrite('../tmp.png', frame_crop)
					frame_crop_ori = misc.imread('../tmp.png')

				# frame_crop_roi = frame_crop_ori  # frame_crop_roi contains learned roi if segmentation is done previously



				# debug
				# print "shape:y1,y2,x1,x2:", crop_y1,", ", crop_y2,", ", crop_x1,", ", crop_x2
				# cv2.imshow("frame_cropped", frame_crop)
				# time for a loop 7.08207583427 from here

				# frame_crop = imutils.resize(cv2.cvtColor(frame_crop, cv2.COLOR_BGR2GRAY), width=320)
				# debug
				# print 'crop size', frame_crop.shape
				# cv2.imshow("TensorFlow Window", imutils.resize(frame_crop.astype(np.uint8), 480))
				# load seg model
				################################################################################################################
				res1, fr_add_cont, frame_crop_roi1, screen_out = do_segment_tf_misc(sess, frame_crop_ori,
																					frame_crop,
																					frame_i, 320, 320,
																					show_bbox=show_bbox,save_imgs=save_imgs)
				# tmp
				# screen_out = True
				#if screen_out:
					# cv2.imshow('res1', res1)
					# cv2.imshow('fr_add1', fr_add_cont)
					# cv2.imshow('roi1', frame_crop_roi1)

					################################################################################################################

				if not screen_out:
					print 'frame ', frame_i, ': do_segment_tf not found(webcam)'

		print 'no frame retrieved'

	del (camera)
	return stop

def EVA_IMAGE_SEG_TF_my_u(MODEL_ID,bg_model,best_avg, h, w, file_i=1, dices=0,save_imgs=False,show_bbox=False,with_gt=False,step_show=False):
	import scipy.misc as misc
	
	res_pass_list, res_fail_list, list_of_imgs_res = [], [], []
	dices, dice_l, dice_r, dice_o, dice_u, dice_v, dice_h, dices_cad, count_test, n_classes = 0, 0, 0, 0, 0, 0, 0, 0, 0, 6
	max_dice, min_dice, avg_dice_h, avg_dice_v, avg_dice_r, avg_dice_l, avg_dice_o, avg_dice_u, overall_cl_acc = 0, 0, 0, 0, 0, 0, 0, 0, 0
	
	save_res_path = PROJ_DIR + 'testbench/seg_mul_imgs/seg_tf/'

	avg_dice,dice_results,INPUT_CH = 0, [], 3
	
	confMat1_TEST = np.zeros((n_classes, n_classes), dtype=np.float)  # hy collect detailed confusion matrix
	confMat2_TEST = np.zeros((2, 2), dtype=np.float)
	
	if not with_gt:
		read_paths_im, files_im,dummy_mask = get_read_path_and_files_for_im(h,w,read_from_file=False)
		files_m, read_paths_m = files_im, read_paths_im
	elif do_6face_classification:
		read_paths_im, read_paths_m, files_im, files_m,dummy_mask = get_test_batch_im_m_6cl(h,w)
	
	else:
		images, masks, dummy_mask, files_im, files_m, images_view = get_test_batch_im_m(h, w,read_from_file=False)
	###########################################################
	# load tenforflow model
	new_graph = tf.Graph()
	with tf.Session(graph=new_graph) as sess2:
		tf_model_path = FLAGS.seg_model_search_p
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=tf_model_path)
		# or manually
		ckpt.model_checkpoint_path, seg_tf_model = get_tf_seg_model(ckpt, tf_model_path,
				model_meta=Seg_MODEL_to_load,copy=False)  # xx.meta
		# model_meta='model_sigmch3conv19_3_h320_II_avg0.14-124000.meta', copy=True) # xx.meta

		tf.train.import_meta_graph(seg_tf_model)

		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess2, ckpt.model_checkpoint_path)
			print "Evaluation with model", ckpt.model_checkpoint_path
			START = True
		else:
			print 'seg model not found.'
			START = False
		
	
		if START:
			batch_ys = dummy_mask
			batch_ys = batch_ys.reshape(-1, h, w, 1)
			for im_ori, ref_mask, i, read_path_im,read_path_m in zip(files_im, files_m, xrange(len(files_im)),read_paths_im,read_paths_m ):
				fn = os.path.basename(im_ori)[:-4]
				
				if i > -1:  # > 46 and i < 54:
					# if fn in cust_list: # 0 and i < 4:#> 46 and i < 54:
					if FLAGS.INFO_0:
						print 'read path:', read_path_im
					im_ori_view = cv2.imread(read_path_im + im_ori)
					
					im_ori_view = cv2.resize(im_ori_view, (h, w))
					im_crop = im_ori_view.copy()
					
				im_crop = np.uint8(im_crop.reshape((h, w, 3)))
				batch_xs = im_crop.reshape(-1, h, w, INPUT_CH)  # cat

				feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}
				# conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)
				pred = sess2.run("prediction:0", feed_dict=feed_dict_view)
				pred = np.squeeze(pred, axis=3)
				pred_int = pred[0].astype(np.uint8)# expected out is always 0 or 1, not between
				pred255 = pred_int * 255
				
				
				#no need threshold for pred
				ret, mask = cv2.threshold(pred255, 254, 255, cv2.THRESH_BINARY_INV)
				#pred_thresh = mask.copy()

				conv19_view = sess2.run("conv19:0", feed_dict=feed_dict_view)
				

				# print '====== conv19 shape:', conv19_view.shape
				
				
				conv19_view = conv19_view[0].astype(np.uint8) * 255  #
				conv19_r, conv19_c, conv19_ch = conv19_view.shape
				conv19_values0 = conv19_view[:, :, 0]
				conv19_values1 = conv19_view[:, :, 1]
				if conv19_ch == 2:
					conv19_values2 = conv19_view[:, :, 1]
					conv19_view = np.expand_dims(conv19_values0, 3)
				elif conv19_ch == 3:
					conv19_values2 = conv19_view[:, :, 2]
				
				if FLAGS.DEBUG:
					cv2.imshow('pred direct',pred_int)
					cv2.imshow('pred thres', pred_thresh)
					cv2.imshow('conv19view', conv19_view)
				cv2.imshow('19', conv19_view) #tmp
				pred_thresh = conv19_values0.copy()
				thresh_res = 0
				idx = pred_thresh[:, :] > thresh_res
				pred_key = 255
				pred_thresh[idx] = pred_key
				
				#tmp
				#pred_thresh = conv19_values0.copy()
				# print 'pred *255', pred #tasks direct value without *255: [0,2], so what we see with *255: 0,255,510
				# print 'conv19 *255', conv19_view  #tasks value 0,1, => 0, 255
				# print 'tf_out uint8 show values:', pred
				# pred_int,pred_thresh, fr_add_cont, old_mask, old_roi_res, \
				# new_mask, image_crop_roi, r1, r2, c1, c2, screen_out,roi_whitebg
				# IMAGE
				fr_add_cont, old_mask, old_roi_res, new_mask, new_roi_res, r1, r2, c1, c2, \
				screen_out, roi_whitebg = ROI(pred_thresh, im_ori_view, h, w, im_i=i,save_file=False)
				
				if with_gt:
					ref_mask = cv2.resize(cv2.imread(read_path_m + ref_mask, 0), (h, w))
					ref_mask_thresh = ref_mask  # * 255.0
					thresh_ref = 0
					idx = ref_mask_thresh[:, :] > thresh_ref
					ref_key = 255
					"""
					all values larger than thresh_at value will be replaced with a new value
					this is only for the case when only one annotation in foreground and the background are interested
					"""
					ref_mask_thresh[idx] = ref_key
					
					#ref_mask = np.squeeze(ref_mask, axis=2)
					#ref_mask_thresh = ref_mask * 255.0
				
				def print_count(img, h, w, str):
					n, diff_v = count_diff_pixel_values(img, h, w)
					min_n = min(n, 5)
					print 'num of diff values in ', str, ':', n, ',    ', diff_v[:min_n]
				
				print_count(pred_thresh, h, w, 'pred')
				# print_count(ref_mask_thresh,h,w,'r255')
				if FLAGS.DEBUG:
					print_count(conv19_values0, h, w, 'con0')
					print_count(conv19_values1, h, w, 'con1')
					print_count(conv19_values2, h, w, 'con2')
				
				##############################################################
				# calc dice
				##############################################################
				pred_key = 255
				if search_str not in fn:
					count_test += 1
					dice = tools.calc_dice_simi(pred_thresh, ref_mask_thresh, fn, k=pred_key)
					print '\nDice for',fn,':',dice
					dices += dice
					if dice < min_dice:
						min_dice = dice
					if dice > max_dice:
						max_dice = dice
					if do_6face_classification:
						num_imgs_of_class = len(os.listdir(read_path_im))
						if 'vorn' in read_path_im:
							dice_v += dice
							avg_dice_v = dice_v/num_imgs_of_class
						if 'hinten' in read_path_im:
							dice_h += dice
							avg_dice_h = dice_h /num_imgs_of_class
						if 'links' in read_path_im:
							dice_l += dice
							avg_dice_l = dice_l /num_imgs_of_class
						if 'rechts' in read_path_im:
							dice_r += dice
							avg_dice_r = dice_r /num_imgs_of_class
						if 'oben' in read_path_im:
							dice_o += dice
							avg_dice_o = dice_o /num_imgs_of_class
						if 'unten' in read_path_im:
							dice_u += dice
							avg_dice_u = dice_u /num_imgs_of_class
				
				if min(new_roi_res.shape) > 1:
					pre_tensor_view, overall_cl_acc, demo_im, confMat1_TEST, stat \
						= EVALUATE_IMAGE_classify_pose(read_path_im,im_ori,im_crop,new_roi_res,
					                             r1, r2, c1, c2,count_test,confMat1_TEST,confMat2_TEST,border,i,n_classes,fn)
				
				# print 'r1, r2, c1, c2, screen_out:', r1, r2, c1, c2, screen_out
				# r1, r2, c1, c2 = r1 + border, r2 - border, c1 + border, c2 - border
				# image_crop_roi = image_crop_roi[r1:r2, c1:c2]  # crop_y1:crop_y2, crop_x1:crop_x2
				prefix = get_model_index(seg_tf_model, '\d')
				
				found_dig = re.search('\d', os.path.basename(Seg_MODEL_to_load))
				if found_dig:
					# dig = found_dig.start()
					# prefix = save_res_path + os.path.basename(bg_model)[dig:-5]
					prefix = save_res_path + get_model_index(seg_tf_model, search_by='-')
				else:
					print 'no file found'
			
				create_stacked_imgs_for_paper = True
				if create_stacked_imgs_for_paper:
					s_size = (320 / 2, 320 / 2)
					if read_path_im == read_path_m:
						list_of_imgs = [cv2.resize(im_ori_view, s_size), cv2.resize(pred_int, s_size),
						                cv2.resize(old_mask, s_size), cv2.resize(new_mask, s_size), cv2.resize(fr_add_cont, s_size),
						                cv2.resize(roi_whitebg, s_size), cv2.resize(pre_tensor_view, s_size)]
						
						#list_of_imgs = [cv2.resize(im_ori_view, (320 / 2, 320 / 2)),
						 #               cv2.resize(new_mask, (320 / 2, 320 / 2)),
						  #              cv2.resize(roi_whitebg, (320 / 2, 320 / 2)),
						   #             cv2.resize(pre_tensor_view, (320 / 2, 320 / 2)),
						    #            cv2.resize(stat, (320 / 2, 320 / 2))]
						
						#tmp
						#cv2.imshow('ori', cv2.resize(im_ori_view, s_size))
						#cv2.imshow('tensor', cv2.resize(pre_tensor_view, s_size))
						
						winname = 'in-pred-oldmask-newmask-cont-pretensor'
					else:
						# list_of_imgs = [cv2.resize(im_ori_view,s_size),cv2.resize(ref_mask_thresh,s_size),cv2.resize(pred_int,s_size),
						#              cv2.resize(old_mask, s_size),cv2.resize(new_mask,s_size),cv2.resize(fr_add_cont,s_size),
						#              cv2.resize(roi_whitebg,s_size),cv2.resize(pre_tensor_view,s_size),cv2.resize(stat,s_size)]
						
						list_of_imgs = [cv2.resize(im_ori_view,s_size),cv2.resize(ref_mask_thresh,s_size),cv2.resize(pred_int,s_size),
						              cv2.resize(old_mask, s_size),cv2.resize(new_mask,s_size),cv2.resize(fr_add_cont,s_size),
						              cv2.resize(roi_whitebg,s_size),cv2.resize(pre_tensor_view,s_size)]
						
						winname = 'in-gt-pred-oldmask-newmask-cont-pretensor'
						#
						
				# prefix = save_res_path + get_model_index(seg_tf_model, search_by='-')
					stacked_imgs = demo_stacked_n_col_images(prefix, fn, list_of_imgs, winname, save_im=False)
					list_of_imgs_res.append(stacked_imgs)

					#########################################
				if step_show:
					k = cv2.waitKey(30) & 0xFF
					while True:
						if k == ord('n'):
							print 'add to fail_list'
								# res = pred
							res_fail_list.append(read_path_im + os.path.basename(files_im[i]))
							# res_fail_list.append(files[i])
							break
						elif k == ord('y'):
							print 'add to pass_list'
							res_pass_list.append(read_path_im + os.path.basename(files_im[i]))
							if save_imgs:
								cv2.imwrite(FLAGS.save_res_path + os.path.basename(files_im[i]), new_roi_res)
							break
						elif k == ord('q'):  # ESC
							break
						else:
							k = cv2.waitKey(30) & 0xFF
							if k != 255:
								print 'k:', k  # 81-l, 83-r, 82-u, 84-d
	
					if cv2.waitKey(1) & 0xFF == ord('q'):
						print 'key interrupt'
						break
	
					print Seg_MODEL_to_load, 'test package:', read_path_im
					print 'res_fail_list=', res_fail_list, '\nres_pass_list=', res_pass_list
					print 'num of fail:', len(res_fail_list), '\nnum of pass:', len(res_pass_list)
			
				
		if count_test > 0:
			avg_dice = float(dices / count_test)
		else:
			avg_dice = 0
		if avg_dice > best_avg:
			best_avg = avg_dice
		print '\nseg avg dice:', avg_dice, 'max dice:', max_dice, ', min dice:', min_dice, ', ', bg_model
		if do_6face_classification:
			print '6classifier overall_cl_acc', overall_cl_acc
			print tools.print_label_title()
			print confMat1_TEST
			print 'seg avg_v:', avg_dice_v, 'seg avg_h:', avg_dice_h, ', seg avg_l:', avg_dice_l, ', \nseg avg_r:', avg_dice_r \
				, ', seg avg_u:', avg_dice_u, ', seg avg_o:', avg_dice_o
		else:
			overall_cl_acc = 0
		print 'classifier model', classifier_model
		print 'use_cut_bounding_box:', use_cut_bounding_box, ', use_seg_limit:', use_seg_limit
		
		num_total_ims = len(list_of_imgs_res)
		print 'list imgs res:', num_total_ims
		# create_stacked_imgs_for_paper = True
		if create_stacked_imgs_for_paper and num_total_ims > 0 and num_total_ims < 20:
			if not do_multiple_test:
				MODEL_ID = ''
			demo_stacked_n_row_images(prefix, MODEL_ID, list_of_imgs_res, winname, save_im=False)
	
		return best_avg, bg_model, overall_cl_acc
	
	
	
def EVALUATE_VIDEO_seg_and_classify(sess,VIDEO_FILE, label_text, num_class, in_ch, h,w,show_bbox= False, step_show=False, save_input_im=False,
                                    save_tensor_imgs=False):  # (v)
	import scipy.misc as misc

	n_classes = num_class
	video = cv2.VideoCapture(VIDEO_FILE)  # hy: changed from cv2.VideoCapture()
	
	video.set(1, 2)  # hy: changed from 1,2000 which was for wheelchair test video,
	# hy: propID=1 means 0-based index of the frame to be decoded/captured next
	test_read_path = ''
	
	if not video.isOpened():
		print "cannot find or open video file"
		exit(-1)
		
	count_test, video_frame_i = 0, 0
	confMat1_TEST = np.zeros((n_classes, n_classes), dtype=np.float)  # hy collect detailed confusion matrix
	confMat2_TEST = np.zeros((2, 2), dtype=np.float)
	while True:  # and video_frame_i < 850:
		
		ret, frame = video.read()
		
		if ret:  # time for a loop 7.28790903091 start from here
			fh, fw = frame.shape[0], frame.shape[1]
			video_frame_i += 1
		# print '\n\n########################################### start, frame', video_frame_i
		# print 'frame shape h,w:', h, w  # 1536 2304
		else:
			break
		
		if cv2.waitKey(1) & 0xFF == ord('q'):  # hy:press key-q to quit
			break
		
		if video_frame_i % 8 == 0 and video_frame_i == 96:#> 1 and video_frame_i < 140:
			# 1-64:rechts,510-960:vorn, 990-1320:hinten, 1350-1470:oben, 1500-2550:unten
			count_test += 1
			# time for a loop 7.43529987335,7.09782910347 variously, start from here
			use_focus_window = 0
			if use_focus_window:
				crop_x1 = 450  # 550
				crop_y1 = 600  # 700# 300
				area_step_size = 1080  # 740# 640
				crop_x2 = crop_x1 + area_step_size * 1
				crop_y2 = crop_y1 + area_step_size * 1 * settings.h_resize / settings.w_resize
				frame_crop = frame[crop_y1:crop_y2, crop_x1:crop_x2]
				frame_crop_color = frame_crop.copy()
				cv2.imwrite('../tmp.png', frame_crop_color)
			# cv2.imwrite('../testbench/frame_color_tmp.jpg', np.uint8(frame_crop_color))
			else:
				crop_x1 = 0
				crop_y1 = 0
				
				crop_x2 = 0 + fw  # 2300  #1920
				crop_y2 = 0 + fh  # 1536  #1080
				frame_crop = frame[crop_y1:crop_y2, crop_x1:crop_x2]
				frame_crop_color = frame_crop.copy()
			
			# debug
			# print "shape:y1,y2,x1,x2:", crop_y1,", ", crop_y2,", ", crop_x1,", ", crop_x2
			# cv2.imshow("frame_cropped", frame_crop)
			# time for a loop 7.08207583427 from here
			
			# frame_crop = imutils.resize(cv2.cvtColor(frame_crop, cv2.COLOR_BGR2GRAY), width=320)
			# debug
			# print 'crop size', frame_crop.shape
			# cv2.imshow("TensorFlow Window", imutils.resize(frame_crop.astype(np.uint8), 480))
			# load seg model
			################################################################################################################
				
				cv2.imwrite('../tmp.png', frame_crop_color)
			frame_crop_misc = misc.imread('../tmp.png') #today
			im_ori_rz_cv2 = cv2.resize(frame_crop_color,(320,320))
				
			#pred_255, fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = \
			#do_segment_tf_video_misc(sess, frame_crop_color,frame_crop_color,video_frame_i, 320, 320,show_bbox=True,save_imgs=save_input_im)
			batch_xs, batch_ys, im_cv2_tmp = get_test_batch_misc(frame_crop_misc, 'tmp.png', 320, 320, load_tmp_file=True,read_path='../')
			#im_rgb,im_f, h, w,load_tmp_file=False,read_path=''
			
			if FLAGS.DEBUG:
				print 'batch_x, batch_y shape:', batch_xs.shape, ', ', batch_ys.shape
				tb_im = np.array(batch_xs)
				tb_im = np.uint8(tb_im.reshape((h, w)))
				cv2.imshow('test_im', tb_im)  # this will be shown in blue tone
			
			feed_dict_view = {"x:0": batch_xs, "y:0": batch_ys, "keep_prob:0": dropout_1s}
			conv19_view = sess.run("conv19:0", feed_dict=feed_dict_view)
			pred = sess.run("prediction:0", feed_dict=feed_dict_view)
			#pred = np.squeeze(pred, axis=3)
			
			# min_p, max_p = min(tf_out[0]), max(tf_out[0])
			# print 'direct min:', min_p, ',  direct max:', max_p
			
			conv19_view = conv19_view[0].astype(np.uint8) * 255  #
			conv19_r, conv19_c, conv19_ch = conv19_view.shape
			conv19_values0 = conv19_view[:, :, 0]
			if conv19_ch == 2:
				conv19_values1 = conv19_view[:, :, 1]
				conv19_values2 = conv19_values1
				conv19_view = np.expand_dims(conv19_values2, 3)
			elif conv19_ch == 3:
				conv19_values2 = conv19_view[:, :, 2]
			
			# if resize for next application required
			# pred = cv2.resize(np.uint8(pred), (h, w))
			
			pred_direct = pred[0].astype(np.uint8) * 255
			pred255 = pred_direct * 255
			pred_thresh = pred.copy()
			
			if FLAGS.DEBUG:
				cv2.imshow('test_im', im_ori_rz_cv2)
				cv2.imshow('conv19_0', conv19_view)
				cv2.imshow('direct',pred_direct )
				cv2.imshow('pred255', pred255)
			#cv2.imwrite('../testbench/tf_imgs/pred255_'+str(video_frame_i)+'.png', pred255)
			
			# model,im_crop_color, im_i, h, w,in_ch,show_bbox=False
			# tmp
			#cv2.imshow('fr_add_cont', fr_add_cont)
			
			################################################################################################################
			load_model2 = 0
			if load_model2 == 1:
				# load seg model
				set_image_dim_ordering(dim_ordering='th')  #
				model = load_model(bg_model2)
				print 'loaded model', bg_model2
				
				res2, fr_add_cont, fg2, screen_out = do_segment_tf_video_misc(model, frame_crop_color, frame_crop, video_frame_i, 320, 320,
				                                                in_ch)
				# tmp
				screen_out = True
				if screen_out and FLAGS.DEBUG:
					cv2.imshow('res2', res2)
					cv2.imshow('fr_add2', fr_add_cont)
					cv2.imshow('fg2', fg2)
				################################################################################################################
				
				# res1 = Image.fromarray(res1)
				rows, cols = res1.shape
				if FLAGS.DEBUG:
					print 'rows,cols', rows, cols
				roi1 = res1[0:rows, 0:cols]
				
				ret, res1 = cv2.threshold(res1, 50, 255, cv2.THRESH_BINARY)
				ret, res2 = cv2.threshold(res2, 50, 255, cv2.THRESH_BINARY)
				res = cv2.bitwise_or(res1, res2)
				res = np.uint8(res)
				
				kernel = np.ones((5, 5), np.uint8)
				erosion = cv2.erode(res, kernel, iterations=1)
				res = cv2.dilate(erosion, kernel, iterations=1)
				
				if FLAGS.DEBUG:
					cv2.imshow('pred255', pred255)
				
				# tmp
				screen_out = False
				if screen_out:
					cv2.imshow('res_fin', res)
			################################################################################################################
			else:
				res = pred255
			################################################################################################################
			
			# fr_add_cont, frame_crop_roi, screen_out = ROI(res, frame_crop_color, 320,320, im_i=video_frame_i, save_file=False)
			#ret, res1 = cv2.threshold(res1, 50, 255, cv2.THRESH_BINARY)
			#ret, res2 = cv2.threshold(res2, 50, 255, cv2.THRESH_BINARY)
			#res = cv2.bitwise_or(res1, res2)
			res = np.uint8(res)
			
			kernel = np.ones((5, 5), np.uint8)
			erosion = cv2.erode(res, kernel, iterations=1)
			res = cv2.dilate(erosion, kernel, iterations=2)
			cv2.imshow('morph_p255', res)
			
			#VIDEO
			fr_add_cont, image_crop_roi, r1, r2, c1, c2, screen_out = ROI(res, im_ori_rz_cv2, h, w, video_frame_i,
			                                                              save_file=False)
			print 'r1, r2, c1, c2, screen_out:', r1, r2, c1, c2, screen_out
			r1, r2, c1, c2 = r1 + border, r2 - border, c1 + border, c2 - border
			image_crop_roi = image_crop_roi[r1:r2, c1:c2]  # crop_y1:crop_y2, crop_x1:crop_x2
			
			if min(image_crop_roi.shape) > 1:
				im_crop = frame_crop_color  # only set specific value when segmentation result is not used for 6cl-classification
				pre_tensor = EVALUATE_IMAGE_classify_pose(label_text, frame_crop_color, im_crop, image_crop_roi,
				                                          r1, r2, c1, c2, count_test, confMat1_TEST, confMat2_TEST, border, video_frame_i,
				                                          n_classes)
			#tmp
			#screen_out, show_bbox = True, True
			if screen_out and show_bbox:
				cv2.imshow('test_im', im_ori_rz_cv2)
				#cv2.imshow('pred_255', pred255)
				#cv2.imshow('reverseMask', image_crop_roi)
				prefix = get_model_index(seg_tf_model, '\d')
				
				found_dig = re.search('\d', os.path.basename(Seg_MODEL_to_load))
				if found_dig:
					# dig = found_dig.start()
					# prefix = save_res_path + os.path.basename(bg_model)[dig:-5]
					prefix = FLAGS.save_res_path + get_model_index(seg_tf_model, search_by='-')
				else:
					print 'no file found'
				
				#####################################################
				step_show = True
				if not step_show:
					if save_tensor_imgs:
						print 'save path:', FLAGS.save_res_path + 'frame_tf_' + str(video_frame_i)
						cv2.imwrite(FLAGS.save_res_path + 'frame_tf_' + str(video_frame_i) + '.png', pre_tensor)
				else:
					k = cv2.waitKey(30) & 0xFF
					while True:
						if k == ord('n'):
							print 'add to fail_list'
							# res_fail_list.append(read_path_im + os.path.basename(files_im[i]))
							# res_fail_list.append(files[i])
							break
						elif k == ord('y'):
							print 'add to pass_list'
							#save_tensor_imgs = False
							if save_tensor_imgs:
								print 'save path:', FLAGS.save_res_path + 'frame_tensor' + str(video_frame_i)
								cv2.imwrite(FLAGS.save_res_path + 'frame_tensor' + str(video_frame_i) + '.png', pre_tensor)
							#save_ori_frame = True
							if save_input_im:
								if save_ori_frame:
									im_save = cv2.resize(frame_crop_color, (1920, 1080), interpolation=cv2.INTER_CUBIC)  # upsampling
								# or save image_crop_roi
								else:
									im_save = cv2.resize(frame_crop_color, (h, w))

								cv2.imwrite(FLAGS.save_res_path + 'frame_' + str(video_frame_i) + '.png', im_save)
							# misc.imsave('../classified/MA_1/pass_misc_' + str(im_i) + '.png', im_save)
							break
						
						elif k == ord('q'):  # ESC
							break
						else:
							k = cv2.waitKey(30) & 0xFF
							if k != 255:
								print 'k:', k  # 81-l, 83-r, 82-u, 84-d
					
					if cv2.waitKey(1) & 0xFF == ord('q'):
						print 'key interrupt'
						break
				
				cv2.waitKey(10)  # required for roi_seg
			
			# tmp_time3=time.time()
			# print 'time for a loop',tmp_time3-tmp_time0 #7.11944699287


##########################################################################################################
if EVALUATE_VIDEO_ == 1:
	new_graph = tf.Graph()
	with tf.Session(graph=new_graph) as sess:
		tf_model_path = FLAGS.seg_model_search_p
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=tf_model_path)
		ckpt.model_checkpoint_path, seg_tf_model = get_tf_seg_model(ckpt, tf_model_path,
		                    model_meta=Seg_MODEL_to_load,copy=True)  # xx.meta
		#model_meta='model_sigmch3conv19_3_h320_II_avg0.14-124000.meta',copy=True)  # xx.meta
		#model_meta='model_sigmch3conv19_3__0_I_loss0.6-5750.meta',
		tf.train.import_meta_graph(seg_tf_model)

		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess, ckpt.model_checkpoint_path)
			print "Evaluation with model", ckpt.model_checkpoint_path
		else:
			print 'not found model'

		stop = False
		test_video_custom = 1
		while not stop:
			#stop = EVALUATE_WEBCAM_seg_tf_and_classify(sess,camera_port, False, num_class=6,show_bbox=True)
			if test_video_custom == 1:
				print '####################################'
				print 'Starting evaluation with k model'
				print Seg_MODEL_to_load, classifier_model
				video_list = ['full/']
				TestFace = 'full'
				video_window_scale = 1
				VIDEO_FILE, crop_x1, crop_y1, area_step_size, video_label = tools.set_video_window(TestFace, video_window_scale)
				print VIDEO_FILE, crop_x1, crop_y1, area_step_size, video_label
				
				
				stop = EVALUATE_VIDEO_seg_and_classify(sess,VIDEO_FILE,TestFace, 6, in_ch=1,h=320,w=320, show_bbox=True,step_show=True, save_input_im=False,
					                                save_tensor_imgs=True)
			
			else:
				video_list = ['hinten/', 'links/', 'oben/', 'rechts/', 'unten/', 'vorn/']
				
				for video_index in xrange(len(video_list)):
					TestFace = video_list[video_index][:-1]  # all # full, 0 hinten, 1 links, 2 oben, 3 rechts, 4 unten, 5 vorn,
					print 'Test face:', TestFace
					
					# TestFace = settings.LABELS[video_index][:-1] #'vorn'  #'hinten' # full, 0 hinten, 1 links, 2 oben, 3 rechts, 4 unten, 5 vorn,
					VIDEO_FILE, crop_x1, crop_y1, area_step_size, video_label = tools.set_video_window(TestFace, scale=2)
					#sess,VIDEO_FILE, label_text, num_class, in_ch, show_bbox= False, step_show=False, save_input_im=False,
                                    #save_tensor_imgs=False
					stop = EVALUATE_VIDEO_seg_and_classify(sess,VIDEO_FILE,TestFace, 6, in_ch=1,h=320,w=320, show_bbox=True,step_show=True, save_input_im=False,
					                                save_tensor_imgs=True)
					print 'test face:', TestFace, 'done\n'
	

cv2.waitKey(0)
cv2.destroyAllWindows()


if EVA_IMAGE_SEG_TF == 1:
	best_avg = 0 
	#dices, avg_dice = EVA_IMAGE_SEG_TF_my_u(MODEL_ID,best_avg, 320, 320, file_i=1,dices=0,save_imgs=True,show_bbox=True,with_gt=FLAGS.with_gt,step_show=False)
	
	seg_avg, model_name, overall_cl_acc \
		= EVA_IMAGE_SEG_TF_my_u(MODEL_ID,Seg_MODEL_to_load,best_avg,
		      320, 320, file_i=1,dices=0,save_imgs=True,
		        show_bbox=True,with_gt=FLAGS.with_gt,step_show=True)
	
	
	print 'avg dices:', seg_avg
	# seg_image_2c(320, 320)
	#eva_image_6c(320, 320)
	print("Evaluation done!")

if EVALUATE_WEBCAM_seg_tf_and_classify == 1:
	print 'Test with Webcam starting ...'
	new_graph = tf.Graph()
	with tf.Session(graph=new_graph) as sess:
		tf_model_path = FLAGS.seg_model_search_p
		ckpt = tf.train.get_checkpoint_state(checkpoint_dir=tf_model_path)
		ckpt.model_checkpoint_path, seg_tf_model = get_tf_seg_model(ckpt, tf_model_path,
		                       model_meta=Seg_MODEL_to_load,copy=True)  # xx.meta
		#model_meta='model_sigmch3conv19_3_h320_II_avg0.14-124000.meta',copy=True)  # xx.meta
		#model_meta='model_sigmch3conv19_3__0_I_loss0.6-5750.meta',
		tf.train.import_meta_graph(seg_tf_model)

		if ckpt and ckpt.model_checkpoint_path:
			saver = tf.train.Saver()
			saver.restore(sess, ckpt.model_checkpoint_path)
			print "Evaluation with model", ckpt.model_checkpoint_path
		else:
			print 'not found model'

		# Camera 0 is the integrated web cam on my netbook
		camera_port = 0
		# EVALUATE_WITH_WEBCAM_track_roi(camera_port,n_classes)
		stop = False
		while not stop:
			stop = EVALUATE_WEBCAM_seg_tf_and_classify(sess,camera_port, False, num_class=6,show_bbox=True,save_imgs=False)
			print 'stop:', stop
