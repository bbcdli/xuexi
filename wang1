import tensorflow as tf
import numpy as np
import tflearn

import settings  # hy: collection of global variables

settings.set_global()
PRINT_ARCHI = True

#when using concat at axis 0 got logits shape [512000,3] and labels shape [102400]
####################################################### Header begin ###################p#############################
###############################
# 0    1      2      3        4        5      6     7      8
# dropout = [1,  1,  1,  1,      1,      1,  1,  1,  1] #1st
dropout = [0.15, 0.25, 0.4, 0.45, 1, 0.4, 0.25, 0.15, 0.15]  # 1st
#dropout = [0.5, 0.5, 0.4, 0.5, 1, 0.4, 0.5, 0.5, 0.5]  # 1st
dropout_1s = [1] * len(dropout)

# output size is now n_classes
n_classes = 2
IMAGE_SIZE = 320  # 320
input_size = IMAGE_SIZE * IMAGE_SIZE  # hy
optimizer_type = 'adam'# 'adam'  # 'adam' #GD-'gradient.descent',#'ProximalGradientDescent', #'SGD', #'RMSprop'
#learning_rate = 0.0005
learning_rate = 0.000498
#learning_rate = 0.00001

classifier_tpye = 3  # 1.category  2.binary  3 ch=3
loss_type = 5  # 1'IoU',  2'sparse_softmax',  3 'loss_dice'   4.sigmoid  5.loss_l2

input_ch = 3
conv19_out = 3  # 2 can set 255, different from fcn, where can only use 256
pad='SAME'  #'VALID'
u = 8
# Tensor must be 4-D with last dim 1, 3, or 4, not [1,320,320,2]

# IoU: conv19_out can be any num
# sparse_softmax: conv19_out can only be 2

######################
# GD
if optimizer_type == 'GD':
	learning_rate = learning_rate  # 0.00405 good #0.09 bad 3549 #0.04049 #0.03049 #0.015 #0.07297 #0.09568# TODO 0.05  0.005 better, 0.001 good \0.02, 0.13799 to 0.14 good for 6 classes,

######################
# RMSprop
if optimizer_type == 'RMSprop':
	decay = 0.00009
	momentum = 0.99
	epsilon_R = 0.009
######################
# SGD
if optimizer_type == 'SGD':
	learning_rate = learning_rate
	momentum_SGD = 0.99  # 0.99
# lr_decay = 0.01
# decay_step = 100
######################
######################
# adam
# Adam with these parameters beta1=0.9,beta2=0.999, epsilon=1e-08 etc the training accuracy is not stable, epsilon = 0.01
if optimizer_type == 'adam':
	learning_rate = learning_rate
	beta1 = 0.9
	beta2 = 0.999
	epsilon = 0.1


####################################################### Header End#### ################################################

def IoU(logits=None, labels=None, pixel_num=input_size):
	with tf.name_scope('cost'):
		# dimension of logits and labels must be equal
		logits = tf.to_float(tf.reshape(logits, (-1, pixel_num)))
		labels = tf.to_float(tf.reshape(labels, (-1, pixel_num)))
		
		# intersection = sum([logits] .* [labels])
		# since only 1s in labels will produce non-zero values, this produce is then the intersection
		intersec = tf.reduce_sum(tf.mul(logits, labels))
		#
		union = tf.reduce_sum(tf.sub(tf.add(logits, labels), tf.mul(logits, labels)))
		loss = tf.sub(tf.constant(1.0, dtype=tf.float32), tf.div(intersec, union))
	return loss


def loss_sigmoid(logits=None, labels=None, pixel_num=input_size):
	with tf.name_scope('cost'):
		logits = tf.to_float(tf.reshape(logits, [-1, pixel_num]))
		labels = tf.to_float(tf.reshape(labels, [-1, pixel_num]))
		cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits, labels)  # no args keyword
		# cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels) #no args keyword, all values
		# go to 1 or 0 quickly
		loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')
	return loss


def loss_dice_coeff(logits=None, labels=None, pixel_num=input_size):
	# input labels and logits must be of type float (use tf.to_float)
	# output: loss value, which is the negative dice coefficient
	# accuracy is the dice coefficient
	pixel_num = input_size
	with tf.name_scope('cost'):
		# num_equals_net_output = conv19_out
		# the dimension of labels and logits should be equal
		print 'logits flat shape:', logits.get_shape(), ',  labels flat shape:', labels.get_shape()
		logits = tf.to_float(tf.reshape(logits, (-1, pixel_num)))
		labels = tf.to_float(tf.reshape(labels, (-1, pixel_num)))
		
		print 'logits flat shape:', logits.get_shape(), ',  labels flat shape:', labels.get_shape()
		
		# using * and axis=1
		# intersection = tf.reduce_sum(labels * logits,axis=1,keep_dims=True,name='intersection')
		# Info: intersection size: (?, 1, 102400)
		# ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,102400,102400]
		
		# using mul
		intersection = tf.reduce_sum(tf.mul(labels, logits))
		# Info: intersection size: ()
		# ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,102400,102400]
		
		smooth = 1  # avoid 0 as output
		loss_inter = tf.add(2. * intersection, smooth, name='loss_inter')
		# loss_inter = tf.add(tf.mul(tf.constant(2), intersection,axis=1,keep_dims=True), smooth)
		loss_u1 = tf.add(tf.reduce_sum(labels), tf.reduce_sum(logits), name='loss_u1')
		loss_u = tf.add(loss_u1, smooth, name='loss_u')
		loss_div = tf.div(loss_inter, loss_u, name='loss_div')  # expected value very close to or equal 1
		loss = tf.sub(1.0, loss_div, name='cost')
		'''
	intersection = tf.reduce_sum(flat_logits * flat_labels, axis=1, keep_dims=True)
		union = tf.reduce_sum(tf.mul(flat_logits, flat_logits), axis=1, keep_dims=True) \
						+ tf.reduce_sum(tf.mul(flat_labels, flat_labels), axis=1, keep_dims=True)
		loss = 1 - tf.reduce_mean(2 * intersection / (union))
	'''
	return loss


# return (2. * intersection + smooth) / (K.sum(labels) + K.sum(logits) + smooth)
def loss_l2(logits=None, labels=None, pixel_num=input_size):
	import keras
	with tf.name_scope('cost'):
		logits = tf.to_float(tf.reshape(logits, [-1, pixel_num]))
		labels = tf.to_float(tf.reshape(labels, [-1, pixel_num]))
		
		predictions = logits
		# **************************************************************************************
		# - weighted cross entropy
		# logits, targets, pos_weight, name=None
		# loss_all = tf.nn.weighted_cross_entropy_with_logits(logits, labels, -0.0008, name=None)
		# loss = tf.reduce_mean(loss_all)
		#################################################
		# - binary crossentropy
		# epsilon = 1e-8
		# loss = tf.reduce_mean(-(labels * tf.log(logits + epsilon) +
		#                      (1. - labels) * tf.log(1. - logits + epsilon)))
		# **************************************************************************************
		
		################################################
		times_diff = tf.mul(1.0, (logits - labels))
		loss = tf.reduce_sum(tf.pow(times_diff, 2)) / (1.0 * pixel_num)
	return loss


def loss2(logits=None, labels=None, n_classes=0, cost_name=''):  # cross_entropy, dice_coefficient
	"""
	Constructs the cost function, either cross_entropy, weighted cross_entropy or dice_coefficient.
	Optional arguments are:
	class_weights: weights for the different classes in case of multi-class imbalance
	regularizer: power of the L2 regularizers added to the loss function
	"""
	
	flat_logits = tf.reshape(logits, [-1, n_classes])
	flat_labels = tf.reshape(labels, [-1, n_classes])
	if cost_name == "cross_entropy":
		# class_weights = cost_kwargs.pop("class_weights", None)
		class_weights = None
		
		if class_weights is not None:
			class_weights = tf.constant(np.array(class_weights, dtype=np.float32))
			
			weight_map = tf.mul(flat_labels, class_weights)
			weight_map = tf.reduce_sum(weight_map, axis=1)
			
			loss_map = tf.nn.softmax_cross_entropy_with_logits(flat_logits, flat_labels)
			weighted_loss = tf.mul(loss_map, weight_map)
			
			loss = tf.reduce_mean(weighted_loss)
		
		else:
			loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(flat_logits,
			                                                              flat_labels))
	else:
		raise ValueError("Unknown cost function: " % cost_name)
	
	# regularizer = cost_kwargs.pop("regularizer", None)
	# regularizer = None
	# if regularizer is not None:
	#    regularizers = sum([tf.nn.l2_loss(variable) for variable in variables])
	#    loss += (regularizer * regularizers)

	return loss

def pool(img, k, pool_type='max',pad='SAME', name=None):
	if pool_type == 'max':
		return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)
	if pool_type == 'avg':
		return tf.nn.avg_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)

def p_relu0(x):
  alphas = tf.get_variable('alpha', x.get_shape()[-1],
                       initializer=tf.constant_initializer(0.0),
                        dtype=tf.float32)
  pos = tf.nn.relu(x)
  neg = alphas * (x - abs(x)) * 0.5
  return pos + neg

def p_relu(_x, name="prelu"):
  _alpha = tf.get_variable(name, shape=_x.get_shape()[-1],
                           dtype=_x.dtype, initializer=tf.constant_initializer(0.1))
  return tf.maximum(0.0, _x) + _alpha * tf.minimum(0.0, _x)

def conv2d(img, w, b, k, pad='SAME', name=None):
	#x = tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b)
	return p_relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b),name=name)
	#return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b), name=name)



def conv2d_leaky(img, w, b, k, pad='SAME', name=None):
	alpha = 0.0001
	x = tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b)
	return tf.maximum(alpha*x,x,name=name)


def conv2d_1(img, w, b, k, pad='SAME', name=None):
	return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b), name=name)


def max_pool(img, k, pad='SAME', name=None):  # 'SAME', 'VALID'
	return tf.nn.max_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)


def avg_pool(img, k, pad='SAME', name=None):  # 'SAME', 'VALID'
	return tf.nn.avg_pool(img, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=pad, name=name)


def train(loss_val, var_list, learning_rate):
	optimizer = tf.train.AdamOptimizer(learning_rate)
	grads = optimizer.compute_gradients(loss_val, var_list=var_list)
	return optimizer.apply_gradients(grads)


def conv_net(_X, _weights, _biases, _dropout, filter_size, filter_size_out, k_conv, k_pool, k_out, SEED):
	# - INPUT Layer
	# Reshape input picture
	
	# _X = tf.reshape(_X, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 3])
	
	################################
	# - Convolution Layer 1,2
	# - Convolution Layer 1,2
	conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'], k_conv, name='conv1')  # 4
	conv2 = conv2d(conv1, _weights['wc2'], _biases['bc2'], k_conv, pad=pad,name='conv2')
	pool1 = pool(conv2, k_pool, name='pool1')
	pool1d = tf.nn.dropout(pool1, _dropout[0])  # TODO comment it later!
	
	################################
	# - Convolution Layer 3,4
	conv3 = conv2d(pool1d, _weights['wc3'], _biases['bc3'], k_conv, name='conv3')
	conv4 = conv2d(conv3, _weights['wc4'], _biases['bc4'], k_conv, pad=pad,name='conv4')
	pool2 = pool(conv4, k_pool, name='pool2')
	pool2d = tf.nn.dropout(pool2, _dropout[1])
	
	# - Convolution Layer 5,6
	conv5 = conv2d(pool2d, _weights['wc5'], _biases['bc5'], k_conv, name='conv5')
	conv6 = conv2d(conv5, _weights['wc6'], _biases['bc6'], k_conv, pad=pad,name='conv6')
	pool3 = pool(conv6, k_pool, name='pool3')
	pool3d = tf.nn.dropout(pool3, _dropout[2])
	
	# - Convolution Layer 7,8
	conv7 = conv2d(pool3d, _weights['wc7'], _biases['bc7'], k_conv, name='conv7')
	conv8 = conv2d(conv7, _weights['wc8'], _biases['bc8'], k_conv, pad=pad,name='conv8')
	pool4 = pool(conv8, k_pool, name='pool4')
	pool4d = tf.nn.dropout(pool4, _dropout[3])
	################################
	
	################################
	# - Convolution Layer 9,10
	conv9 = conv2d(pool4d, _weights['wc9'], _biases['bc9'], k_conv, name='conv9')
	conv10 = conv2d(conv9, _weights['wc10'], _biases['bc10'], k_conv,pad=pad, name='conv10')
	conv10 = tf.nn.dropout(conv10, _dropout[4])
	
	
	def conv2d_transpose(name_or_scope, input_tensor, out_ch, ksize=3, stride=2, re_shape=None, padding=pad):
		strides = [1, stride, stride, 1]
		debug = False
		with tf.variable_scope(name_or_scope):
			in_ch = input_tensor.get_shape()[3].value
			
			if re_shape is None:
				# get shape out of input_tensor
				in_shape = tf.shape(input_tensor)
				if debug:
					print 'input shape:', in_shape.get_shape()
				
				if (padding is 'SAME'):
					out_h = in_shape[1] * stride  # 20*2 = 40
					out_w = in_shape[2] * stride  # 20*2 = 40
				elif padding is 'VALID':
					out_h = ((in_shape[1] - 1) * stride) - 1  # (20-1)*2 -1 = 38
					out_w = ((in_shape[2] - 1) * stride) - 1  # (20-1)*2 -1 = 38
				# out_h = ((in_shape[1] - 1) * stride) + 1 #
				# out_w = ((in_shape[2] - 1) * stride) + 1
				new_shape = [in_shape[0], out_h, out_w,
				             out_ch]  # batch size, h,w,ch: ?,20,20,32, ch is num of classes
			else:
				# or from defined re_shape
				new_shape = [re_shape[0], re_shape[1], re_shape[2], out_ch]
			output_shape = tf.pack(new_shape)  # ?,40,40,32
			
			print '\nup Layer: %s, ch-in: %d, ch-out: %d' % (name_or_scope, in_ch, out_ch)
			weights_tensor_shape = [ksize, ksize, out_ch, in_ch]  # 3,3,32,32
			
			# create weights tensor for transpose
			num_input = ksize * ksize * in_ch / stride  # 3x3x32/2 = 9x16
			stddev = (2 / num_input) ** 0.5  # 2/(9x16) ** 0.5 = 1/72 ** 0.5 #'**' uppacking args list
			# using stddev = 0.1 the same value as for other nodes produces better learning speed
			# 1_0_diff increases quicker, once pred_train for one of labels becomes 0, the training must be
			# terminated, otherwise, the other values will also be trained to be 0.
			weights = tf.get_variable('w_transp', weights_tensor_shape,
			                          initializer=tf.truncated_normal_initializer(stddev=0.1, seed=SEED))
			
			# weights = self.get_deconv_filter(weight_tensor_shape)
			deconv = tf.nn.conv2d_transpose(input_tensor, weights, output_shape,
			                                strides=strides, padding=pad)
			# conv=tf.nn.conv2d(img,w,strides=[1,k,k,1],padding)
			# add=tf.nn.bias_add(conv,bias_)
			# activation=tf.nn.relu(add,name=name)
			# tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding=pad), b),name=name)
			
			# initialize bias
			bias = tf.get_variable('b', [out_ch], initializer=tf.truncated_normal_initializer(0.1, seed=SEED))
			# bias = tf.get_variable('b', [output_ch], initializer=tf.truncated_normal_initializer(0.1, seed=SEED))
			
			deconv_add = tf.nn.bias_add(deconv, bias)
			activation = tf.nn.relu(deconv_add)
			
			if debug:
				deconv = tf.Print(deconv, [tf.shape(deconv)],
				                  message='Shape of %s' % name_or_scope,
				                  summarize=4, first_n=1)
		
		return activation
	
	# UP_1
	# re_shape:[?,h,w,ch]
	up1 = conv2d_transpose('up1', conv10, 4*u, ksize=3, stride=2, re_shape=None, padding=pad) #(?, 40, 40, 32)
	
	concat_axis = 3
	up1_concat = tf.concat(concat_axis, [conv8, up1])  # conv8: 40,40,32, merged: (?, 40, 40, 64)
	
	up1_concatd = tf.nn.dropout(up1_concat, _dropout[5])
	# - Convolution Layer 11,12
	conv11 = conv2d(up1_concatd, _weights['wc11'], _biases['bc11'], k_conv, name='conv11')  # conv11_out = 32
	conv12 = conv2d(conv11, _weights['wc12'], _biases['bc12'], k_conv, name='conv12')
	
	##########################################################
	# UP_2
	up2 = conv2d_transpose('up2', conv12, 4*u, ksize=3, stride=2, re_shape=None, padding=pad)
	
	up2s = tf.slice(up2, [0, 0, 0, 0], [0, 80, 80,     2*u])
	conv6s = tf.slice(conv6, [0, 0, 0, 0], [0, 80, 80, 2*u])
	concat_axis = 3
	up2_concat1 = tf.concat(concat_axis, [conv6s, up2s], name='up2conv6')  # conv6: 80,80,32
	# the input ch for conv13 is 32, so the output ch for up2_concat must be reduced to 32
	#tf.random_crop(value, size, seed=None, name=None)
	#up2_concats = tf.slice(up2_concat, [1:-1, 0, 0, 0], [1:-1, 80, 80, 4*u]) #[1:-1, :, :, :]
	#up2_concats = tf.slice(up2_concat, [0, 0, 0, 0], [0, 80, 80, 4*u]) #[1:-1, :, :, :] #(up2conv6, Slice/begin, Slice/size)
	
	#up2_concat = conv2d(up2_concat1, _weights['concat2'], _biases['bc_concat2'], k_conv, name='concat2')
	
	up2_concatd = tf.nn.dropout(up2_concat1, _dropout[6])
	
	# - Convolution Layer 13,14
	conv13 = conv2d(up2_concatd, _weights['wc13'], _biases['bc13'], k_conv, name='conv13') #32
	conv14 = conv2d(conv13, _weights['wc14'], _biases['bc14'], k_conv, name='conv14')
	
	##########################################################
	# UP_3
	up3 = conv2d_transpose('up3', conv14, 2*u, ksize=3, stride=2, re_shape=None, padding=pad)  # ?, 160, 160, 32  16

	conv4s = tf.slice(conv4, [0, 0, 0, 0], [0, 160, 160, 2*u])  #  cov4  # ?, 160, 160,16
	up3s   = tf.slice(up3, [0, 0, 0, 0], [0, 160, 160,   2*u])
	concat_axis = 3
	up3_concat1 = tf.concat(concat_axis, [conv4s, up3s])

	up3_concatd = tf.nn.dropout(up3_concat1, _dropout[7])
	
	
	# - Convolution Layer 15,16
	conv15 = conv2d(up3_concatd, _weights['wc15'], _biases['bc15'], k_conv, name='conv15')  #16, 16
	conv16 = conv2d(conv15, _weights['wc16'], _biases['bc16'], k_conv, name='conv16')
	# conv16 can be 8
	
	##########################################################
	# UP_4 (16-in, 8-out)
	
	if PRINT_ARCHI:
		print 'input tensor', _X.get_shape()
		print 'conv1 ( f=', filter_size, 'k=', k_conv, ')', conv1.get_shape()
		print 'conv2 ( f=', filter_size, 'k=', k_conv, ')', conv2.get_shape(), '<===4'
		print 'conv2 - max pooling (k=', k_pool, ')', pool1.get_shape()
		print '- dropout ( keep rate', dropout[0], ')', pool1d.get_shape()
		print '\nconv3 ( f=', filter_size, 'k=', k_conv, ')', conv3.get_shape()
		print 'conv4 ( f=', filter_size, 'k=', k_conv, ')', conv4.get_shape(), '<===3'
		print 'conv4 max pooling ( k=', k_pool, ')', pool2.get_shape()
		print '- dropout ( keep rate', dropout[1], ')', pool2d.get_shape()
		print '\nconv5 ( f=', filter_size, 'k=', k_conv, ')', conv5.get_shape()
		print 'conv6 ( f=', filter_size, 'k=', k_conv, ')', conv6.get_shape(), '<===2'
		print 'conv6 max pooling ( k=', k_pool, ')', pool3.get_shape()
		print '- dropout ( keep rate', dropout[2], ')', pool3d.get_shape()
		print '\nconv7 ( f=', filter_size, 'k=', k_conv, ')', conv7.get_shape()
		print 'conv8 ( f=', filter_size, 'k=', k_conv, ')', conv8.get_shape(), '<===1'
		print 'conv8 max pooling ( k=', k_pool, ')', pool4.get_shape()
		print '- dropout ( keep rate', dropout[3], ')', pool4d.get_shape()
		print '\nconv9 ( f=', filter_size, 'k=', k_conv, ')', conv9.get_shape()
		print 'conv10 ( f=', filter_size, 'k=', k_conv, ')', conv10.get_shape()
		print '- dropout ( keep rate', dropout[4], ')', conv10.get_shape()
		print 'conv8 shape:', conv8.get_shape(), ',  up1 transpose shape:', up1.get_shape(), '<=== 1'  # [?,40,40,32]
		print 'up1,conv8 concat(a=', concat_axis, ') shape:', up1_concat.get_shape()
		print 'up1 concat - dropout ( keep rate', dropout[5], ')', up1_concatd.get_shape()
		print '\nconv11 ( f=', filter_size, 'k=', k_conv, ')', conv11.get_shape()
		print 'conv12 ( f=', filter_size, 'k=', k_conv, ')', conv12.get_shape()
		print 'conv6 shape:', conv6.get_shape(), ',  up2 transpose shape:', up2.get_shape(), '<=== 2'
		#print 'up2,conv6 concat (a=', concat_axis, ') shape:', up2_concat.get_shape()
		#print 'up2_concat slice shape:', up2_concats.get_shape()
		print '- merge conv6, dropout ( keep rate', dropout[6], ')', up2_concatd.get_shape()  # ?, 80, 80, 64
		print '\nconv13 ( f=', filter_size, 'k=', k_conv, ')', conv13.get_shape()  # ?, 80, 80, 32
		print 'conv14 ( f=', filter_size, 'k=', k_conv, ')', conv14.get_shape()  # ?, 80, 80, 32
		print '\nup3 transposed shape:', up3.get_shape()
		print 'conv4:', conv4.get_shape(), ',  up3:', up3.get_shape()
		#print 'conv4 slice:', conv4s.get_shape(), ',  up3 slice:', up3s.get_shape(), '<=== 3'
		#print 'up3,conv4s concat(a=', concat_axis, ') shape:', up3_concat.get_shape()  # (?, 160, 160, 48)    #use slice (1, 160, 160, 32)
		print '- up3 concat, dropout ( keep rate', dropout[7], ')', up3_concatd.get_shape()  # ?, 160, 160, 48
		print '\nconv15 ( f=', filter_size, 'k=', k_conv, ')', conv15.get_shape()  # ?, 160, 160, 16
		print 'conv16 ( f=', filter_size, 'k=', k_conv, ')', conv16.get_shape()  # ?, 160, 160, 8
	
	up4 = conv2d_transpose('up4', conv16, u, ksize=3, stride=2, re_shape=None, padding=pad) #tmp u to 2u
	print '\nup4 transposed shape:', up4.get_shape()  # (?, 320, 320, 8)
	
	#up4s = tf.slice(up4,[0,0,0,0], [0,320,320,8])
	#print 'conv2:', conv2.get_shape(), ',  up4:', up4s.get_shape(), '<=== 4'  # conv2: (?, 320, 320, 8)
	
	concat_axis = 0
	up4_concat1 = tf.concat(concat_axis, [conv2, up4]) #-1,320,320,8
	print 'up4_concat1:', up4_concat1.get_shape()
	
	#up4_concat = conv2d(up4_concat1, _weights['concat4'], _biases['bc_concat4'], k_conv, name='concat4')
	
	#concat_axis = 3
	#up4_concat = tf.concat(concat_axis, [up4_concat, up4s]) #-1,320,320,16
	#print 'up4_concat:', up4_concat.get_shape()
	
	#print 'up4,conv2 concat (a=', concat_axis, ') shape:', up4_concat.get_shape()
	# conv4 first output size (?, 160, 160, 16)
	up4_concat = tf.nn.dropout(up4_concat1, _dropout[8])
	print '- dropout ( keep rate', dropout[8], ')', up4_concat.get_shape()
	
	# - Convolution Layer 17,18
	conv17 = conv2d(up4_concat, _weights['wc17'], _biases['bc17'], k_conv, name='conv17')
	#                                 8, 8
	print '\nconv17 ( f=', filter_size, 'k=', k_conv, ')', conv17.get_shape()  # ?, 320, 320, 8
	
	conv18 = conv2d(conv17, _weights['wc18'], _biases['bc18'], k_conv, name='conv18')
	print 'conv18 ( f=', filter_size, 'k=', k_conv, ')', conv18.get_shape()  # ?, 320, 320, 8

	# without activation, many outputs are negative, so should not use
	# or use relu, when it combined with softmax logits, all values go to 1 quickly
	# conv19 = conv2d(conv18, _weights['wc19'], _biases['bc19'], k_out, name='conv19')
	#
	# or use sigmoid activation for final conv layer, when it combined with softmax logits, all values go to 0 quickly
	# tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(img, w, strides=[1, k, k, 1], padding='SAME'), b))
	#
	
	
	conv19 = tf.nn.sigmoid(tf.nn.bias_add(tf.nn.conv2d(conv18, _weights['wc19'],
	              strides=[1, k_out, k_out, 1], padding=pad), _biases['bc19']),name='conv19')  # 1, 1
	print '\nconv19 ( f=', filter_size, 'k=', k_out, ')', conv19.get_shape()  # ?, 320, 320, 1
	
	
	
	# Output, class prediction
	#annotation_pred_max = tf.argmax(conv19, dimension=3)
	annotation_pred_max = tf.argmax(conv19, axis=3) #use tf.abs not work
	#annotation_pred_max = tf.argmax(conv19, axis=3)
	annotation_pred = tf.expand_dims(annotation_pred_max, dim=3, name='prediction')
	
	return [_X, conv1, conv2, conv3, conv4, conv5, conv6, conv7, \
	        conv8, conv9, conv10, up1, conv11, conv12, up2, conv13, conv14, up3, conv15,
	        conv16, up4, conv17, conv18, conv19, annotation_pred]


###################################################################
def define_model():
	# General input for tensorflow
	# hy: Graph input, same placeholders for various architectures
	x = tf.placeholder(tf.float32, [None, IMAGE_SIZE, IMAGE_SIZE, input_ch], name="x")
	
	################################################ Graph 4conv begin
	keep_prob = tf.placeholder(tf.float32, len(dropout), name="keep_prob")
	# hy: define receptive field size
	filter_size, filter_size_out = 3, 1
	k_conv, k_out, k_pool = 1, 1, 2
	
	
	SEED = 8  # hy: number of filters in conv1  8, 16, 64
	conv1_out, conv2_out, conv3_out, conv4_out = u, u, 2*u, 2*u
	conv5_out, conv6_out, conv7_out, conv8_out, conv9_out, conv10_out = 4*u, 4*u, 4*u, 4*u, 4*u, 4*u
	
	# must use the same name for up1 here and in net
	#up1, concat1, conv11_out, conv12_out, up2, concat2, conv13_out, conv14_out = 4*u, 8*u, 4*u, 4*u, 4*u, 4*u, 4*u, 4*u  # concat2:64 or 32
	up1,   concat1, conv11_out, conv12_out, up2,   concat21, concat2, conv13_out, conv14_out \
    = 4*u, 8*u,     4*u,        4*u,        4*u,    4*u,     4*u,     4*u,        4*u  # concat2:64 or 32
	
	## upsampling using fcn deconv
	# up3, concat3, conv15_out, conv16_out,  up4,concat4, conv17_out, conv18_out = 32, 16, 16, 8,         16, 8,  8,  8  # conv16_out:16 or 8
	
	# upsampling using version 1
	#up3, concat3, conv15_out, conv16_out,    up4, concat4, conv17_out, conv18_out = 4*u, 4*u, 2*u, u,       2*u, u, u, u  # conv16_out:16 or 8
	#                                                                                  ^
	
	up3,   concat31, concat3,   conv15_out, conv16_out,     up4, concat41,concat4, conv17_out, conv18_out \
	= 2*u, 4*u,      4*u,       2*u,        2*u,            u,   u,        u,       u,          u  # conv16_out:16 or 8
	# stddev = sqrt(2 / fan_in)  #np.random.randn(n) / sqrt(n)
	
	weights = {
		'wc1': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, input_ch, conv1_out], stddev=np.sqrt(2.0 / SEED).astype(np.float32),
			                    seed=SEED),
			name="wc1"),
		
		'wc2': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, SEED, conv2_out], stddev=np.sqrt(2.0 / SEED).astype(np.float32),
			                    seed=SEED),
			name="wc2"),
		'wc3': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv2_out, conv3_out],
		                                       stddev=np.sqrt(2.0 / conv2_out).astype(np.float32), seed=SEED),
		                   name="wc3"),
		'wc4': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv3_out, conv4_out],
		                                       stddev=np.sqrt(2.0 / conv3_out).astype(np.float32), seed=SEED),
		                   name="wc4"),
		'wc5': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv4_out, conv5_out],
		                                       stddev=np.sqrt(2.0 / conv4_out).astype(np.float32), seed=SEED),
		                   name="wc5"),
		'wc6': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv5_out, conv6_out],
		                                       stddev=np.sqrt(2.0 / conv5_out).astype(np.float32), seed=SEED),
		                   name="wc6"),
		'wc7': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv6_out, conv7_out],
		                                       stddev=np.sqrt(2.0 / conv6_out).astype(np.float32), seed=SEED),
		                   name="wc7"),
		'wc8': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv7_out, conv8_out],
		                                       stddev=np.sqrt(2.0 / conv7_out).astype(np.float32), seed=SEED),
		                   name="wc8"),
		'wc9': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv8_out, conv9_out],
		                                       stddev=np.sqrt(2.0 / conv8_out).astype(np.float32), seed=SEED),
		                   name="wc9"),
		'wc10': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv9_out, conv10_out],
		                                        stddev=np.sqrt(2.0 / conv9_out).astype(np.float32), seed=SEED),
		                    name="wc10"),
		
		'up1': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv10_out, up1],
		                                       stddev=np.sqrt(2.0 / conv10_out).astype(np.float32), seed=SEED)),
		'concat1': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up1, concat1], stddev=np.sqrt(2.0 / up1).astype(np.float32),
			                    seed=SEED)),
		
		'wc11': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat1, conv11_out],
		                                        stddev=np.sqrt(2.0 / concat1).astype(np.float32), seed=SEED),
		                    name="wc11"),
		'wc12': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv11_out, conv12_out],
		                                        stddev=np.sqrt(2.0 / conv11_out).astype(np.float32), seed=SEED),
		                    name="wc12"),
		
		'up2': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv12_out, up2],
		                                       stddev=np.sqrt(2.0 / conv12_out).astype(np.float32), seed=SEED)),
		
		'concat21': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up2, concat21], stddev=np.sqrt(2.0 / up2).astype(np.float32),
			                    seed=SEED)),
		
		'concat2': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, concat21, concat2], stddev=np.sqrt(2.0 / concat21).astype(np.float32),
			                    seed=SEED)),
		
		'wc13': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat2, conv13_out],
		                                        stddev=np.sqrt(2.0 / concat2).astype(np.float32), seed=SEED),
		                    name="wc13"),
		'wc14': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv13_out, conv14_out],
		                                        stddev=np.sqrt(2.0 / conv13_out).astype(np.float32), seed=SEED),
		                    name="wc14"),
		
		'up3': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv14_out, up3],
		                                       stddev=np.sqrt(2.0 / conv14_out).astype(np.float32), seed=SEED)),
		'concat31': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up3, concat31], stddev=np.sqrt(2.0 / up3).astype(np.float32),
			                    seed=SEED)),
		
		'concat3': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, concat31, concat3], stddev=np.sqrt(2.0 / concat31).astype(np.float32),
			                    seed=SEED)),
		
		'wc15': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat3, conv15_out],
		                                        stddev=np.sqrt(2.0 / concat3).astype(np.float32), seed=SEED),
		                    name="wc15"),
		'wc16': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv15_out, conv16_out],
		                                        stddev=np.sqrt(2.0 / conv15_out).astype(np.float32), seed=SEED),
		                    name="wc16"),
		
		'up4': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv16_out, up4],
		                                       stddev=np.sqrt(2.0 / conv16_out).astype(np.float32), seed=SEED)),
		'concat41': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, up4, concat41], stddev=np.sqrt(2.0 / up4).astype(np.float32),
			                    seed=SEED)),
		
		'concat4': tf.Variable(
			tf.truncated_normal([filter_size, filter_size, concat41, concat4], stddev=np.sqrt(2.0 / concat41).astype(np.float32),
			                    seed=SEED)),
		
		'wc17': tf.Variable(tf.truncated_normal([filter_size, filter_size, concat4, conv17_out],
		                                        stddev=np.sqrt(2.0 / concat4).astype(np.float32), seed=SEED),
		                    name="wc17"),
		'wc18': tf.Variable(tf.truncated_normal([filter_size, filter_size, conv17_out, conv18_out],
		                                        stddev=np.sqrt(2.0 / conv17_out).astype(np.float32), seed=SEED),
		                    name="wc18"),
		'wc19': tf.Variable(tf.truncated_normal([filter_size_out, filter_size_out, conv18_out, conv19_out],
		                                        stddev=np.sqrt(2.0 / conv18_out).astype(np.float32), seed=SEED),
		                    name="wc19"),
	}
	
	biases = {
		'bc1': tf.Variable(tf.truncated_normal([conv1_out]), name="bc1"),
		'bc2': tf.Variable(tf.truncated_normal([conv2_out]), name="bc2"),  # hy: use variable, instead fixed number
		'bc3': tf.Variable(tf.truncated_normal([conv3_out]), name="bc3"),
		'bc4': tf.Variable(tf.truncated_normal([conv4_out]), name="bc4"),
		'bc5': tf.Variable(tf.truncated_normal([conv5_out]), name="bc5"),
		'bc6': tf.Variable(tf.truncated_normal([conv6_out]), name="bc6"),
		'bc7': tf.Variable(tf.truncated_normal([conv7_out]), name="bc7"),
		'bc8': tf.Variable(tf.truncated_normal([conv8_out]), name="bc8"),
		'bc9': tf.Variable(tf.truncated_normal([conv9_out]), name="bc9"),
		'bc10': tf.Variable(tf.truncated_normal([conv10_out]), name="bc10"),
		
		'bc_up1': tf.Variable(tf.truncated_normal([up1])),
		'bc_concat1': tf.Variable(tf.truncated_normal([concat1])),
		
		'bc11': tf.Variable(tf.truncated_normal([conv11_out]), name="bc11"),
		'bc12': tf.Variable(tf.truncated_normal([conv12_out]), name="bc12"),
		
		'bc_up2': tf.Variable(tf.truncated_normal([up2])),
		'bc_concat21': tf.Variable(tf.truncated_normal([concat21])),
		'bc_concat2': tf.Variable(tf.truncated_normal([concat2])),
		
		'bc13': tf.Variable(tf.truncated_normal([conv13_out]), name="bc13"),
		'bc14': tf.Variable(tf.truncated_normal([conv14_out]), name="bc14"),
		
		'bc_up3': tf.Variable(tf.truncated_normal([up3])),
		'bc_concat31': tf.Variable(tf.truncated_normal([concat31])),
		'bc_concat3': tf.Variable(tf.truncated_normal([concat3])),
		
		'bc15': tf.Variable(tf.truncated_normal([conv15_out]), name="bc15"),
		'bc16': tf.Variable(tf.truncated_normal([conv16_out]), name="bc16"),
		
		'bc_up4': tf.Variable(tf.truncated_normal([up4])),
		'bc_concat41': tf.Variable(tf.truncated_normal([concat41])),
		'bc_concat4': tf.Variable(tf.truncated_normal([concat4])),
		
		'bc17': tf.Variable(tf.truncated_normal([conv17_out]), name="bc17"),
		'bc18': tf.Variable(tf.truncated_normal([conv18_out]), name="bc18"),
		'bc19': tf.Variable(tf.truncated_normal([conv19_out]), name="bc19"),
	}
	
	# hy: try with zero mean
	# tf.image.per_image_whitening(x)
	# this operation computes (x-mean)/adjusted_stddev
	
	#################################
	# tmp: test
	[_X, conv1, conv2, conv3, conv4, conv5, conv6, conv7, \
	 conv8, conv9, conv10, up1, conv11, conv12, up2, conv13, conv14, up3, conv15, \
	 conv16, up4, conv17, conv18, conv19, annotation_pred] \
		= conv_net(x, weights, biases, keep_prob, filter_size, filter_size_out, k_conv, k_pool, k_out, SEED)
	
	#################################
	y = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name="y")
	# logits = conv19,  labels: y
	y_int = tf.cast(y, tf.int32)
	#sparse_softmax can only take [0,3) for y and logits
	cost = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=conv19,
	                                                                      labels=tf.squeeze(y_int, squeeze_dims=[3]),
	                                                                      name="loss")))
	#conv19 = tf.reshape(conv19, shape=[None,IMAGE_SIZE,IMAGE_SIZE,3])
	
	#cost = tf.reduce_mean((tf.nn.softmax_cross_entropy_with_logits(logits=conv19,
	#																	  labels=y_int,
	#																	  name="loss")))

	#############################################################################
	# Define loss function and optimizer
	# ****************************************************************************
	# ****************************************************************************
	trainable_var = tf.all_variables()
	if optimizer_type == 'adam':
		# hy: Adam with these parameters beta1=0.9,beta2=0.999, epsilon=1e-08 etc the training
		# accuracy is not stable, epsilon = 0.01 better for these data
		print '\noptimizer:', optimizer_type, 'learning_rate:', learning_rate, '\nbeta11:', beta1, \
			'\tbeta2:', beta2, '\tepsilon:', epsilon
		# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon,
		#                                  use_locking=False, name='Adam').minimize(cost)
		# trainable_var = tf.all_variables()
		optimizer = tf.train.AdamOptimizer(learning_rate)
		grads = optimizer.compute_gradients(cost, var_list=trainable_var)
		optimize_op = optimizer.apply_gradients(grads)  # must feed optimizer... to a variable to be as tensor run in sess
	
	# hy: Adam with only learning rate as parameter can also be used to continue a training that was done previously with beta,epsilon setup
	# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # TODO change to ADAM
	if optimizer_type == 'GD':
		# hy: GradientDescentOptimizer
		print '\noptimizer:', optimizer_type, '\tlearning_rate:', learning_rate
		optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate, name="GD").minimize(cost)
	# for learning_rate_i in xrange(5):
	# optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_i*(1+learning_rate_i*0.000001),name="GD").minimize(cost)
	
	if optimizer_type == 'SGD':
		print '\noptimizer:', optimizer_type, '\tlearning_rate:', learning_rate, '\tmomentum:', momentum_SGD
		optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum_SGD, name='SGD').minimize(
			cost)
	# = tf.train.MomentumOptimizer(LEARNING_RATE, MOMENTUM).minimize(cost)
	
	#################################################################
	# Build the summary operation based on the TF collection of Summaries.
	# Adding variables to be visualized
	tf.summary.scalar('Loss', cost)
	# tf.summary.image("a_ground_truth", tf.cast(y*255, tf.uint8), max_outputs=2)    #tf.cast(annotation, tf.uint8)
	tf.summary.image("a_ground_truth", tf.reshape(tf.to_float(y), shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=2)
	tf.summary.image("b_pred", tf.reshape(tf.to_float(annotation_pred * 255), shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]),
	                 max_outputs=4)
	
	#tf.summary.image('c_conv19', tf.reshape(conv19*255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, conv19_out]), max_outputs=4)
	#c_conv19_s1 = tf.slice(conv19, [0, 0, 0, 0], [0, IMAGE_SIZE, IMAGE_SIZE, 1])
	#c_conv19_s2 = tf.slice(conv19, [0, 0, 0, 0], [0, IMAGE_SIZE, IMAGE_SIZE, 2])
	tf.summary.image('c_conv19', tf.reshape(conv19*255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	#tf.summary.image('c_conv19s2', tf.reshape(c_conv19_s2*255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	tf.summary.image('c_ori_rgb', tf.reshape(x * 255, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, input_ch]), max_outputs=4)
	
	def view_conv_(layers, layer_names):
		for layer, layer_name in zip(layers, layer_names):
			conv_view_size = layer.get_shape().as_list()[1]
			tf.summary.image(layer_name, tf.reshape(layer, shape=[-1, conv_view_size, conv_view_size, 1]), max_outputs=4)
	
	view_conv_([conv1, conv3, conv5, conv7, conv10],
	           ['conv1', 'conv3', 'conv5', 'conv7', 'conv10'])

	
	##############
	tf.summary.histogram('histogram_conv1w', weights['wc1'])
	tf.summary.histogram('histogram_conv10w', weights['wc10'])
	tf.summary.histogram('histogram_conv19w', weights['wc19'])
	
	summary_op = tf.summary.merge_all()
	
	return (learning_rate, dropout, dropout_1s, optimizer_type, classifier_tpye, loss_type,
	        x,y, y_int, keep_prob, optimizer, cost, summary_op, optimize_op,
	        conv19_out, conv19, annotation_pred)


import keras
import seg_net_arch as u_a
import tensorflow as tf
import numpy as np


def init_tf_var():
	x = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 3), name='x')
	y = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1), name='y')
	
	test_result = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1), name='test_result')
	conv19view = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, 1), name='conv19view')
	dropout = [0.15, 0.25, 0.4, 0.45, 1, 0.4, 0.25, 0.15, 0.15]
	keep_prob = tf.placeholder(tf.float32, len(dropout), name="keep_prob")
	dropout_1s = [1] * len(dropout)
	######
	tf.summary.image('original_rgb', tf.reshape(x, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 3]), max_outputs=4)
	tf.summary.image('ground_truth', tf.reshape(y, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	tf.summary.image('test_result', tf.reshape(test_result, shape=[-1, IMAGE_SIZE, IMAGE_SIZE, 1]), max_outputs=4)
	##############
	summary_op = tf.summary.merge_all()
	##############
	return x, y, conv19view, test_result, keep_prob, dropout, dropout_1s, summary_op


def run_model_k(images, masks):
	keras.callbacks.History()
	epochs = 3000  # 1200
	learning_rate = 0.0002
	decay_rate = learning_rate / epochs
	momentum = 0.99
	sgd = u_a.SGD(lr=learning_rate, momentum=momentum)  # hy:decay_rate
	model, conv19 = u_a.get_model(IMAGE_SIZE, IMAGE_SIZE)  # (?, 1, 320, 320)
	
	##############
	'''
	conv19view = tf.transpose(conv19, perm=[0,2,3,1])
	print 'conv19 shape:', conv19.get_shape()
	tf.summary.image('conv19', tf.reshape(conv19, shape=[-1, 320, 320, 1]), max_outputs=4)
	batch_xs = np.transpose(images, (0, 3, 1, 2))  # -1,3, 320,320
	batch_ys = np.transpose(masks, (0, 3, 1, 2))  # -1,3, 320,320
	######
	tf.summary.image('original_color', tf.reshape(input_img, shape=[-1, 320, 320, 1]), max_outputs=4)
	#tf.summary.image("ground_truth", tf.reshape(tf.to_float(y), shape=[-1, tensor_h, tensor_w, 1]), max_outputs=2)
	#tf.summary.image('conv19_255', tf.reshape(conv19_255, shape=[-1, tensor_h, tensor_w, 1]), max_outputs=4)

	##############
	tf.summary.histogram('histogram_conv19', conv19view)
	summary_op = tf.summary.merge_all()
	##############
	train_res = sess.run(summary_op, feed_dict={'x:0': batch_xs[0], 'y:0': batch_ys[0], 'keep_prob:0': dropout_1s})
	train_writer.add_summary(train_res, train_step)
	##############
	'''
	loss_type, optimizer_type = 'binary_crossentropy', 'sgd'
	model.compile(loss='binary_crossentropy', optimizer=sgd)
	
	# images.reshape((None,1,h,w))
	# fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None,
	#    shuffle=True, class_weight=None, sample_weight=None)
	# docu example
	# weights.{epoch:02d}-{val_loss:.2f}.hdf5 # val_loss, must first define validation set
	# '/Users/Alex/checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')
	
	model_path = '../testbench/seg_mul/kintf_seg/'
	save_model_name = model_path + 'weights.{epoch:02d}'  # model_path + 'test_' + '{epoch:02d}.hdf5'
	save_params = keras.callbacks.ModelCheckpoint(filepath=model_path + 'weights.{epoch:02d}.hdf5',
	                                              monitor='val_loss', verbose=2,
	                                              save_best_only=False, save_weights_only=False, mode='auto')
	
	# keras.callbacks.TensorBoard(log_dir='../Tensorboard_data/sum108/', histogram_freq=1, write_graph=True, write_images=True)
	
	class LossHistory(keras.callbacks.Callback):
		def on_train_begin(self, logs={}):
			self.losses = []
		
		def on_batch_end(self, batch, logs={}):
			self.losses.append(logs.get('loss'))
	
	# train
	history_train = model.fit(images, masks, batch_size=1, nb_epoch=epochs, callbacks=[save_params], shuffle=True)
	
	history = LossHistory()
	print history.losses
	print 'val keys', history_train.history.keys()
	
	model.save(model_path + 'model_' + '.h5')
